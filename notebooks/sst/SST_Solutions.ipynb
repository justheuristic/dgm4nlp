{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SST-Solutions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probabll/dgm4nlp/blob/master/notebooks/sst/SST_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Udt3kHMdWvYe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will need to import some helper code, so we need to run this"
      ]
    },
    {
      "metadata": {
        "id": "U8eXUCRiWvYi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "nb_dir = os.path.split(os.getcwd())[0]\n",
        "if nb_dir not in sys.path:\n",
        "    sys.path.append(nb_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rYhItDYMZi6a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Colab\n",
        "\n",
        "On [colab](https://colab.research.google.com) you can point to a notebook on our [github repo](https://github.com/probabll/dgm4nlp) then you can run the following:"
      ]
    },
    {
      "metadata": {
        "id": "_shCMftIx1rW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "using_colab = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N-fFME2OW22i",
        "colab_type": "code",
        "outputId": "5ca0f139-607f-488c-ba99-8f23bfdb3e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "cell_type": "code",
      "source": [
        "if using_colab:\n",
        "  !rm -fr dgm4nlp sst\n",
        "  !git clone https://github.com/probabll/dgm4nlp.git\n",
        "  !cp -R dgm4nlp/notebooks/sst ./  \n",
        "  !ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'dgm4nlp'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 43 (delta 11), reused 20 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (43/43), done.\n",
            "dgm4nlp  sample_data  sst\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l_7NCZlZacNu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can start our lab."
      ]
    },
    {
      "metadata": {
        "id": "s9mH-rUhWvYq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "# CPU should be fine for this lab\n",
        "device = torch.device('cpu')  \n",
        "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
        "from collections import OrderedDict\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "okMoxTJ9bWjc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sentiment Classification \n",
        "\n",
        "\n",
        "We are going to augment a sentiment classifier with a layer of discrete latent variables which will help us improve the model's interpretability. But first, let's quickly review the baseline task.\n",
        "\n",
        "\n",
        "In sentiment classification, we have some text input $x = \\langle x_1, \\ldots, x_n \\rangle$, e.g. a sentence or short paragraph, which expresses a certain sentiment $y$, i.e. one of $K$ classes, towards a subject (e.g. a film or a product). \n",
        "\n",
        "\n",
        "\n",
        "We can learn a sentiment classifier by learning a categorical distribution over classes for a given input:\n",
        "\n",
        "\\begin{align}\n",
        "Y|x &\\sim \\text{Cat}(f(x; \\theta))\n",
        "\\end{align}\n",
        "\n",
        "A categorical distribution over $K$ classes is parameterised by a $K$-dimensional probability vector, here we use a neural network $f$ to map from the input to this probability vector. Technically we say *a neural network parameterises our model*, that is, it computes the parameters of our categorical observation model. The figure below is a graphical depiction of the model: circled nodes are random variables (a shaded node is an observed variable), uncircled nodes are deterministic, a plate indicates multiple draws.\n",
        "\n",
        "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/classifier.png\"  height=\"100\">\n",
        "\n",
        "The neural network (NN) $f(\\cdot; \\theta)$ has parameters of its own, i.e. the weights of the various architecture blocks used, which we denoted generically by $\\theta$.\n",
        "\n",
        "Suppose we have a dataset $\\mathcal D = \\{(x^{(1)}, y^{(1)}), \\ldots, (x^{(N)}, y^{(N)})\\}$ containing $N$ i.i.d. observations. Then we can use the log-likelihood function \n",
        "\\begin{align}\n",
        "\\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{N} \\log P(y^{(k)}|x^{(k)}, \\theta) \\\\\n",
        "&= \\sum_{k=1}^{N} \\log \\text{Cat}(y^{(k)}|f(x^{(k)}; \\theta))\n",
        "\\end{align}\n",
        " to estimate $\\theta$ by maximisation:\n",
        " \\begin{align}\n",
        " \\theta^\\star = \\arg\\max_{\\theta \\in \\Theta} \\mathcal L(\\theta|\\mathcal D)\n",
        " \\end{align}\n",
        " \n",
        "\n",
        "We can use stochastic gradient-ascent to find a local optimum of $\\mathcal L(\\theta|\\mathcal D)$, which only requires a gradient estimate:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{|\\mathcal D|} \\log P(y^{(k)}|x^{(k)}, \\theta) \\\\ \n",
        "&= \\sum_{k=1}^{|\\mathcal D|} \\frac{1}{N} N \\log P(y^{(k)}|x^{(k)}, \\theta)  \\\\\n",
        "&= \\mathbb E_{\\mathcal U(1/N)} \\left[ N \\log P(y^{(K)}|x^{(K)}, \\theta) \\right]  \\\\\n",
        "&\\overset{\\text{MC}}{\\approx} \\frac{N}{M} \\sum_{s=1}^M \\log P(y^{(k_s)}|x^{(k_s)}, \\theta) \\\\\n",
        "&\\text{where }K_s \\sim \\mathcal U(1/N)\n",
        "\\end{align}\n",
        "\n",
        "This is a Monte Carlo (MC) estimate of the gradient computed on $M$ data points selected uniformly at random from $\\mathcal D$.\n",
        "\n",
        "For as long as $f$ remains differentiable wrt to its inputs and parameters, we can rely on automatic differentiation to obtain gradient estimates.\n",
        "\n",
        "In what follows we show how to design $f$ and how to extend this basic model to a latent-variable model.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "lB2lEsNuWvYx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Architecture\n",
        "\n",
        "\n",
        "The function $f$ conditions on a high-dimensional input (i.e. text), so we need to convert it to continuous real vectors. This is the job an *encoder*. \n",
        "\n",
        "**Embedding Layer**\n",
        "\n",
        "The first step is to convert the words in $x$ to vectors, which in this lab we will do with a pre-trained embedding layer (we will use GloVe).\n",
        "\n",
        "We will denote the embedding of the $i$th word of the input by:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf x_i = \\text{glove}(x_i)\n",
        "\\end{equation}\n",
        "\n",
        "**Encoder Layer**\n",
        "\n",
        "In this lab, an encoder takes a sequence of input vectors $\\mathbf x_1^n$, each $I$-dimensional, and produces a sequence of output vectors $\\mathbf t_1^n$, each $O$-dimensional and a summary vector $\\mathbf h \\in \\mathbb R^O$:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathbf t_1^n, \\mathbf h = \\text{encoder}(\\mathbf x_1^n; \\theta_{\\text{enc}})\n",
        "\\end{equation}\n",
        "\n",
        "where we use $\\theta_{\\text{enc}}$ to denote the subset of parameters in $\\theta$ that are specific to this encoder block. \n",
        "\n",
        "*Remark:* in practice for a correct batched implementation, our encoders also take a mask matrix and a vector of lengths.\n",
        "\n",
        "Examples of encoding functions can be a feed-forward NN (with an aggregator based on sum or average/max pooling) or a recurrent NN (e.g. an LSTM/GRU). Other architectures are also possible.\n",
        "\n",
        "**Output Layer**\n",
        "\n",
        "From our summary vector $\\mathbf h$, we need to parameterise a categorical distribution over $K$ classes, thus we use\n",
        "\n",
        "\\begin{align}\n",
        "f(x; \\theta) &= \\text{softmax}(\\text{dense}_K(\\mathbf h; \\theta_{\\text{output}}))\n",
        "\\end{align}\n",
        "\n",
        "where $\\text{dense}_K$ is a dense layer with $K=5$ outputs and $\\theta_{\\text{output}}$ corresponds to its parameters (weight matrix and bias vector). Note that we need to use the softmax activation function in order to guarantee that the output of $f$ is a normalised probability vector.\n"
      ]
    },
    {
      "metadata": {
        "id": "Kc15Nv2i41cq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementation\n",
        "\n",
        "We provide a few encoders which implement the following abstract Encoder class:"
      ]
    },
    {
      "metadata": {
        "tags": [
          "encoders"
        ],
        "id": "xwEPXT2MWvYz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    For you to focus on DGMs and abstract away from certain architecture details, \n",
        "     we will be providing some helper classes.\n",
        "     \n",
        "    An encoder is one of them.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "    def forward(self, inputs, mask, lengths):\n",
        "        \"\"\"\n",
        "        The inputs are batch-first tensors\n",
        "        \n",
        "        :param inputs: [B, T, d]\n",
        "        :param mask: [B, T]\n",
        "        :param lengths: [B]\n",
        "        :returns: [B, T, d], [B, d]\n",
        "            where the first tensor is the transformed input\n",
        "            and the second tensor is a summary of all inputs\n",
        "        \"\"\"\n",
        "        pass\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WA5wmkcRg9Am",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will mostly use a bag-of-words encoder (to keep everything lightweight), but we also provide a feed-forward and an LSTM encoder for you:"
      ]
    },
    {
      "metadata": {
        "tags": [
          "encoders"
        ],
        "id": "ISjc0K1aWvY4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Passthrough(Encoder):\n",
        "    \"\"\"\n",
        "    This encoder does not do anything, it simply passes the input forward and summarises \n",
        "        them via a sum.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Passthrough, self).__init__()\n",
        "        \n",
        "    def forward(self, inputs, mask, lengths, **kwargs):\n",
        "        # inputs: [B, T, d]\n",
        "        # mask: [B, T]\n",
        "        # lengths: [B]\n",
        "        \n",
        "        # [B, T, d], [B, d]\n",
        "        return inputs, (inputs * mask.unsqueeze(-1).float()).sum(dim=1) \n",
        "\n",
        "    \n",
        "class FFEncoder(Encoder):\n",
        "    \"\"\"\n",
        "    A typical feed-forward NN with tanh hidden activations.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, output_size, \n",
        "                 activation=None, \n",
        "                 hidden_sizes=[], \n",
        "                 aggregator='sum',\n",
        "                 dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param input_size: int\n",
        "        :param output_size: int\n",
        "        :param hidden_sizes: list of integers (dimensionality of hidden layers)\n",
        "        :param aggregator: 'sum' or 'avg'\n",
        "        :param dropout: dropout rate\n",
        "        \"\"\"\n",
        "        super(FFEncoder, self).__init__()\n",
        "        layers = []\n",
        "        if hidden_sizes:                    \n",
        "            for i, size in enumerate(hidden_sizes):\n",
        "                if dropout > 0.:\n",
        "                  layers.append(('dropout%d' % i, nn.Dropout(p=dropout)))\n",
        "                layers.append(('linear%d' % i, nn.Linear(input_size, size)))\n",
        "                layers.append(('tanh%d' % i, nn.Tanh()))\n",
        "                input_size = size\n",
        "        if dropout > 0.:\n",
        "          layers.append(('dropout', nn.Dropout(p=dropout)))\n",
        "        layers.append(('linear', nn.Linear(input_size, output_size)))       \n",
        "        self.layer = nn.Sequential(OrderedDict(layers))     \n",
        "        self.activation = activation\n",
        "        if not aggregator in ['sum', 'avg']:\n",
        "            raise ValueError(\"I can only aggregate outputs using 'sum' or 'avg'\")\n",
        "        self.aggregator = aggregator\n",
        "        \n",
        "    def forward(self, x, mask, lengths):\n",
        "        # [B, T, d]\n",
        "        y = self.layer(x)\n",
        "        if not self.activation is None:\n",
        "            y = self.activation(y)\n",
        "        # [B, d]\n",
        "        s = (y * mask.unsqueeze(-1).float()).sum(dim=1)\n",
        "        if self.aggregator == 'avg':\n",
        "            s /= lengths.unsqueeze(-1).float()\n",
        "        return y, s\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "class LSTMEncoder(Encoder):\n",
        "    \"\"\"\n",
        "    This module encodes a sequence into a single vector using an LSTM,\n",
        "     it also returns the hidden states at each time step.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, hidden_size: int = 200,\n",
        "                 batch_first: bool = True,\n",
        "                 bidirectional: bool = True):\n",
        "        \"\"\"\n",
        "        :param in_features:\n",
        "        :param hidden_size:\n",
        "        :param batch_first:\n",
        "        :param bidirectional:\n",
        "        \"\"\"\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(in_features, hidden_size, batch_first=batch_first,\n",
        "                            bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, x, mask, lengths):\n",
        "        \"\"\"\n",
        "        Encode sentence x\n",
        "        :param x: sequence of word embeddings, shape [B, T, E]\n",
        "        :param mask: byte mask that is 0 for invalid positions, shape [B, T]\n",
        "        :param lengths: the lengths of each input sequence [B]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        packed_sequence = pack_padded_sequence(x, lengths, batch_first=True)\n",
        "        outputs, (hx, cx) = self.lstm(packed_sequence)\n",
        "        outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
        "\n",
        "        # classify from concatenation of final states\n",
        "        if self.lstm.bidirectional:\n",
        "            final = torch.cat([hx[-2], hx[-1]], dim=-1)\n",
        "        else:  # classify from final state\n",
        "            final = hx[-1]\n",
        "\n",
        "        return outputs, final\n",
        "    \n",
        "    \n",
        "def get_encoder(layer, in_features, hidden_size, bidirectional=True):\n",
        "    \"\"\"Returns the requested layer.\"\"\"\n",
        "\n",
        "    # TODO: make pass and average layers\n",
        "    if layer == \"pass\":\n",
        "        return Passthrough()\n",
        "    elif layer == 'ff':\n",
        "        return FFEncoder(in_features, 2 * hidden_size, hidden_sizes=[hidden_size], aggregator='sum')\n",
        "    elif layer == \"lstm\":\n",
        "        return LSTMEncoder(in_features, hidden_size,\n",
        "                           bidirectional=bidirectional)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown layer\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kY8LZiMN5CHW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sentiment Classification with Latent Rationale\n",
        "\n",
        "A latent rationale is a compact and informative fragment of the input based on which a NN classifier makes its decisions. [Lei et al (2016)](http://aclweb.org/anthology/D16-1011) proposed to induce such rationales along with a regression model for multi-aspect sentiment analsysis, their model is trained via REINFORCE on a dataset of beer reviews.\n",
        "\n",
        "*Remark:* the model we will develop here can be seen as a probabilistic version of their model. The rest of this notebook focus on our own probabilitisc view of the model.\n",
        "\n",
        "The picture below depicts our latent-variable model for rationale extraction:\n",
        "\n",
        "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/rationale.png\"  height=\"200\">\n",
        "\n",
        "where we augment the model with a collection of latent variables $z = \\langle z_1, \\ldots, z_n\\rangle$ where $z_i$ is a binary latent variable. Each latent variable $z_i$ regulates whether or not the input $x_i$ is available to the classifier.  We use $x \\odot z$ to denote the selected words, which, in the terminology of Lei et al, is a latent rationale.\n",
        "\n",
        "Again the classifier parameterises a Categorical distribution over $K=5$ outcomes, though this time it can encode only a selection of the input:\n",
        "\n",
        "\\begin{align}\n",
        "    Z_i & \\sim \\text{Bern}(p_1) \\\\\n",
        "    Y|z,x &\\sim \\text{Cat}(f(x \\odot z; \\theta))\n",
        "\\end{align}\n",
        "\n",
        "where we have a shared and fixed Bernoulli prior (with parameter $p_1$) for all $n$ latent variables.\n",
        "\n",
        "Here is an example design for $f$:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf x_i &= z_i \\, \\text{glove}(x_i) \\\\\n",
        "\\mathbf t_1^n, \\mathbf h &= \\text{encoder}(\\mathbf x_1^n; \\theta_{\\text{enc}}) \\\\\n",
        "f(x \\odot z; \\theta) &= \\text{softmax}(\\text{dense}_K(\\mathbf h; \\theta_{\\text{output}}))\n",
        "\\end{align}\n",
        "\n",
        "where:\n",
        "* $z_i$ either leaves $\\mathbf x_i$ unchanged or turns it into a vector of zeros;\n",
        "* the encoder only sees features from selected inputs, i.e. $x_i$ for which $z_i = 1$;\n",
        "* $\\text{dense}_K$ is a linear layer with $K=5$ outputs.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hDHNxLHMWvY-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prior\n",
        "\n",
        "\n",
        "Our prior is a Bernoulli with fixed parameter $0 < p_1 < 1$:\n",
        "\n",
        "\\begin{align}\n",
        "Z_i & \\sim \\text{Bern}(p_1)\n",
        "\\end{align}\n",
        "\n",
        "As we will be using Bernoulli priors and posteriors, it is a good idea to implement a Bernoulli class:"
      ]
    },
    {
      "metadata": {
        "id": "3uPgpyMSWvZA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.distributions import Bernoulli as PyTBernoulli\n",
        "\n",
        "class Bernoulli:\n",
        "    \"\"\"\n",
        "    This class encapsulates a collection of Bernoulli distributions. \n",
        "    Each Bernoulli is uniquely specified by p_1, where\n",
        "        Bernoulli(X=x|p_1) = pow(p_1, x) + pow(1 - p_1, 1 - x)\n",
        "    is the Bernoulli probability mass function (pmf).    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, logits=None, probs=None):\n",
        "        \"\"\"\n",
        "        We can specify a Bernoulli distribution via a logit or a probability. \n",
        "         You need to specify at least one, and if you specify both, beware that\n",
        "         in this implementation logits will be used.\n",
        "         \n",
        "        Recall that: probs = sigmoid(logits).\n",
        "         \n",
        "        :param logits: a tensor of logits (a logit is defined as log (p_1/p_0))\n",
        "            where p_0 = 1 - p_1\n",
        "        :param probs: a tensor of probabilities, each in (0, 1)\n",
        "        \n",
        "        \"\"\"        \n",
        "        if probs is None and logits is None:\n",
        "            raise ValueError('I need probabilities or logits')        \n",
        "        if logits is None:            \n",
        "            self.probs = probs\n",
        "        else:\n",
        "            #self._bernoulli = PyTBernoulli(logits=logits)\n",
        "            self.probs = torch.sigmoid(logits)\n",
        "    \n",
        "    def sample(self):\n",
        "        \"\"\"Returns a sample with the same shape as the parameters\"\"\"\n",
        "        #return self._bernoulli.sample([1]).squeeze(0)\n",
        "        #print('<0', (self.probs < 0).sum(), '>1', (self.probs > 1).sum())\n",
        "        return torch.bernoulli(self.probs)\n",
        "    \n",
        "    def log_pmf(self, x):\n",
        "        \"\"\"\n",
        "        Assess the log probability of a sample. \n",
        "        :param x: either a single sample (0 or 1) or a tensor of samples with the same shape as the parameters.\n",
        "        :returns: tensor with log probabilities with the same shape as parameters\n",
        "            (if the input is a single sample we broadcast it to the shape of the parameters)\n",
        "        \"\"\"\n",
        "        # x * torch.log(self.probs) + (1 - x) * torch.log(1. - self.probs)\n",
        "        return torch.where(x == 1., torch.log(self.probs), torch.log(1. - self.probs))\n",
        "    \n",
        "    def kl(self, other: 'Bernoulli'):\n",
        "        \"\"\"\n",
        "        Compute the KL divergence between two Bernoulli distributions (from self to other).\n",
        "        \n",
        "        :return: KL[self||other] with same shape parameters\n",
        "        \"\"\"\n",
        "        p1 = self.probs\n",
        "        p0 = 1. - self.probs\n",
        "        q1 = other.probs\n",
        "        q0 = 1. - other.probs        \n",
        "        return p1 * (torch.log(p1) - torch.log(q1)) + p0 * (torch.log(p0) - torch.log(q0))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "spUk6DH2WvZL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u0yfkCZlWvZP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classifier\n",
        "\n",
        "The classifier encodes only a selection of the input, which we denote $x \\odot z$, and parameterises a Categorical distribution over $5$ outcomes (sentiment levels).\n",
        "\n",
        "Thus let's implement a Categorical distribution (we will only need to be able to assess its lgo pmf):"
      ]
    },
    {
      "metadata": {
        "id": "IWygoUSFWvZQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Categorical:\n",
        "    \n",
        "    def __init__(self, log_probs):\n",
        "        # [B, K]: class probs\n",
        "        self.log_probs = log_probs\n",
        "        \n",
        "    def log_pmf(self, y):\n",
        "        \"\"\"\n",
        "        :param y: [B] integers\n",
        "        \"\"\"\n",
        "        return torch.gather(self.log_probs, 1, y.unsqueeze(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CdrM_YRI8xBF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "and a classifier architecture:"
      ]
    },
    {
      "metadata": {
        "id": "RAPFOoK6WvZT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    \"\"\"\n",
        "    The Encoder takes an input text (and rationale z) and computes p(y|x,z)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 embed:        nn.Embedding = None,\n",
        "                 hidden_size:  int = 200,\n",
        "                 output_size:  int = 1,\n",
        "                 dropout:      float = 0.1,\n",
        "                 layer:        str = \"pass\",\n",
        "                 ):\n",
        "\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        emb_size = embed.weight.shape[1]\n",
        "        enc_size = hidden_size * 2\n",
        "        self.embed_layer = nn.Sequential(\n",
        "            embed,\n",
        "            nn.Dropout(p=dropout)\n",
        "        )\n",
        "\n",
        "        self.enc_layer = get_encoder(layer, emb_size, hidden_size)\n",
        "\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(enc_size, output_size),\n",
        "            nn.LogSoftmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        self.report_params()\n",
        "\n",
        "    def report_params(self):\n",
        "        count = 0\n",
        "        for name, p in self.named_parameters():\n",
        "            if p.requires_grad and \"embed\" not in name:\n",
        "                count += np.prod(list(p.shape))\n",
        "        print(\"{} #params: {}\".format(self.__class__.__name__, count))\n",
        "\n",
        "    def forward(self, x, mask, z) -> Categorical:\n",
        "\n",
        "        rnn_mask = mask\n",
        "        emb = self.embed_layer(x)\n",
        "\n",
        "        # [B, T]\n",
        "        rnn_mask = z > 0.\n",
        "        # [B, T, 1]\n",
        "        z_mask = z.unsqueeze(-1).float()\n",
        "        # [B, T, E]\n",
        "        emb = emb * z_mask\n",
        "\n",
        "        lengths = mask.long().sum(1)\n",
        "\n",
        "        # encode the sentence\n",
        "        _, final = self.enc_layer(emb, rnn_mask, lengths)\n",
        "\n",
        "        # predict sentiment from final state(s)\n",
        "        log_probs = self.output_layer(final)        \n",
        "        return Categorical(log_probs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p2waCCBF9MaH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "\n",
        "Computing the log-likelihood of an observation requires marginalising over assignments over $z$:\n",
        "\n",
        "\\begin{align}\n",
        "P(y|x,\\theta,p_1) &= \\sum_{z_1 = 0}^1 \\cdots \\sum_{z_n=0}^1 P(z|p_1)\\times P(y|x,z, \\theta) \\\\\n",
        "&= \\sum_{z_1 = 0}^1 \\cdots \\sum_{z_n=0}^1 \\left( \\prod_{i=1}^n \\text{Bern}(z_i|p_1)\\right) \\times \\text{Cat}(y|f(x \\odot z; \\theta)) \n",
        "\\end{align}\n",
        "\n",
        "This is clearly intractable: there are $2^n$ possible assignments to $z$ and because the classifier conditions on all latent selectors, there's no way to simplify the expression.\n",
        "\n",
        "We will avoid computing this intractable marginal by instead employing an independently parameterised inference model.\n",
        "This inference model $Q(z|x, y, \\lambda)$ is an approximation to the true postrerior $P(z|x, y, \\theta, p_1)$, and we use $\\lambda$ to denote its parameters.\n"
      ]
    },
    {
      "metadata": {
        "id": "0jcVdYTg8Wun",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We make a *mean field* assumption, whereby we model latent variables independently given the input:\n",
        "\\begin{align}\n",
        "Q(z|x, y, \\lambda) \n",
        "    &= \\prod_{i=1}^{n} Q(z_i|x; \\lambda) \\\\\n",
        "    &= \\prod_{i=1}^{n} \\text{Bern}(z_i|g_i(x; \\lambda)) \n",
        "\\end{align}\n",
        "\n",
        "where $g(x; \\lambda)$ is a NN that maps from $x = \\langle x_1, \\ldots, x_n\\rangle$ to $n$ Bernoulli parameters, each of which, is a probability value (thus $0 < g_i(x; \\lambda) < 1$).\n",
        "\n",
        "Note that though we could condition on $y$ for approximate posterior inference, we are opportunistically leaving it out. This way, $Q$ is directly available at test time for making predictions. The figure below is a graphical depiction of the inference model (we show a dashed arrow from $y$ to $z$ to remind you that in principle the label is also available).\n",
        "\n",
        "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/inference.png\"  height=\"200\">\n",
        "\n",
        "Here is an example design for $g$:\n",
        "\\begin{align}\n",
        "\\mathbf x_i &= \\text{glove}(x_i) \\\\\n",
        "\\mathbf t_1^n, \\mathbf h &= \\text{encoder}(\\mathbf x_1^n; \\lambda_{\\text{enc}}) \\\\\n",
        "g_i(x; \\lambda) &= \\sigma(\\text{dense}_1(\\mathbf t_i; \\lambda_{\\text{output}}))\n",
        "\\end{align}\n",
        "where\n",
        "* $\\text{glove}$ is a pre-trained embedding function;\n",
        "* $\\text{dense}_1$ is a dense layer with a single output;\n",
        "* and $\\sigma(\\cdot)$ is the sigmoid function, necessary to parameterise a Bernoulli distribution.\n",
        "\n",
        "Here we implement this product of Bernoulli distributions:"
      ]
    },
    {
      "metadata": {
        "id": "0is57kbz8awl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ProductOfBernoullis(nn.Module):\n",
        "    \"\"\"\n",
        "    This is an inference network that parameterises independent Bernoulli distributions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 embed:       nn.Embedding,\n",
        "                 hidden_size: int = 200,\n",
        "                 dropout:     float = 0.1,\n",
        "                 layer:       str = \"lstm\"\n",
        "                 ):\n",
        "\n",
        "        super(ProductOfBernoullis, self).__init__()\n",
        "\n",
        "        emb_size = embed.weight.shape[1]\n",
        "        enc_size = hidden_size * 2\n",
        "\n",
        "        self.embed_layer = nn.Sequential(embed)\n",
        "        self.enc_layer = get_encoder(layer, emb_size, hidden_size)\n",
        "        self.logit_layer = nn.Linear(enc_size, 1, bias=True)\n",
        "        \n",
        "        self.report_params()\n",
        "\n",
        "    def report_params(self):\n",
        "        count = 0\n",
        "        for name, p in self.named_parameters():\n",
        "            if p.requires_grad and \"embed\" not in name:\n",
        "                count += np.prod(list(p.shape))\n",
        "        print(\"{} #params: {}\".format(self.__class__.__name__, count))\n",
        "\n",
        "    def forward(self, x, mask) -> Bernoulli:\n",
        "        \"\"\"\n",
        "        It takes a tensor of tokens (integers)\n",
        "         and predicts a Bernoulli distribution for each position.\n",
        "        \n",
        "        :param x: [B, T]\n",
        "        :param mask: [B, T]\n",
        "        :returns: Bernoulli\n",
        "        \"\"\"\n",
        "\n",
        "        # encode sentence\n",
        "        # [B]\n",
        "        lengths = mask.long().sum(1)\n",
        "        # [B, T, E]\n",
        "        emb = self.embed_layer(x)  \n",
        "        # [B, T, d]\n",
        "        h, _ = self.enc_layer(emb, mask, lengths)\n",
        "\n",
        "        # compute parameters for Bernoulli p(z|x)\n",
        "        # [B, T, 1] Bernoulli distributions\n",
        "        logits = self.logit_layer(h)\n",
        "        # [B, T]\n",
        "        logits = logits.squeeze(-1)\n",
        "        return Bernoulli(logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fcCu7vkKWvZX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Parameter Estimation\n",
        "\n",
        "In variational inference, our objective is to maximise the *evidence lowerbound* (ELBO):\n",
        "\n",
        "\\begin{align}\n",
        "\\log P(y|x) &\\ge \\mathbb E_{Q(z|x, y, \\lambda)}\\left[ \\log P(y|x, z, \\theta, p_1) \\right] - \\text{KL}(Q(z|x, y, \\lambda) || P(z|p_1)) \\\\\n",
        "&\\overset{\\text{MF}}{=}\\mathbb E_{Q(z|x, y, \\lambda)}\\left[ \\log P(y|x, z, \\theta, p_1) \\right] - \\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1)) \n",
        "\\end{align}\n",
        "\n",
        "where the *mean field* assumption we made implies that the KL term is simply a sum of KL divergences from a Bernoulli posterior to a Bernoulli prior.\n",
        "\n",
        "Note that the ELBO remains intractable, namely, solving the expectation in closed form still requires $2^n$ evaluations of the classifier network. Though unlike the true posterior, $Q(z|x,y,\\lambda)$ is known and can be used to obtain gradient estimates based on samples.\n",
        "\n",
        "### Gradient of the classifier network\n",
        "\n",
        "For the classifier, we encounter no problem:\n",
        "\n",
        "\\begin{align}\n",
        "&\\nabla_\\theta\\sum_{z} Q(z|x, \\lambda)\\log P(y|x,z,\\theta) \\\\\n",
        "&=\\sum_{z} Q(z|x, \\lambda)\\nabla_\\theta\\log P(y|x,z,\\theta) \\\\\n",
        "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[\\nabla_\\theta\\log P(y|x,z,\\theta) \\right] \\\\\n",
        "&\\overset{\\text{MC}}{\\approx} \\frac{1}{J} \\sum_{j=1}^J \\nabla_\\theta \\log P(y|x, z^{(j)}, \\theta) \n",
        "\\end{align}\n",
        "where $z^{(j)} \\sim Q(z|x,\\lambda)$.\n",
        "\n",
        "\n",
        "### Gradient of the inference network\n",
        "\n",
        "For the inference model, we have to use the *score function estimator* (a.k.a. REINFORCE):\n",
        "\n",
        "\\begin{align}\n",
        "&\\nabla_\\lambda\\sum_{z} Q(z|x, \\lambda)\\log P(y|x,z,\\theta)\\\\\n",
        "&=\\sum_{z} \\nabla_\\lambda Q(z|x, \\lambda)\\log P(y|x,z,\\theta) \\\\\n",
        "&=\\sum_{z}  \\underbrace{Q(z|x, \\lambda) \\nabla_\\lambda \\log Q(z|x, \\lambda)}_{\\nabla_\\lambda Q(z|x, \\lambda)} \\log P(y|x,z,\\theta) \\\\\n",
        "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[ \\log P(y|x,z,\\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda) \\right] \\\\\n",
        "&\\overset{\\text{MC}}{\\approx} \\frac{1}{J} \\sum_{j=1}^J  \\log P(y|x, z^{(j)}, \\theta) \\nabla_\\lambda \\log Q(z^{(j)}|x, \\lambda) \n",
        "\\end{align}\n",
        "\n",
        "where $z^{(j)} \\sim Q(z|x,\\lambda)$."
      ]
    },
    {
      "metadata": {
        "id": "6cdfkOYdC0LQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementation\n",
        "\n",
        "Let's implement the model and the loss (negative ELBO):"
      ]
    },
    {
      "metadata": {
        "id": "b2DxK0_GWvZY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import softplus\n",
        "#from discrete.util import get_z_stats\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Reimplementation of Lei et al. (2016). Rationalizing Neural Predictions\n",
        "    for Stanford Sentiment.\n",
        "    (Does classfication instead of regression.)\n",
        "\n",
        "    Consists of:\n",
        "    - Encoder that computes p(y | x, z)\n",
        "    - Generator that computes p(z | x) independently or dependently with an RNN.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab:       object = None,\n",
        "                 vocab_size:  int = 0,\n",
        "                 emb_size:    int = 200,\n",
        "                 hidden_size: int = 200,\n",
        "                 output_size: int = 1,\n",
        "                 prior_p1:    float = 0.1,\n",
        "                 dropout:     float = 0.1,\n",
        "                 layer_cls:   str = 'pass',\n",
        "                 layer_inf:   str = 'lstm',\n",
        "                 ):\n",
        "\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.embed = embed = nn.Embedding(vocab_size, emb_size, padding_idx=1)\n",
        "\n",
        "        # TODO: rename to obs_model\n",
        "        self.cls_net = Classifier(\n",
        "            embed=embed, hidden_size=hidden_size, output_size=output_size,\n",
        "            dropout=dropout, layer=layer_cls)\n",
        "        \n",
        "        # TODO: rename to q_z\n",
        "        self.inference_net = ProductOfBernoullis(\n",
        "            embed=embed, hidden_size=hidden_size,\n",
        "            dropout=dropout, layer=layer_inf)\n",
        "        \n",
        "        self.prior_p1 = prior_p1\n",
        "\n",
        "    def predict(self, py, **kwargs):\n",
        "        \"\"\"\n",
        "        Predict deterministically.\n",
        "        :param x:\n",
        "        :return: predictions, optional (dict with optional statistics)\n",
        "        \"\"\"\n",
        "        assert not self.training, \"should be in eval mode for prediction\"\n",
        "        return py.log_probs.argmax(-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Generate a sequence of zs with the Generator.\n",
        "        Then predict with sentence x (zeroed out with z) using Encoder.\n",
        "\n",
        "        :param x: [B, T] (that is, batch-major is assumed)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        mask = (x != 1)  # [B,T]\n",
        "\n",
        "        qz = self.inference_net(x, mask)\n",
        "\n",
        "        if self.training:  # sample\n",
        "            # [B, T]\n",
        "            z = qz.sample()\n",
        "        else:  # deterministic\n",
        "            # [B, T]\n",
        "            # TODO: consider this\n",
        "            z = (qz.probs >= 0.5).float()\n",
        "            #z = qz.sample()\n",
        "            \n",
        "        z = torch.where(mask, z, torch.zeros_like(z))\n",
        "        \n",
        "        py = self.cls_net(x, mask, z)\n",
        "        return py, qz, z\n",
        "\n",
        "    def get_loss(self, py, targets, \n",
        "                 q_z: Bernoulli, \n",
        "                 z, \n",
        "                 mask=None,\n",
        "                 iter_i=0, \n",
        "                 kl_weight=1.0,\n",
        "                 min_kl=0.0,\n",
        "                 ll_mean=0.,\n",
        "                 ll_std=1.,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        This computes the loss for the whole model.\n",
        "        We stick to the variable names of the original code as much as\n",
        "        possible.\n",
        "\n",
        "        :param logits:\n",
        "        :param targets:\n",
        "        :param sparsity:\n",
        "        :param coherent:\n",
        "        :param mask:\n",
        "        :param kwargs:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        assert mask is not None, \"provide mask\"\n",
        "\n",
        "        lengths = mask.sum(1).float()\n",
        "        batch_size = mask.size(0)\n",
        "        terms = OrderedDict()\n",
        "\n",
        "        # shape: [B]\n",
        "        # log p(y|x,z) where z ~ q\n",
        "        #one_hot_target = (targets.unsqueeze(-1) == torch.arange(5, device=device).reshape(1, 5)).float()            \n",
        "        #ll = torch.sum(py.log_probs * one_hot_target, dim=-1)\n",
        "        # [B]\n",
        "        ll = py.log_pmf(targets)\n",
        "        \n",
        "        # KL(q||p)\n",
        "        # [B, T]\n",
        "        #prior_p1 = self.prior_p1\n",
        "        #p_z = Bernoulli(probs=torch.full_like(q_z.probs, self.prior_p1))\n",
        "        prior_p1 = np.random.beta(0.5, 0.5)\n",
        "        p_z = Bernoulli(probs=torch.full_like(q_z.probs, prior_p1))\n",
        "        kl = q_z.kl(p_z)\n",
        "        kl = torch.where(mask, kl, torch.zeros_like(kl))\n",
        "                \n",
        "        # Compute the log density of the sample\n",
        "        # [B, T]\n",
        "        log_q_z = q_z.log_pmf(z)\n",
        "        log_q_z = torch.where(mask, log_q_z, torch.zeros_like(log_q_z))\n",
        "        # We have independent Bernoullis, thus we just sum their log probabilities\n",
        "        # [B]\n",
        "        log_q_z = log_q_z.sum(1)\n",
        "        \n",
        "        # surrogate objective for score function estimator\n",
        "        # [B]\n",
        "        reward = (ll.detach() - torch.full_like(ll, ll_mean)) / torch.full_like(ll, ll_std)\n",
        "        sf_surrogate = (reward * log_q_z)\n",
        "\n",
        "        # Make terms in the ELBO\n",
        "        # []\n",
        "        ll = ll.mean()\n",
        "        sf_surrogate = sf_surrogate.mean()\n",
        "        # KL may require annealing and free-bits\n",
        "        # [B]\n",
        "        kl = kl.sum(dim=-1)\n",
        "        kl_fb = torch.max(torch.full_like(kl, min_kl), kl)\n",
        "        # []\n",
        "        kl = kl.mean() \n",
        "        kl_fb = kl_fb.mean() \n",
        "        kl_fb = kl_fb * kl_weight\n",
        "        \n",
        "        terms['elbo'] = (ll - kl_fb).item()\n",
        "        terms['ll'] = ll.item()\n",
        "        terms['kl_fb'] = kl_fb.item()\n",
        "        terms['kl'] = kl.item()\n",
        "        terms['kl_weight'] = kl_weight\n",
        "        terms['sf'] = sf_surrogate.item()\n",
        "        terms['reward'] = reward.mean().item()\n",
        "        terms['ll_mean'] = ll_mean\n",
        "        terms['ll_std'] = ll_std\n",
        "        terms['selected'] = (z.sum(1) / lengths).mean().item()\n",
        "        terms['prior_p1'] = prior_p1\n",
        "        terms['avg_p1'] = (torch.where(mask, q_z.probs, torch.zeros_like(q_z.probs)).sum() / mask.sum().float()).item()\n",
        "        # TODO log min and max p1 in batch (mask properly)\n",
        "        return - ll - sf_surrogate + kl_fb, terms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wNQDXTpqWvZa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "\n",
        "class MovingStats:\n",
        "    \n",
        "    def __init__(self, memory=-1):\n",
        "        self.data = deque([])\n",
        "        self.memory = memory\n",
        "        \n",
        "    def append(self, value):\n",
        "        if self.memory != 0:\n",
        "            if self.memory > 0 and len(self.data) == self.memory:\n",
        "                self.data.popleft()\n",
        "            self.data.append(value)\n",
        "        \n",
        "    def mean(self):\n",
        "        if len(self.data):\n",
        "            return np.mean([x for x in self.data])\n",
        "        else:\n",
        "            return 0.\n",
        "    \n",
        "    def std(self):\n",
        "        return 1.  # np.std(self.data) if len(self.data) > 1 else 1.\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "081YSfU9WvZc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ]
    },
    {
      "metadata": {
        "id": "9Pc80gseWvZd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sst.util import make_kv_string, get_minibatch, prepare_minibatch, print_parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_WVr97kilIRV",
        "colab_type": "code",
        "outputId": "4ca5a92b-b713-491a-de09-278f552c2417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "cell_type": "code",
      "source": [
        "from sst.sstutil import examplereader, Vocabulary, load_glove\n",
        "from collections import OrderedDict\n",
        "import torch.optim\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import time\n",
        "from sst.evaluate import evaluate\n",
        "\n",
        "\n",
        "cfg = dict()\n",
        "\n",
        "# Data\n",
        "cfg['training_path'] = \"sst/data/sst/train.txt\"\n",
        "cfg['dev_path'] = \"sst/data/sst/dev.txt\"\n",
        "cfg['test_path'] = \"sst/data/sst/test.txt\"\n",
        "cfg['word_vectors'] = 'sst/data/sst/glove.840B.300d.filtered.txt'\n",
        "# Architecture\n",
        "cfg['num_iterations'] = -20  # use negative for epochs and positive for iterations\n",
        "cfg['print_every'] = 100\n",
        "cfg['eval_every'] = -1\n",
        "cfg['batch_size'] = 25\n",
        "cfg['eval_batch_size'] = 25\n",
        "cfg['subphrases'] = False\n",
        "cfg['min_phrase_length'] = 2\n",
        "cfg['lowercase'] = True\n",
        "cfg['fix_emb'] = True\n",
        "cfg['embed_size'] = 300\n",
        "cfg['hidden_size'] = 150\n",
        "cfg['num_layers'] = 1\n",
        "cfg['dropout'] = 0.5\n",
        "cfg['layer_inf'] = 'pass'\n",
        "cfg['layer_cls'] = 'pass'\n",
        "cfg['save_path'] = 'data/results'\n",
        "cfg['baseline_memory'] = 1000\n",
        "cfg['prior_p1'] = 0.3\n",
        "cfg['min_kl'] = 0.  # use more than 0 to enable free bits\n",
        "cfg['kl_weight'] = 1.  # start from zero to enable annealing\n",
        "cfg['kl_inc'] = 0.00001  \n",
        "# Optimiser\n",
        "cfg['lr'] = 0.0002\n",
        "cfg['weight_decay'] = 1e-5\n",
        "cfg['lr_decay'] = 0.5\n",
        "cfg['patience'] = 5\n",
        "cfg['cooldown'] = 5\n",
        "cfg['threshold'] = 1e-4\n",
        "cfg['min_lr'] = 1e-5\n",
        "cfg['max_grad_norm'] = 5.\n",
        "\n",
        "\n",
        "print('# Configuration')\n",
        "for k, v in cfg.items():\n",
        "    print(\"{:20} : {:10}\".format(k, v))\n",
        "    \n",
        "# Let's load the data into memory.\n",
        "print(\"Loading data\")\n",
        "train_data = list(examplereader(\n",
        "    cfg['training_path'],\n",
        "    lower=cfg['lowercase'], \n",
        "    subphrases=cfg['subphrases'],\n",
        "    min_length=cfg['min_phrase_length']))\n",
        "dev_data = list(examplereader(cfg['dev_path'], lower=cfg['lowercase']))\n",
        "test_data = list(examplereader(cfg['test_path'], lower=cfg['lowercase']))\n",
        "\n",
        "print(\"train\", len(train_data))\n",
        "print(\"dev\", len(dev_data))\n",
        "print(\"test\", len(test_data))\n",
        "\n",
        "iters_per_epoch = len(train_data) // cfg[\"batch_size\"]\n",
        "\n",
        "if cfg[\"eval_every\"] == -1:\n",
        "    eval_every = iters_per_epoch\n",
        "    print(\"Set eval_every to {}\".format(iters_per_epoch))\n",
        "\n",
        "if cfg[\"num_iterations\"] < 0:\n",
        "    num_iterations = iters_per_epoch * -1 * cfg[\"num_iterations\"]\n",
        "    print(\"Set num_iterations to {}\".format(num_iterations))\n",
        "\n",
        "print('\\n# Example')\n",
        "example = dev_data[0]\n",
        "print(\"First dev example:\", example)\n",
        "print(\"First dev example tokens:\", example.tokens)\n",
        "print(\"First dev example label:\", example.label)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Configuration\n",
            "training_path        : sst/data/sst/train.txt\n",
            "dev_path             : sst/data/sst/dev.txt\n",
            "test_path            : sst/data/sst/test.txt\n",
            "word_vectors         : sst/data/sst/glove.840B.300d.filtered.txt\n",
            "num_iterations       :        -20\n",
            "print_every          :        100\n",
            "eval_every           :         -1\n",
            "batch_size           :         25\n",
            "eval_batch_size      :         25\n",
            "subphrases           :          0\n",
            "min_phrase_length    :          2\n",
            "lowercase            :          1\n",
            "fix_emb              :          1\n",
            "embed_size           :        300\n",
            "hidden_size          :        150\n",
            "num_layers           :          1\n",
            "dropout              :        0.5\n",
            "layer_inf            : pass      \n",
            "layer_cls            : pass      \n",
            "save_path            : data/results\n",
            "baseline_memory      :       1000\n",
            "prior_p1             :        0.3\n",
            "min_kl               :        0.0\n",
            "kl_weight            :        1.0\n",
            "kl_inc               :      1e-05\n",
            "lr                   :     0.0002\n",
            "weight_decay         :      1e-05\n",
            "lr_decay             :        0.5\n",
            "patience             :          5\n",
            "cooldown             :          5\n",
            "threshold            :     0.0001\n",
            "min_lr               :      1e-05\n",
            "max_grad_norm        :        5.0\n",
            "Loading data\n",
            "train 8544\n",
            "dev 1101\n",
            "test 2210\n",
            "Set eval_every to 341\n",
            "Set num_iterations to 6820\n",
            "\n",
            "# Example\n",
            "First dev example: Example(tokens=['it', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'buy', 'and', 'accorsi', '.'], label=3, transitions=[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1], token_labels=[2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2])\n",
            "First dev example tokens: ['it', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'buy', 'and', 'accorsi', '.']\n",
            "First dev example label: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "6PMqtVj0WvZf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    \n",
        "    vocab = Vocabulary()  # populated by load_glove\n",
        "    glove_path = cfg[\"word_vectors\"]\n",
        "    vectors = load_glove(glove_path, vocab)\n",
        "\n",
        "    #writer = SummaryWriter(log_dir=cfg[\"save_path\"])\n",
        "\n",
        "    # Map the sentiment labels 0-4 to a more readable form (and the opposite)\n",
        "    i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
        "    t2i = OrderedDict({p: i for p, i in zip(i2t, range(len(i2t)))})\n",
        "\n",
        "\n",
        "    print('\\n# Constructing model')\n",
        "    model = Model(\n",
        "        vocab_size=len(vocab.w2i), \n",
        "        emb_size=cfg[\"embed_size\"],\n",
        "        hidden_size=cfg[\"hidden_size\"], \n",
        "        output_size=len(t2i),\n",
        "        prior_p1=cfg['prior_p1'],\n",
        "        vocab=vocab, \n",
        "        dropout=cfg[\"dropout\"], \n",
        "        layer_cls=cfg[\"layer_cls\"],\n",
        "        layer_inf=cfg[\"layer_inf\"])\n",
        "\n",
        "    print('\\n# Loading embeddings')\n",
        "    with torch.no_grad():\n",
        "        model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "        if cfg[\"fix_emb\"]:\n",
        "            print(\"fixed word embeddings\")\n",
        "            model.embed.weight.requires_grad = False\n",
        "        model.embed.weight[1] = 0.  # padding zero\n",
        "\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=cfg[\"lr\"],\n",
        "                     weight_decay=cfg[\"weight_decay\"])\n",
        "\n",
        "    scheduler = ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", factor=cfg[\"lr_decay\"], patience=cfg[\"patience\"],\n",
        "        verbose=True, cooldown=cfg[\"cooldown\"], threshold=cfg[\"threshold\"],\n",
        "        min_lr=cfg[\"min_lr\"])\n",
        "\n",
        "    iter_i = 0\n",
        "    train_loss = 0.\n",
        "    print_num = 0\n",
        "    start = time.time()\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    best_eval = 1.0e9\n",
        "    best_iter = 0\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # print model\n",
        "    print(model)\n",
        "    print_parameters(model)\n",
        "\n",
        "    batch_size = cfg['batch_size']\n",
        "    eval_batch_size = cfg['eval_batch_size']\n",
        "    print_every = cfg['print_every']\n",
        "\n",
        "    kl_inc = cfg['kl_inc']\n",
        "    kl_weight = cfg['kl_weight']\n",
        "    min_kl = cfg['min_kl']\n",
        "    ll_moving_stats = MovingStats(cfg['baseline_memory'])\n",
        "\n",
        "    while True:  # when we run out of examples, shuffle and continue\n",
        "        for batch in get_minibatch(train_data, batch_size=batch_size, shuffle=True):\n",
        "\n",
        "            epoch = iter_i // iters_per_epoch\n",
        "\n",
        "            # forward pass\n",
        "            model.train()\n",
        "            x, targets, _ = prepare_minibatch(batch, model.vocab, device=device)\n",
        "\n",
        "            # with autograd.detect_anomaly():\n",
        "\n",
        "            py, q_z, z = model(x)\n",
        "\n",
        "            mask = (x != 1)\n",
        "            # \"KL annealing\"\n",
        "            kl_weight += kl_inc\n",
        "            if kl_weight > 1.:\n",
        "                kl_weight = 1.0\n",
        "                \n",
        "            loss, terms = model.get_loss(\n",
        "                py, \n",
        "                targets, \n",
        "                q_z=q_z,\n",
        "                z=z,\n",
        "                mask=mask, \n",
        "                kl_weight=kl_weight,\n",
        "                min_kl=min_kl,\n",
        "                ll_mean=ll_moving_stats.mean(),\n",
        "                ll_std=ll_moving_stats.std(),\n",
        "                iter_i=iter_i)\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            ll_moving_stats.append(terms['ll'])\n",
        "\n",
        "            # backward pass\n",
        "            model.zero_grad()  # erase previous gradients\n",
        "\n",
        "            loss.backward()  # compute new gradients\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=cfg['max_grad_norm'])\n",
        "\n",
        "            # update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            print_num += 1\n",
        "            iter_i += 1\n",
        "\n",
        "            # print info\n",
        "            if iter_i % print_every == 0:\n",
        "\n",
        "                train_loss = train_loss / print_every\n",
        "                #writer.add_scalar('data/train_loss', train_loss, iter_i)\n",
        "                #for k, v in loss_optional.items():\n",
        "                #    writer.add_scalar('data/'+k, v, iter_i)\n",
        "\n",
        "                print_str = make_kv_string(terms)\n",
        "                print(\"Epoch %r Iter %r loss=%.4f %s\" %\n",
        "                      (epoch, iter_i, train_loss, print_str))\n",
        "                losses.append(train_loss)\n",
        "                print_num = 0\n",
        "                train_loss = 0.\n",
        "\n",
        "            # evaluate\n",
        "            if iter_i % eval_every == 0:\n",
        "\n",
        "                dev_eval, rationales = evaluate(\n",
        "                    model, dev_data, \n",
        "                    batch_size=eval_batch_size, \n",
        "                    device=device,\n",
        "                    cfg=cfg, iter_i=iter_i)\n",
        "                accuracies.append(dev_eval[\"acc\"])\n",
        "\n",
        "                #for k, v in dev_eval.items():\n",
        "                #    writer.add_scalar('data/dev/'+k, v, iter_i)\n",
        "\n",
        "                print(\"\\n# epoch %r iter %r: dev %s\" % (\n",
        "                    epoch, iter_i, make_kv_string(dev_eval)))\n",
        "                \n",
        "                for exid in range(3):\n",
        "                    print(' dev%d [gold=%d,pred=%d]:' % (exid, dev_data[exid].label, rationales[exid][1]),  \n",
        "                          ' '.join(rationales[exid][0]))\n",
        "                print()\n",
        "\n",
        "                #test_eval = evaluate(\n",
        "                #    model, test_data, batch_size=eval_batch_size, device=device,\n",
        "                #    cfg=cfg, iter_i=iter_i)\n",
        "                #for k, v in test_eval.items():\n",
        "                #    writer.add_scalar('data/test/'+k, v, iter_i)\n",
        "\n",
        "                #print(\"# epoch %r iter %r: tst %s\" % (\n",
        "                #    epoch, iter_i, make_kv_string(test_eval)))\n",
        "\n",
        "                # adjust learning rate\n",
        "\n",
        "                scheduler.step(dev_eval[\"loss\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "A5uYKcw-WvZl",
        "colab_type": "code",
        "outputId": "03d60920-8087-4eee-bc56-3acdb8ac0241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5743
        }
      },
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Constructing model\n",
            "Classifier #params: 1505\n",
            "ProductOfBernoullis #params: 301\n",
            "\n",
            "# Loading embeddings\n",
            "fixed word embeddings\n",
            "Model(\n",
            "  (embed): Embedding(20727, 300, padding_idx=1)\n",
            "  (cls_net): Classifier(\n",
            "    (embed_layer): Sequential(\n",
            "      (0): Embedding(20727, 300, padding_idx=1)\n",
            "      (1): Dropout(p=0.5)\n",
            "    )\n",
            "    (enc_layer): Passthrough()\n",
            "    (output_layer): Sequential(\n",
            "      (0): Dropout(p=0.5)\n",
            "      (1): Linear(in_features=300, out_features=5, bias=True)\n",
            "      (2): LogSoftmax()\n",
            "    )\n",
            "  )\n",
            "  (inference_net): ProductOfBernoullis(\n",
            "    (embed_layer): Sequential(\n",
            "      (0): Embedding(20727, 300, padding_idx=1)\n",
            "    )\n",
            "    (enc_layer): Passthrough()\n",
            "    (logit_layer): Linear(in_features=300, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "embed.weight             [20727, 300] requires_grad=False\n",
            "cls_net.output_layer.1.weight [5, 300]     requires_grad=True\n",
            "cls_net.output_layer.1.bias [5]          requires_grad=True\n",
            "inference_net.logit_layer.weight [1, 300]     requires_grad=True\n",
            "inference_net.logit_layer.bias [1]          requires_grad=True\n",
            "\n",
            "Total parameters: 6219906\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 0 Iter 100 loss=18.5538 elbo -7.0856 ll -2.1522 kl_fb 4.9334 kl 4.9334 kl_weight 1.0000 sf -0.7733 reward 0.0559 ll_mean -2.2081 ll_std 1.0000 selected 0.4017 prior_p1 0.1735 avg_p1 0.4762\n",
            "Epoch 0 Iter 200 loss=17.2795 elbo -9.9084 ll -1.4843 kl_fb 8.4241 kl 8.4241 kl_weight 1.0000 sf -7.4901 reward 0.6119 ll_mean -2.0962 ll_std 1.0000 selected 0.4657 prior_p1 0.8816 avg_p1 0.4828\n",
            "Epoch 0 Iter 300 loss=15.5464 elbo -6.8274 ll -1.8287 kl_fb 4.9987 kl 4.9987 kl_weight 1.0000 sf -2.7055 reward 0.1933 ll_mean -2.0220 ll_std 1.0000 selected 0.4861 prior_p1 0.8022 avg_p1 0.4870\n",
            "\n",
            "# epoch 0 iter 341: dev loss -4.5215 elbo -15.3446 ll -1.5949 kl_fb 13.7496 kl 13.7496 kl_weight 1.0000 sf 19.8661 reward -1.5949 ll_mean 0.0000 ll_std 1.0000 selected 0.2215 prior_p1 0.5169 avg_p1 0.4831 acc 0.2525\n",
            " dev0 [gold=3,pred=4]: it **'s** a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
            " dev1 [gold=2,pred=4]: **no** one goes **unindicted** here **,** which is probably for the best .\n",
            " dev2 [gold=3,pred=0]: and if you 're not nearly moved to **tears** by a couple **of** scenes **,** you 've got **ice** **water** in **your** veins .\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 1 Iter 400 loss=16.2136 elbo -2.6167 ll -1.8918 kl_fb 0.7249 kl 0.7249 kl_weight 1.0000 sf -1.1437 reward 0.0855 ll_mean -1.9773 ll_std 1.0000 selected 0.5130 prior_p1 0.3559 avg_p1 0.4874\n",
            "Epoch 1 Iter 500 loss=16.5952 elbo -5.2432 ll -1.7763 kl_fb 3.4668 kl 3.4668 kl_weight 1.0000 sf -1.9510 reward 0.1606 ll_mean -1.9369 ll_std 1.0000 selected 0.4350 prior_p1 0.2203 avg_p1 0.5057\n",
            "Epoch 1 Iter 600 loss=18.8046 elbo -24.4061 ll -1.7753 kl_fb 22.6308 kl 22.6308 kl_weight 1.0000 sf -1.5820 reward 0.1305 ll_mean -1.9058 ll_std 1.0000 selected 0.4930 prior_p1 0.9804 avg_p1 0.4998\n",
            "\n",
            "# epoch 1 iter 682: dev loss -5.6105 elbo -13.9066 ll -1.5595 kl_fb 12.3472 kl 12.3472 kl_weight 1.0000 sf 19.5171 reward -1.5595 ll_mean 0.0000 ll_std 1.0000 selected 0.1917 prior_p1 0.5450 avg_p1 0.4828 acc 0.2870\n",
            " dev0 [gold=3,pred=1]: it 's a lovely film with lovely **performances** by buy and accorsi .\n",
            " dev1 [gold=2,pred=3]: **no** one goes **unindicted** here **,** which is probably for the best .\n",
            " dev2 [gold=3,pred=0]: and if you 're not nearly moved to **tears** by a couple of scenes **,** you 've got **ice** **water** in **your** veins .\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 2 Iter 700 loss=16.1275 elbo -1.7027 ll -1.4546 kl_fb 0.2480 kl 0.2480 kl_weight 1.0000 sf -6.0809 reward 0.4220 ll_mean -1.8766 ll_std 1.0000 selected 0.4833 prior_p1 0.4085 avg_p1 0.4808\n",
            "Epoch 2 Iter 800 loss=18.5782 elbo -9.8008 ll -1.5965 kl_fb 8.2044 kl 8.2044 kl_weight 1.0000 sf -3.3069 reward 0.2537 ll_mean -1.8502 ll_std 1.0000 selected 0.4930 prior_p1 0.1055 avg_p1 0.4742\n",
            "Epoch 2 Iter 900 loss=17.3192 elbo -1.7459 ll -1.6068 kl_fb 0.1391 kl 0.1391 kl_weight 1.0000 sf -3.2984 reward 0.2231 ll_mean -1.8300 ll_std 1.0000 selected 0.4681 prior_p1 0.4210 avg_p1 0.4728\n",
            "Epoch 2 Iter 1000 loss=15.3574 elbo -12.7520 ll -1.6033 kl_fb 11.1487 kl 11.1487 kl_weight 1.0000 sf -2.6976 reward 0.2054 ll_mean -1.8086 ll_std 1.0000 selected 0.4857 prior_p1 0.9058 avg_p1 0.4791\n",
            "\n",
            "# epoch 2 iter 1023: dev loss -5.9724 elbo -13.4599 ll -1.5585 kl_fb 11.9014 kl 11.9014 kl_weight 1.0000 sf 19.4323 reward -1.5585 ll_mean 0.0000 ll_std 1.0000 selected 0.1152 prior_p1 0.4689 avg_p1 0.4794 acc 0.2916\n",
            " dev0 [gold=3,pred=1]: it 's a lovely film with lovely **performances** by buy and accorsi .\n",
            " dev1 [gold=2,pred=3]: **no** one goes **unindicted** here , which is probably for the best .\n",
            " dev2 [gold=3,pred=0]: and if you 're not nearly moved to **tears** by a couple of scenes , you 've got **ice** **water** in your veins .\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 3 Iter 1100 loss=18.4613 elbo -16.9335 ll -1.4907 kl_fb 15.4429 kl 15.4429 kl_weight 1.0000 sf -2.9940 reward 0.2600 ll_mean -1.7506 ll_std 1.0000 selected 0.4311 prior_p1 0.9523 avg_p1 0.4761\n",
            "Epoch 3 Iter 1200 loss=16.8036 elbo -15.1465 ll -1.4757 kl_fb 13.6708 kl 13.6708 kl_weight 1.0000 sf -2.9365 reward 0.2362 ll_mean -1.7118 ll_std 1.0000 selected 0.4349 prior_p1 0.9315 avg_p1 0.4723\n",
            "Epoch 3 Iter 1300 loss=19.0699 elbo -1.4655 ll -1.4285 kl_fb 0.0370 kl 0.0370 kl_weight 1.0000 sf -3.1581 reward 0.2542 ll_mean -1.6827 ll_std 1.0000 selected 0.5170 prior_p1 0.4992 avg_p1 0.4750\n",
            "\n",
            "# epoch 3 iter 1364: dev loss -6.5357 elbo -12.8501 ll -1.5338 kl_fb 11.3163 kl 11.3163 kl_weight 1.0000 sf 19.3858 reward -1.5338 ll_mean 0.0000 ll_std 1.0000 selected 0.1342 prior_p1 0.4332 avg_p1 0.4847 acc 0.3170\n",
            " dev0 [gold=3,pred=4]: it 's a **lovely** film with **lovely** **performances** by buy and accorsi .\n",
            " dev1 [gold=2,pred=3]: **no** one goes **unindicted** here , which is probably for the best .\n",
            " dev2 [gold=3,pred=0]: and if you 're not nearly moved to **tears** by a couple of scenes , you 've got **ice** **water** in your veins .\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 4 Iter 1400 loss=17.1760 elbo -51.2736 ll -1.4975 kl_fb 49.7761 kl 49.7761 kl_weight 1.0000 sf -1.9570 reward 0.1568 ll_mean -1.6543 ll_std 1.0000 selected 0.5654 prior_p1 0.9988 avg_p1 0.4873\n",
            "Epoch 4 Iter 1500 loss=15.2887 elbo -10.2797 ll -1.6577 kl_fb 8.6219 kl 8.6219 kl_weight 1.0000 sf 0.3282 reward -0.0240 ll_mean -1.6337 ll_std 1.0000 selected 0.4732 prior_p1 0.8827 avg_p1 0.5021\n",
            "Epoch 4 Iter 1600 loss=15.0868 elbo -12.0794 ll -1.5635 kl_fb 10.5159 kl 10.5159 kl_weight 1.0000 sf -0.7161 reward 0.0524 ll_mean -1.6159 ll_std 1.0000 selected 0.4788 prior_p1 0.0974 avg_p1 0.5038\n",
            "Epoch 4 Iter 1700 loss=16.7345 elbo -10.1806 ll -1.6685 kl_fb 8.5121 kl 8.5121 kl_weight 1.0000 sf 0.9327 reward -0.0667 ll_mean -1.6018 ll_std 1.0000 selected 0.4778 prior_p1 0.8789 avg_p1 0.5020\n",
            "\n",
            "# epoch 4 iter 1705: dev loss -4.6480 elbo -14.2193 ll -1.4694 kl_fb 12.7499 kl 12.7499 kl_weight 1.0000 sf 18.8673 reward -1.4694 ll_mean 0.0000 ll_std 1.0000 selected 0.5531 prior_p1 0.5614 avg_p1 0.4983 acc 0.3624\n",
            " dev0 [gold=3,pred=1]: **it** **'s** a **lovely** **film** **with** **lovely** **performances** by **buy** and accorsi **.**\n",
            " dev1 [gold=2,pred=3]: **no** **one** goes unindicted here **,** which **is** probably **for** **the** best **.**\n",
            " dev2 [gold=3,pred=1]: and **if** **you** **'re** **not** nearly moved **to** **tears** by a couple **of** scenes **,** **you** 've got **ice** **water** in **your** veins **.**\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 5 Iter 1800 loss=16.4570 elbo -14.8112 ll -1.5391 kl_fb 13.2722 kl 13.2722 kl_weight 1.0000 sf -0.6176 reward 0.0520 ll_mean -1.5911 ll_std 1.0000 selected 0.4820 prior_p1 0.9378 avg_p1 0.4825\n",
            "Epoch 5 Iter 1900 loss=13.7539 elbo -6.4170 ll -1.4665 kl_fb 4.9505 kl 4.9505 kl_weight 1.0000 sf -1.5369 reward 0.1118 ll_mean -1.5784 ll_std 1.0000 selected 0.4845 prior_p1 0.1708 avg_p1 0.4772\n",
            "Epoch 5 Iter 2000 loss=16.2191 elbo -3.2913 ll -1.3643 kl_fb 1.9269 kl 1.9269 kl_weight 1.0000 sf -2.2613 reward 0.2066 ll_mean -1.5709 ll_std 1.0000 selected 0.4874 prior_p1 0.7082 avg_p1 0.4726\n",
            "\n",
            "# epoch 5 iter 2046: dev loss 2.4592 elbo -21.7033 ll -1.5293 kl_fb 20.1740 kl 20.1740 kl_weight 1.0000 sf 19.2441 reward -1.5293 ll_mean 0.0000 ll_std 1.0000 selected 0.1077 prior_p1 0.4576 avg_p1 0.4823 acc 0.3079\n",
            " dev0 [gold=3,pred=3]: it 's a lovely film with lovely **performances** by buy and accorsi .\n",
            " dev1 [gold=2,pred=1]: **no** one goes **unindicted** here , which is probably for the best .\n",
            " dev2 [gold=3,pred=0]: and if you 're not nearly moved to **tears** by a couple of scenes , you 've got **ice** water in your veins .\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 6 Iter 2100 loss=15.4722 elbo -15.4650 ll -1.4495 kl_fb 14.0156 kl 14.0156 kl_weight 1.0000 sf -1.5746 reward 0.1103 ll_mean -1.5598 ll_std 1.0000 selected 0.4899 prior_p1 0.0640 avg_p1 0.4868\n",
            "Epoch 6 Iter 2200 loss=12.5068 elbo -2.3097 ll -1.5671 kl_fb 0.7426 kl 0.7426 kl_weight 1.0000 sf 0.2300 reward -0.0166 ll_mean -1.5505 ll_std 1.0000 selected 0.4387 prior_p1 0.3633 avg_p1 0.4956\n",
            "Epoch 6 Iter 2300 loss=13.9059 elbo -8.7778 ll -1.6254 kl_fb 7.1524 kl 7.1524 kl_weight 1.0000 sf 1.0802 reward -0.0795 ll_mean -1.5460 ll_std 1.0000 selected 0.5061 prior_p1 0.8629 avg_p1 0.5062\n",
            "\n",
            "# epoch 6 iter 2387: dev loss -3.6018 elbo -14.8371 ll -1.4374 kl_fb 13.3997 kl 13.3997 kl_weight 1.0000 sf 18.4390 reward -1.4374 ll_mean 0.0000 ll_std 1.0000 selected 0.6879 prior_p1 0.4713 avg_p1 0.5029 acc 0.3760\n",
            " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** **by** **buy** **and** accorsi **.**\n",
            " dev1 [gold=2,pred=3]: **no** **one** goes unindicted here **,** **which** **is** probably **for** **the** best **.**\n",
            " dev2 [gold=3,pred=1]: **and** **if** **you** **'re** **not** nearly moved **to** **tears** **by** **a** couple **of** **scenes** **,** **you** 've got **ice** **water** **in** **your** veins **.**\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 7 Iter 2400 loss=13.3832 elbo -2.2678 ll -1.4112 kl_fb 0.8566 kl 0.8566 kl_weight 1.0000 sf -1.7482 reward 0.1292 ll_mean -1.5403 ll_std 1.0000 selected 0.5741 prior_p1 0.6476 avg_p1 0.5043\n",
            "Epoch 7 Iter 2500 loss=14.5417 elbo -13.3815 ll -1.7947 kl_fb 11.5868 kl 11.5868 kl_weight 1.0000 sf 3.3837 reward -0.2592 ll_mean -1.5354 ll_std 1.0000 selected 0.4967 prior_p1 0.0794 avg_p1 0.5003\n",
            "Epoch 7 Iter 2600 loss=14.3447 elbo -8.9540 ll -1.6457 kl_fb 7.3084 kl 7.3084 kl_weight 1.0000 sf 1.3197 reward -0.1164 ll_mean -1.5292 ll_std 1.0000 selected 0.4726 prior_p1 0.8825 avg_p1 0.4976\n",
            "Epoch 7 Iter 2700 loss=13.8103 elbo -4.2942 ll -1.4327 kl_fb 2.8615 kl 2.8615 kl_weight 1.0000 sf -1.0536 reward 0.0925 ll_mean -1.5252 ll_std 1.0000 selected 0.4386 prior_p1 0.2261 avg_p1 0.4956\n",
            "\n",
            "# epoch 7 iter 2728: dev loss -1.5034 elbo -17.5256 ll -1.4748 kl_fb 16.0508 kl 16.0508 kl_weight 1.0000 sf 19.0290 reward -1.4748 ll_mean 0.0000 ll_std 1.0000 selected 0.2710 prior_p1 0.5030 avg_p1 0.4935 acc 0.3660\n",
            " dev0 [gold=3,pred=3]: it 's a **lovely** **film** **with** **lovely** **performances** by buy and accorsi .\n",
            " dev1 [gold=2,pred=4]: **no** one goes unindicted here **,** which is probably for the best .\n",
            " dev2 [gold=3,pred=3]: and if you 're not nearly moved to **tears** by a couple of scenes **,** you 've got **ice** **water** in your veins .\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 8 Iter 2800 loss=17.9056 elbo -2.8945 ll -1.5914 kl_fb 1.3031 kl 1.3031 kl_weight 1.0000 sf 0.9605 reward -0.0725 ll_mean -1.5189 ll_std 1.0000 selected 0.4319 prior_p1 0.3225 avg_p1 0.5003\n",
            "Epoch 8 Iter 2900 loss=12.2971 elbo -5.3069 ll -1.4618 kl_fb 3.8451 kl 3.8451 kl_weight 1.0000 sf -0.6748 reward 0.0542 ll_mean -1.5160 ll_std 1.0000 selected 0.4697 prior_p1 0.8062 avg_p1 0.5151\n",
            "Epoch 8 Iter 3000 loss=14.1767 elbo -7.2183 ll -1.4588 kl_fb 5.7595 kl 5.7595 kl_weight 1.0000 sf -0.6149 reward 0.0517 ll_mean -1.5105 ll_std 1.0000 selected 0.5257 prior_p1 0.8609 avg_p1 0.5187\n",
            "\n",
            "# epoch 8 iter 3069: dev loss -5.2660 elbo -13.5832 ll -1.4578 kl_fb 12.1253 kl 12.1253 kl_weight 1.0000 sf 18.8492 reward -1.4578 ll_mean 0.0000 ll_std 1.0000 selected 0.5465 prior_p1 0.4507 avg_p1 0.5000 acc 0.3660\n",
            " dev0 [gold=3,pred=3]: it **'s** a **lovely** **film** **with** **lovely** **performances** **by** buy and accorsi **.**\n",
            " dev1 [gold=2,pred=3]: **no** **one** goes unindicted here **,** **which** **is** probably **for** **the** best **.**\n",
            " dev2 [gold=3,pred=1]: and if you **'re** not nearly moved **to** **tears** **by** a couple **of** **scenes** **,** you 've got **ice** **water** in **your** veins **.**\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 9 Iter 3100 loss=13.5600 elbo -2.1344 ll -1.1677 kl_fb 0.9667 kl 0.9667 kl_weight 1.0000 sf -5.0818 reward 0.3403 ll_mean -1.5080 ll_std 1.0000 selected 0.5384 prior_p1 0.6493 avg_p1 0.5038\n",
            "Epoch 9 Iter 3200 loss=15.5915 elbo -5.8466 ll -1.6550 kl_fb 4.1916 kl 4.1916 kl_weight 1.0000 sf 2.2924 reward -0.1494 ll_mean -1.5056 ll_std 1.0000 selected 0.5548 prior_p1 0.2272 avg_p1 0.5092\n",
            "Epoch 9 Iter 3300 loss=16.0261 elbo -3.0790 ll -1.4909 kl_fb 1.5881 kl 1.5881 kl_weight 1.0000 sf -0.1297 reward 0.0105 ll_mean -1.5013 ll_std 1.0000 selected 0.5062 prior_p1 0.7203 avg_p1 0.5222\n",
            "Epoch 9 Iter 3400 loss=14.5831 elbo -16.1694 ll -1.4177 kl_fb 14.7518 kl 14.7518 kl_weight 1.0000 sf -1.3906 reward 0.0817 ll_mean -1.4994 ll_std 1.0000 selected 0.5126 prior_p1 0.9264 avg_p1 0.5200\n",
            "\n",
            "# epoch 9 iter 3410: dev loss -4.0488 elbo -13.2689 ll -1.3962 kl_fb 11.8727 kl 11.8727 kl_weight 1.0000 sf 17.3176 reward -1.3962 ll_mean 0.0000 ll_std 1.0000 selected 0.8357 prior_p1 0.4957 avg_p1 0.5209 acc 0.3996\n",
            " dev0 [gold=3,pred=2]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** **by** **buy** **and** accorsi **.**\n",
            " dev1 [gold=2,pred=3]: **no** **one** **goes** unindicted **here** **,** **which** **is** **probably** **for** **the** **best** **.**\n",
            " dev2 [gold=3,pred=1]: **and** **if** **you** **'re** **not** **nearly** moved **to** **tears** **by** **a** **couple** **of** **scenes** **,** **you** **'ve** **got** **ice** **water** **in** **your** veins **.**\n",
            "\n",
            "Epoch     9: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Shuffling training data\n",
            "Epoch 10 Iter 3500 loss=14.0251 elbo -9.1346 ll -1.3256 kl_fb 7.8090 kl 7.8090 kl_weight 1.0000 sf -2.5245 reward 0.1685 ll_mean -1.4941 ll_std 1.0000 selected 0.5102 prior_p1 0.1520 avg_p1 0.5165\n",
            "Epoch 10 Iter 3600 loss=14.7404 elbo -13.8360 ll -1.3590 kl_fb 12.4770 kl 12.4770 kl_weight 1.0000 sf -1.6608 reward 0.1327 ll_mean -1.4916 ll_std 1.0000 selected 0.5062 prior_p1 0.9382 avg_p1 0.5154\n",
            "Epoch 10 Iter 3700 loss=14.0665 elbo -3.3828 ll -1.4962 kl_fb 1.8866 kl 1.8866 kl_weight 1.0000 sf 0.1190 reward -0.0091 ll_mean -1.4872 ll_std 1.0000 selected 0.4798 prior_p1 0.7187 avg_p1 0.5080\n",
            "\n",
            "# epoch 10 iter 3751: dev loss -2.5460 elbo -15.4236 ll -1.4100 kl_fb 14.0136 kl 14.0136 kl_weight 1.0000 sf 17.9696 reward -1.4100 ll_mean 0.0000 ll_std 1.0000 selected 0.7691 prior_p1 0.5352 avg_p1 0.5100 acc 0.3851\n",
            " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** **by** **buy** **and** accorsi **.**\n",
            " dev1 [gold=2,pred=3]: **no** **one** **goes** unindicted here **,** **which** **is** **probably** **for** **the** best **.**\n",
            " dev2 [gold=3,pred=1]: **and** **if** **you** **'re** **not** **nearly** moved **to** **tears** **by** **a** **couple** **of** **scenes** **,** **you** **'ve** **got** **ice** **water** **in** **your** veins **.**\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 11 Iter 3800 loss=14.0362 elbo -11.4815 ll -1.6119 kl_fb 9.8696 kl 9.8696 kl_weight 1.0000 sf 1.8056 reward -0.1243 ll_mean -1.4877 ll_std 1.0000 selected 0.5035 prior_p1 0.1152 avg_p1 0.5105\n",
            "Epoch 11 Iter 3900 loss=16.8356 elbo -29.3621 ll -1.5368 kl_fb 27.8253 kl 27.8253 kl_weight 1.0000 sf 0.7003 reward -0.0523 ll_mean -1.4845 ll_std 1.0000 selected 0.5091 prior_p1 0.9871 avg_p1 0.5121\n",
            "Epoch 11 Iter 4000 loss=14.6294 elbo -19.6921 ll -1.3462 kl_fb 18.3459 kl 18.3459 kl_weight 1.0000 sf -1.8544 reward 0.1379 ll_mean -1.4841 ll_std 1.0000 selected 0.5000 prior_p1 0.0417 avg_p1 0.5084\n",
            "\n",
            "# epoch 11 iter 4092: dev loss -2.9376 elbo -15.6839 ll -1.4353 kl_fb 14.2486 kl 14.2486 kl_weight 1.0000 sf 18.6214 reward -1.4353 ll_mean 0.0000 ll_std 1.0000 selected 0.5379 prior_p1 0.4811 avg_p1 0.5010 acc 0.3624\n",
            " dev0 [gold=3,pred=1]: it **'s** a **lovely** **film** **with** **lovely** **performances** **by** **buy** and accorsi .\n",
            " dev1 [gold=2,pred=3]: **no** **one** goes unindicted here **,** which **is** probably **for** **the** best .\n",
            " dev2 [gold=3,pred=1]: and **if** **you** **'re** **not** nearly moved **to** **tears** **by** a couple **of** **scenes** **,** **you** **'ve** got **ice** water **in** **your** veins .\n",
            "\n",
            "Epoch 12 Iter 4100 loss=15.9182 elbo -1.6272 ll -1.6065 kl_fb 0.0207 kl 0.0207 kl_weight 1.0000 sf 1.5173 reward -0.1220 ll_mean -1.4845 ll_std 1.0000 selected 0.5038 prior_p1 0.4814 avg_p1 0.4991\n",
            "Shuffling training data\n",
            "Epoch 12 Iter 4200 loss=14.2398 elbo -2.8460 ll -1.4763 kl_fb 1.3696 kl 1.3696 kl_weight 1.0000 sf -0.1239 reward 0.0088 ll_mean -1.4851 ll_std 1.0000 selected 0.4677 prior_p1 0.6749 avg_p1 0.4981\n",
            "Epoch 12 Iter 4300 loss=14.6639 elbo -2.7295 ll -1.5319 kl_fb 1.1976 kl 1.1976 kl_weight 1.0000 sf 0.5442 reward -0.0478 ll_mean -1.4841 ll_std 1.0000 selected 0.5220 prior_p1 0.6850 avg_p1 0.5022\n",
            "Epoch 12 Iter 4400 loss=15.6007 elbo -28.0534 ll -1.4636 kl_fb 26.5899 kl 26.5899 kl_weight 1.0000 sf -0.2588 reward 0.0204 ll_mean -1.4840 ll_std 1.0000 selected 0.5005 prior_p1 0.9863 avg_p1 0.5008\n",
            "\n",
            "# epoch 12 iter 4433: dev loss -2.9282 elbo -15.5586 ll -1.4340 kl_fb 14.1246 kl 14.1246 kl_weight 1.0000 sf 18.4868 reward -1.4340 ll_mean 0.0000 ll_std 1.0000 selected 0.6983 prior_p1 0.5353 avg_p1 0.5047 acc 0.3688\n",
            " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** **by** **buy** and accorsi **.**\n",
            " dev1 [gold=2,pred=3]: **no** **one** **goes** unindicted here **,** **which** **is** **probably** **for** **the** best **.**\n",
            " dev2 [gold=3,pred=1]: and **if** **you** **'re** **not** nearly moved **to** **tears** **by** **a** **couple** **of** **scenes** **,** **you** **'ve** **got** **ice** **water** **in** **your** veins **.**\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 13 Iter 4500 loss=16.3909 elbo -5.3997 ll -1.4842 kl_fb 3.9155 kl 3.9155 kl_weight 1.0000 sf -0.0188 reward 0.0016 ll_mean -1.4858 ll_std 1.0000 selected 0.4817 prior_p1 0.2035 avg_p1 0.5072\n",
            "Epoch 13 Iter 4600 loss=15.6980 elbo -2.0125 ll -1.6180 kl_fb 0.3945 kl 0.3945 kl_weight 1.0000 sf 1.6788 reward -0.1324 ll_mean -1.4856 ll_std 1.0000 selected 0.4863 prior_p1 0.4056 avg_p1 0.5072\n",
            "Epoch 13 Iter 4700 loss=14.5569 elbo -2.9716 ll -1.4846 kl_fb 1.4870 kl 1.4870 kl_weight 1.0000 sf -0.0311 reward 0.0029 ll_mean -1.4875 ll_std 1.0000 selected 0.4664 prior_p1 0.7123 avg_p1 0.5034\n",
            "\n",
            "# epoch 13 iter 4774: dev loss -6.4753 elbo -12.6808 ll -1.4803 kl_fb 11.2005 kl 11.2005 kl_weight 1.0000 sf 19.1561 reward -1.4803 ll_mean 0.0000 ll_std 1.0000 selected 0.2016 prior_p1 0.4861 avg_p1 0.4939 acc 0.3524\n",
            " dev0 [gold=3,pred=1]: it 's a lovely **film** with lovely **performances** **by** **buy** and accorsi .\n",
            " dev1 [gold=2,pred=3]: **no** one goes **unindicted** here , which is probably for the best .\n",
            " dev2 [gold=3,pred=3]: and if you 're not nearly moved to **tears** **by** a couple of scenes , you 've got **ice** water in your veins .\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 14 Iter 4800 loss=14.4905 elbo -17.6481 ll -1.5065 kl_fb 16.1416 kl 16.1416 kl_weight 1.0000 sf 0.2472 reward -0.0230 ll_mean -1.4835 ll_std 1.0000 selected 0.5197 prior_p1 0.9671 avg_p1 0.4973\n",
            "Epoch 14 Iter 4900 loss=12.7613 elbo -3.2019 ll -1.6227 kl_fb 1.5792 kl 1.5792 kl_weight 1.0000 sf 1.6084 reward -0.1374 ll_mean -1.4853 ll_std 1.0000 selected 0.5201 prior_p1 0.2936 avg_p1 0.4995\n",
            "Epoch 14 Iter 5000 loss=13.6113 elbo -2.6478 ll -1.4447 kl_fb 1.2031 kl 1.2031 kl_weight 1.0000 sf -0.5024 reward 0.0395 ll_mean -1.4842 ll_std 1.0000 selected 0.4570 prior_p1 0.3221 avg_p1 0.4967\n",
            "Epoch 14 Iter 5100 loss=13.2360 elbo -4.3726 ll -1.5137 kl_fb 2.8589 kl 2.8589 kl_weight 1.0000 sf 0.3544 reward -0.0318 ll_mean -1.4819 ll_std 1.0000 selected 0.4718 prior_p1 0.7655 avg_p1 0.4910\n",
            "\n",
            "# epoch 14 iter 5115: dev loss -3.9779 elbo -15.2437 ll -1.4921 kl_fb 13.7515 kl 13.7515 kl_weight 1.0000 sf 19.2216 reward -1.4921 ll_mean 0.0000 ll_std 1.0000 selected 0.1599 prior_p1 0.4963 avg_p1 0.4912 acc 0.3442\n",
            " dev0 [gold=3,pred=3]: it 's a lovely **film** with lovely **performances** **by** **buy** and accorsi .\n",
            " dev1 [gold=2,pred=3]: **no** one goes **unindicted** here , which is probably for the best .\n",
            " dev2 [gold=3,pred=3]: and if you 're not nearly moved to **tears** **by** a couple of scenes , you 've got **ice** water in your veins .\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 15 Iter 5200 loss=14.0736 elbo -30.1020 ll -1.5664 kl_fb 28.5356 kl 28.5356 kl_weight 1.0000 sf 1.0852 reward -0.0839 ll_mean -1.4826 ll_std 1.0000 selected 0.4959 prior_p1 0.9868 avg_p1 0.4892\n",
            "Epoch 15 Iter 5300 loss=12.9121 elbo -12.4334 ll -1.4723 kl_fb 10.9611 kl 10.9611 kl_weight 1.0000 sf -0.1457 reward 0.0103 ll_mean -1.4826 ll_std 1.0000 selected 0.5100 prior_p1 0.0971 avg_p1 0.5039\n",
            "Epoch 15 Iter 5400 loss=15.3547 elbo -34.3211 ll -1.4856 kl_fb 32.8356 kl 32.8356 kl_weight 1.0000 sf 0.0265 reward -0.0018 ll_mean -1.4837 ll_std 1.0000 selected 0.4837 prior_p1 0.0106 avg_p1 0.4973\n",
            "\n",
            "# epoch 15 iter 5456: dev loss -2.2084 elbo -17.0051 ll -1.4929 kl_fb 15.5122 kl 15.5122 kl_weight 1.0000 sf 19.2135 reward -1.4929 ll_mean 0.0000 ll_std 1.0000 selected 0.1509 prior_p1 0.4872 avg_p1 0.4907 acc 0.3451\n",
            " dev0 [gold=3,pred=1]: it 's a lovely film with lovely **performances** by **buy** and accorsi .\n",
            " dev1 [gold=2,pred=1]: **no** one goes **unindicted** here , which is probably for the best .\n",
            " dev2 [gold=3,pred=1]: and if you 're not nearly moved to **tears** by a couple of scenes , you 've got **ice** water in your veins .\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 16 Iter 5500 loss=13.8210 elbo -27.5678 ll -1.4514 kl_fb 26.1164 kl 26.1164 kl_weight 1.0000 sf -0.3752 reward 0.0306 ll_mean -1.4820 ll_std 1.0000 selected 0.4656 prior_p1 0.9853 avg_p1 0.4893\n",
            "Epoch 16 Iter 5600 loss=14.4422 elbo -62.2821 ll -1.4349 kl_fb 60.8472 kl 60.8472 kl_weight 1.0000 sf -0.6346 reward 0.0482 ll_mean -1.4830 ll_std 1.0000 selected 0.5345 prior_p1 0.9996 avg_p1 0.4949\n",
            "Epoch 16 Iter 5700 loss=14.1710 elbo -31.5833 ll -1.7353 kl_fb 29.8481 kl 29.8481 kl_weight 1.0000 sf 3.8497 reward -0.2523 ll_mean -1.4830 ll_std 1.0000 selected 0.4636 prior_p1 0.9824 avg_p1 0.4956\n",
            "\n",
            "# epoch 16 iter 5797: dev loss -3.9170 elbo -14.9186 ll -1.4512 kl_fb 13.4674 kl 13.4674 kl_weight 1.0000 sf 18.8356 reward -1.4512 ll_mean 0.0000 ll_std 1.0000 selected 0.6573 prior_p1 0.5102 avg_p1 0.5018 acc 0.3579\n",
            " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** **by** **buy** and accorsi **.**\n",
            " dev1 [gold=2,pred=3]: **no** **one** **goes** unindicted here **,** **which** **is** **probably** **for** **the** best **.**\n",
            " dev2 [gold=3,pred=1]: and **if** **you** **'re** **not** nearly moved **to** **tears** **by** **a** couple **of** **scenes** **,** **you** 've **got** **ice** **water** **in** **your** veins **.**\n",
            "\n",
            "Epoch 17 Iter 5800 loss=13.5278 elbo -1.7489 ll -1.6342 kl_fb 0.1147 kl 0.1147 kl_weight 1.0000 sf 2.1063 reward -0.1484 ll_mean -1.4858 ll_std 1.0000 selected 0.5414 prior_p1 0.4517 avg_p1 0.5021\n",
            "Shuffling training data\n",
            "Epoch 17 Iter 5900 loss=14.1301 elbo -4.2806 ll -1.5510 kl_fb 2.7296 kl 2.7296 kl_weight 1.0000 sf 0.8583 reward -0.0669 ll_mean -1.4841 ll_std 1.0000 selected 0.5198 prior_p1 0.2484 avg_p1 0.5006\n",
            "Epoch 17 Iter 6000 loss=15.6270 elbo -23.0428 ll -1.3484 kl_fb 21.6944 kl 21.6944 kl_weight 1.0000 sf -1.8054 reward 0.1359 ll_mean -1.4843 ll_std 1.0000 selected 0.5278 prior_p1 0.9741 avg_p1 0.5052\n",
            "Epoch 17 Iter 6100 loss=14.4658 elbo -2.2233 ll -1.6021 kl_fb 0.6212 kl 0.6212 kl_weight 1.0000 sf 1.3520 reward -0.1173 ll_mean -1.4848 ll_std 1.0000 selected 0.4984 prior_p1 0.3681 avg_p1 0.5011\n",
            "\n",
            "# epoch 17 iter 6138: dev loss -7.2166 elbo -11.5789 ll -1.4449 kl_fb 10.1340 kl 10.1340 kl_weight 1.0000 sf 18.7956 reward -1.4449 ll_mean 0.0000 ll_std 1.0000 selected 0.5474 prior_p1 0.5125 avg_p1 0.5000 acc 0.3597\n",
            " dev0 [gold=3,pred=1]: it 's a lovely **film** **with** lovely **performances** **by** **buy** and accorsi **.**\n",
            " dev1 [gold=2,pred=3]: **no** **one** goes unindicted here **,** **which** **is** probably **for** **the** best **.**\n",
            " dev2 [gold=3,pred=1]: and if you **'re** **not** nearly moved **to** **tears** **by** a couple **of** **scenes** **,** you 've got **ice** **water** **in** **your** veins **.**\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 18 Iter 6200 loss=16.3401 elbo -12.0248 ll -1.3314 kl_fb 10.6934 kl 10.6934 kl_weight 1.0000 sf -2.3574 reward 0.1518 ll_mean -1.4832 ll_std 1.0000 selected 0.4938 prior_p1 0.8895 avg_p1 0.4948\n",
            "Epoch 18 Iter 6300 loss=16.1861 elbo -12.1401 ll -1.4779 kl_fb 10.6622 kl 10.6622 kl_weight 1.0000 sf -0.0450 reward 0.0036 ll_mean -1.4815 ll_std 1.0000 selected 0.5119 prior_p1 0.9187 avg_p1 0.5045\n",
            "Epoch 18 Iter 6400 loss=13.7932 elbo -4.3946 ll -1.5764 kl_fb 2.8182 kl 2.8182 kl_weight 1.0000 sf 1.2380 reward -0.0952 ll_mean -1.4813 ll_std 1.0000 selected 0.5074 prior_p1 0.7581 avg_p1 0.5046\n",
            "\n",
            "# epoch 18 iter 6479: dev loss -6.4873 elbo -11.7200 ll -1.4256 kl_fb 10.2944 kl 10.2944 kl_weight 1.0000 sf 18.2073 reward -1.4256 ll_mean 0.0000 ll_std 1.0000 selected 0.7876 prior_p1 0.5608 avg_p1 0.5098 acc 0.3715\n",
            " dev0 [gold=3,pred=3]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** **by** **buy** **and** accorsi **.**\n",
            " dev1 [gold=2,pred=3]: **no** **one** **goes** unindicted here **,** **which** **is** **probably** **for** **the** **best** **.**\n",
            " dev2 [gold=3,pred=3]: **and** **if** **you** **'re** **not** **nearly** moved **to** **tears** **by** **a** **couple** **of** **scenes** **,** **you** **'ve** **got** **ice** **water** **in** **your** veins **.**\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 19 Iter 6500 loss=14.6576 elbo -2.8449 ll -1.4493 kl_fb 1.3956 kl 1.3956 kl_weight 1.0000 sf -0.4594 reward 0.0319 ll_mean -1.4813 ll_std 1.0000 selected 0.5232 prior_p1 0.6832 avg_p1 0.5074\n",
            "Epoch 19 Iter 6600 loss=13.3268 elbo -98.4989 ll -1.3643 kl_fb 97.1346 kl 97.1346 kl_weight 1.0000 sf -1.7037 reward 0.1151 ll_mean -1.4794 ll_std 1.0000 selected 0.4984 prior_p1 1.0000 avg_p1 0.5173\n",
            "Epoch 19 Iter 6700 loss=13.9213 elbo -2.1093 ll -1.3716 kl_fb 0.7377 kl 0.7377 kl_weight 1.0000 sf -1.3499 reward 0.1056 ll_mean -1.4772 ll_std 1.0000 selected 0.5224 prior_p1 0.3838 avg_p1 0.5223\n",
            "Epoch 19 Iter 6800 loss=14.6128 elbo -24.6373 ll -1.5102 kl_fb 23.1270 kl 23.1270 kl_weight 1.0000 sf 0.4648 reward -0.0344 ll_mean -1.4759 ll_std 1.0000 selected 0.4972 prior_p1 0.9794 avg_p1 0.5190\n",
            "\n",
            "# epoch 19 iter 6820: dev loss 4.3883 elbo -21.7624 ll -1.3982 kl_fb 20.3642 kl 20.3642 kl_weight 1.0000 sf 17.3741 reward -1.3982 ll_mean 0.0000 ll_std 1.0000 selected 0.8481 prior_p1 0.6177 avg_p1 0.5211 acc 0.3869\n",
            " dev0 [gold=3,pred=1]: **it** **'s** **a** **lovely** **film** **with** **lovely** **performances** **by** **buy** **and** accorsi **.**\n",
            " dev1 [gold=2,pred=3]: **no** **one** **goes** unindicted **here** **,** **which** **is** **probably** **for** **the** **best** **.**\n",
            " dev2 [gold=3,pred=1]: **and** **if** **you** **'re** **not** **nearly** **moved** **to** **tears** **by** **a** **couple** **of** **scenes** **,** **you** **'ve** **got** **ice** **water** **in** **your** veins **.**\n",
            "\n",
            "Shuffling training data\n",
            "Epoch 20 Iter 6900 loss=14.6743 elbo -35.5720 ll -1.4966 kl_fb 34.0754 kl 34.0754 kl_weight 1.0000 sf 0.2910 reward -0.0201 ll_mean -1.4765 ll_std 1.0000 selected 0.5210 prior_p1 0.0118 avg_p1 0.5210\n",
            "Epoch 20 Iter 7000 loss=13.3574 elbo -4.3503 ll -1.4125 kl_fb 2.9378 kl 2.9378 kl_weight 1.0000 sf -0.6907 reward 0.0630 ll_mean -1.4755 ll_std 1.0000 selected 0.5168 prior_p1 0.2334 avg_p1 0.5145\n",
            "Epoch 20 Iter 7100 loss=14.5939 elbo -4.9697 ll -1.5465 kl_fb 3.4232 kl 3.4232 kl_weight 1.0000 sf 0.9814 reward -0.0731 ll_mean -1.4734 ll_std 1.0000 selected 0.5426 prior_p1 0.7815 avg_p1 0.5125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-e921bc11fb4c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                     cfg=cfg, iter_i=iter_i)\n\u001b[0m\u001b[1;32m    137\u001b[0m                 \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/sst/evaluate.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, data, batch_size, device, iter_i, cfg)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-dcdba641958c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B,T]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mqz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-913f4fc22eaf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# [B, T, E]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;31m# [B, T, d]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m         return F.embedding(\n\u001b[1;32m    117\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "SVsWgmlIWvZq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2VntYV3WvZt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rTxG1AvPWvZv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}