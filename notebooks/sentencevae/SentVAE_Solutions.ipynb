{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentVAE_Solutions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probabll/dgm4nlp/blob/solutions/notebooks/sentencevae/SentVAE_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "AcYW7MDKScQ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import re\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.distributions import Normal\n",
        "from torch.distributions.kl import kl_divergence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pMktCZC3Vwyg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this notebook you will work with a deep generative language model that maps sentences from a continuous latent space. We will use text data in English and pytorch. \n",
        "\n",
        "The first section concerns data manipulation and data loading classes necessary for our implementation. You do not need to modify anything in this part of the code, but it is useful to read through as it might illustrate some good practices."
      ]
    },
    {
      "metadata": {
        "id": "ca_L1zfDScRC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's first download the Penn Treebank dataset that we will be using for this notebook: these are the sentences in the PTB English corpus."
      ]
    },
    {
      "metadata": {
        "id": "pYl0oQJVScRE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c7107637-4967-4736-8004-33772656a806"
      },
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/jhcross/span-parser/master/data/\"\n",
        "train_file = \"02-21.10way.clean\"\n",
        "val_file = \"22.auto.clean\"\n",
        "test_file = \"23.auto.clean\"\n",
        "\n",
        "print(\"Downloading data files...\")\n",
        "if not os.path.isfile(train_file):\n",
        "    urllib.request.urlretrieve(url + train_file, filename=train_file)\n",
        "if not os.path.isfile(val_file):\n",
        "    urllib.request.urlretrieve(url + val_file, filename=val_file)\n",
        "if not os.path.isfile(test_file):\n",
        "    urllib.request.urlretrieve(url + test_file, filename=test_file)\n",
        "print(\"Download complete.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data files...\n",
            "Download complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oPlLBwayScRN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "\n",
        "In order to work with text data, we need to transform the text into something that our algorithms can work with. The first step of this process is converting words into word ids. We do this by constructing a vocabulary from the data, assigning a new word id to each new word it encounters."
      ]
    },
    {
      "metadata": {
        "id": "S4wtwvwaScRP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "UNK_TOKEN = \"<unk>\"\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "SOS_TOKEN = \"<s>\"\n",
        "EOS_TOKEN = \"</s>\"\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.idx_to_word = {0: UNK_TOKEN, 1: PAD_TOKEN, 2: SOS_TOKEN, 3: EOS_TOKEN}\n",
        "        self.word_to_idx = {UNK_TOKEN: 0, PAD_TOKEN: 1, SOS_TOKEN: 2, EOS_TOKEN: 3}\n",
        "        self.word_freqs = {}\n",
        "    \n",
        "    def __getitem__(self, key):\n",
        "        return self.word_to_idx[key] if key in self.word_to_idx else self.word_to_idx[UNK_TOKEN]\n",
        "    \n",
        "    def word(self, idx):\n",
        "        return self.idx_to_word[idx]\n",
        "    \n",
        "    def size(self):\n",
        "        return len(self.word_to_idx)\n",
        "    \n",
        "    @staticmethod\n",
        "    def from_data(filenames):\n",
        "        \"\"\"\n",
        "            Creates a vocabulary from a list of data files. It assumes that the data files have been\n",
        "            tokenized and pre-processed beforehand.\n",
        "        \"\"\"\n",
        "        vocab = Vocabulary()\n",
        "        for filename in filenames:\n",
        "            with open(filename) as f:\n",
        "                for line in f:\n",
        "                    \n",
        "                    # Strip whitespace and the newline symbol.\n",
        "                    line = tokens_from_treestring(line.strip())\n",
        "                    \n",
        "                    # Split the sentences into words and assign word ids to each\n",
        "                    # new word it encounters.\n",
        "                    for word in line.split():\n",
        "                        if word not in vocab.word_to_idx:\n",
        "                            idx = len(vocab.word_to_idx)\n",
        "                            vocab.word_to_idx[word] = idx\n",
        "                            vocab.idx_to_word[idx] = word\n",
        "                            \n",
        "        return vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5yGmXEQ2TQSJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1a497103-5d06-487b-9bf7-b3012501c9da"
      },
      "cell_type": "code",
      "source": [
        "# Construct a vocabulary from the training and validation data.\n",
        "print(\"Constructing vocabulary...\")\n",
        "vocab = Vocabulary.from_data([train_file, val_file])\n",
        "print(\"Constructed a vocabulary of %d word types\" % vocab.size())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Constructing vocabulary...\n",
            "Constructed a vocabulary of 45233 word types\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jLP1Yy2_TtzR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7f6fb56a-8f7b-46a6-93b9-82df4bb82e29"
      },
      "cell_type": "code",
      "source": [
        "# some examples\n",
        "print('the', vocab['the'])\n",
        "print('the', vocab['thing'])\n",
        "print('VAE', vocab['VAE'])  # something UNKNOWN"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the 20\n",
            "the 630\n",
            "VAE 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Wwdq1DtScRX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We also need to load the data files into memory. We create a simple class `TextDataset` that stores the data as a list of sentences:"
      ]
    },
    {
      "metadata": {
        "id": "JYC5fXfIScRa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    \"\"\"\n",
        "        A simple class that loads a list of sentences into memory from a text file,\n",
        "        split by newlines. This does not do any memory optimisation, \n",
        "        so if your dataset is very large, you might want to use an alternative \n",
        "        class.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, text_file):\n",
        "        self.data = []\n",
        "        with open(text_file) as f:\n",
        "            for line in f:\n",
        "                self.data.append(tokens_from_treestring(line.strip()))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SehavEDQTfIe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "c2b611b6-c5c6-418e-cae7-ca34969c7cff"
      },
      "cell_type": "code",
      "source": [
        "# Load the training, validation, and test datasets into memory.\n",
        "train_dataset = TextDataset(train_file)\n",
        "val_dataset = TextDataset(val_file)\n",
        "test_dataset = TextDataset(test_file)\n",
        "\n",
        "# Print some samples from the data:\n",
        "print(\"Sample from training data: \\\"%s\\\"\" % train_dataset[np.random.choice(len(train_dataset))])\n",
        "print(\"Sample from validation data: \\\"%s\\\"\" % val_dataset[np.random.choice(len(val_dataset))])\n",
        "print(\"Sample from test data: \\\"%s\\\"\" % test_dataset[np.random.choice(len(test_dataset))])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample from training data: \"The unit is a loosely constructed group of about 3,900 independent brokers and financial planners who sell insurance annuities limited partnerships mutual funds and other investments for Integrated and other firms\"\n",
            "Sample from validation data: \"The screen was a sea of red\"\n",
            "Sample from test data: \"We simply do n't have strong leadership to try to reduce the deficit and make tough choices House Budget Committee Chairman Leon Panetta D. Calif said yesterday on NBC News 's Meet the Press\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PNQ1TG-3ScRh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now it's time to write a function that converts a sentence into a list of word ids using the vocabulary we created before. This function is `create_batch` in the code cell below. This function creates a batch from a list of sentences, and makes sure that each sentence starts with a start-of-sentence symbol and ends with an end-of-sentence symbol. Because not all sentences are of equal length in a certain batch, sentences are padded with padding symbols so that they match the length of the largest sentence in the batch. The function returns an input batch, an output batch, a mask of 1s for words and 0s for padding symbols, and the sequence lengths of each sentence in the batch. The output batch is shifted by one word, to reflect the predictions that the model is expected to make. For example, for a sentence\n",
        "\\begin{align}\n",
        "    \\text{The dog runs .}\n",
        "\\end{align}\n",
        "the input sequence is\n",
        "\\begin{align}\n",
        "    \\text{SOS The dog runs .}\n",
        "\\end{align}\n",
        "and the output sequence is\n",
        "\\begin{align}\n",
        "    \\text{The dog runs . EOS}\n",
        "\\end{align}\n",
        "\n",
        "You can see the output is shifted wrt the input, that's because we will be computing a distribution for the next word in context of its prefix, and that's why we need to shift the sequence this way.\n",
        "\n",
        "\n",
        "Lastly, we create an inverse function `batch_to_sentences` that recovers the list of sentences from a padded batch of word ids to use during test time."
      ]
    },
    {
      "metadata": {
        "id": "GcYfsIaBScRj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_batch(sentences, vocab, device, word_dropout=0.):\n",
        "    \"\"\"\n",
        "    Converts a list of sentences to a padded batch of word ids. Returns\n",
        "    an input batch, an output batch shifted by one, a sequence mask over\n",
        "    the input batch, and a tensor containing the sequence length of each\n",
        "    batch element.\n",
        "    :param sentences: a list of sentences, each a list of token ids\n",
        "    :param vocab: a Vocabulary object for this dataset\n",
        "    :param device: \n",
        "    :param word_dropout: rate at which we omit words from the context (input)\n",
        "    :returns: a batch of padded inputs, a batch of padded outputs, mask, lengths\n",
        "    \"\"\"\n",
        "    tok = np.array([[SOS_TOKEN] + sen.split() + [EOS_TOKEN] for sen in sentences])\n",
        "    seq_lengths = [len(sen)-1 for sen in tok]\n",
        "    max_len = max(seq_lengths)\n",
        "    pad_id = vocab[PAD_TOKEN]\n",
        "    pad_id_input = [\n",
        "        [vocab[sen[t]] if t < seq_lengths[idx] else pad_id for t in range(max_len)]\n",
        "            for idx, sen in enumerate(tok)]\n",
        "    \n",
        "    # Replace words of the input with <unk> with p = word_dropout.\n",
        "    if word_dropout > 0.:\n",
        "        unk_id = vocab[UNK_TOKEN]\n",
        "        word_drop =  [\n",
        "            [unk_id if (np.random.random() < word_dropout and t < seq_lengths[idx]) else word_ids[t] for t in range(max_len)] \n",
        "                for idx, word_ids in enumerate(pad_id_input)]\n",
        "    \n",
        "    # The output batch is shifted by 1.\n",
        "    pad_id_output = [\n",
        "        [vocab[sen[t+1]] if t < seq_lengths[idx] else pad_id for t in range(max_len)]\n",
        "            for idx, sen in enumerate(tok)]\n",
        "    \n",
        "    # Convert everything to PyTorch tensors.\n",
        "    batch_input = torch.tensor(pad_id_input)\n",
        "    batch_output = torch.tensor(pad_id_output)\n",
        "    seq_mask = (batch_input != vocab[PAD_TOKEN])\n",
        "    seq_length = torch.tensor(seq_lengths)\n",
        "    \n",
        "    # Move all tensors to the given device.\n",
        "    batch_input = batch_input.to(device)\n",
        "    batch_output = batch_output.to(device)\n",
        "    seq_mask = seq_mask.to(device)\n",
        "    seq_length = seq_length.to(device)\n",
        "    \n",
        "    return batch_input, batch_output, seq_mask, seq_length\n",
        "\n",
        "\n",
        "def batch_to_sentences(tensors, vocab: Vocabulary):\n",
        "    \"\"\"\n",
        "    Converts a batch of word ids back to sentences.\n",
        "    :param tensors: [B, T] word ids\n",
        "    :param vocab: a Vocabulary object for this dataset\n",
        "    :returns: an array of strings (each a sentence).\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    batch_size = tensors.size(0)\n",
        "    for idx in range(batch_size):\n",
        "        sentence = [vocab.word(t.item()) for t in tensors[idx,:]]\n",
        "        \n",
        "        # Filter out the start-of-sentence and padding tokens.\n",
        "        sentence = list(filter(lambda t: t != PAD_TOKEN and t != SOS_TOKEN, sentence))\n",
        "        \n",
        "        # Remove the end-of-sentence token and all tokens following it.\n",
        "        if EOS_TOKEN in sentence:\n",
        "            sentence = sentence[:sentence.index(EOS_TOKEN)]\n",
        "            \n",
        "        sentences.append(\" \".join(sentence))\n",
        "    return np.array(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l7R5_4wwScRq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In PyTorch the RNN functions expect inputs to be sorted from long sentences to shorter ones. Therefore we create a simple wrapper class for the DataLoader class that sorts sentences from long to short:  "
      ]
    },
    {
      "metadata": {
        "id": "Xjtwes5iScRs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SortingTextDataLoader:\n",
        "    \"\"\"\n",
        "    A wrapper for the DataLoader class that sorts a list of sentences by their\n",
        "    lengths in descending order.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataloader):\n",
        "        self.dataloader = dataloader\n",
        "        self.it = iter(dataloader)\n",
        "    \n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        sentences = None\n",
        "        for s in self.it:\n",
        "            sentences = s\n",
        "            break\n",
        "\n",
        "        if sentences is None:\n",
        "            self.it = iter(self.dataloader)\n",
        "            raise StopIteration\n",
        "        \n",
        "        sentences = np.array(sentences)\n",
        "        sort_keys = sorted(range(len(sentences)), \n",
        "                           key=lambda idx: len(sentences[idx].split()), \n",
        "                           reverse=True)\n",
        "        sorted_sentences = sentences[sort_keys]\n",
        "        return sorted_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bkrOx_-NScRw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "## Deterministic language model\n",
        "\n",
        "In language modelling, we model a sentence $x = \\langle x_1, \\ldots, x_n \\rangle$  of length $n = |x|$ as a sequence of categorical draws:\n",
        "\n",
        "\\begin{align}\n",
        "X_i|x_{<i} & \\sim \\text{Cat}(f(x_{<i}; \\theta)) \n",
        "& i = 1, \\ldots, n \\\\\n",
        "\\end{align}\n",
        "\n",
        "where we use $x_{<i}$ to denote a (possibly empty) prefix string, and thus the model makes no Markov assumption. We map from the conditioning context, the prefix $x_{<i}$, to the categorical parameters using a fixed neural network architecture whose parameters we collectively denote by $\\theta$.\n",
        "\n",
        "This assigns the following likelihood to the sentence\n",
        "\\begin{align}\n",
        "    P(x|\\theta) &= \\prod_{i=1}^n P(x_i|x_{<i}, \\theta) \\\\\n",
        "    &= \\prod_{i=1}^n \\text{Cat}(x_i|f(x_{<i}; \\theta))  \n",
        "\\end{align}\n",
        "where the categorical pmf is $\\text{Cat}(k|\\pi) = \\pi_k$. \n",
        "\n",
        "\n",
        "Suppose we have a dataset $\\mathcal D = \\{x^{(1)}, \\ldots, x^{(N)}\\}$ containing $N$ i.i.d. observations. Then we can use the log-likelihood function \n",
        "\\begin{align}\n",
        "\\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{N} \\log P(x^{(k)}| \\theta) \\\\\n",
        "&= \\sum_{k=1}^{N} \\sum_{i=1}^{|x^{(k)}|} \\log \\text{Cat}(x^{(k)}_i|f(x^{(k)}_{<i}; \\theta))\n",
        "\\end{align}\n",
        " to estimate $\\theta$ by maximisation:\n",
        " \\begin{align}\n",
        " \\theta^\\star = \\arg\\max_{\\theta \\in \\Theta} \\mathcal L(\\theta|\\mathcal D) ~ .\n",
        " \\end{align}\n",
        " \n",
        "\n",
        "We can use stochastic gradient-ascent to find a local optimum of $\\mathcal L(\\theta|\\mathcal D)$, which only requires a gradient estimate:\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla_\\theta \\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{|\\mathcal D|} \\nabla_\\theta  \\log P(x^{(k)}|\\theta) \\\\ \n",
        "&= \\sum_{k=1}^{|\\mathcal D|} \\frac{1}{N} N \\nabla_\\theta  \\log P(x^{(k)}| \\theta)  \\\\\n",
        "&= \\mathbb E_{\\mathcal U(1/N)} \\left[ N \\nabla_\\theta  \\log P(x^{(K)}| \\theta) \\right]  \\\\\n",
        "&\\overset{\\text{MC}}{\\approx} \\frac{N}{M} \\sum_{m=1}^M \\nabla_\\theta  \\log P(x^{(k_m)}|\\theta) \\\\\n",
        "&\\text{where }K_m \\sim \\mathcal U(1/N)\n",
        "\\end{align}\n",
        "\n",
        "This is a Monte Carlo (MC) estimate of the gradient computed on $M$ data points selected uniformly at random from $\\mathcal D$.\n",
        "\n",
        "For as long as $f$ remains differentiable wrt to its inputs and parameters, we can rely on automatic differentiation to obtain gradient estimates.\n",
        "\n",
        "\n",
        "An example design for $f$ is:\n",
        "\\begin{align}\n",
        "\\mathbf x_i &= \\text{emb}(x_i; \\theta_{\\text{emb}}) \\\\\n",
        "\\mathbf h_i &= \\text{rnn}(\\mathbf h_{i-1}, \\mathbf x_{i-1}; \\theta_{\\text{rnn}}) \\\\\n",
        "f(x_{<i}; \\theta) &= \\text{softmax}(\\text{dense}_v(\\mathbf h_{i};  \\theta_{\\text{out}}))\n",
        "\\end{align}\n",
        "where \n",
        "* $\\text{emb}$ is a fixed embedding layer with parameters $\\theta_{\\text{emb}}$;\n",
        "* $\\text{rnn}$ is a recurrent architecture with parameters $\\theta_{\\text{rnn}}$, e.g. an LSTM or GRU, and $\\mathbf h_0$ is part of the architecture's parameters;\n",
        "* $\\text{dense}_v$ is a dense layer with $v$ outputs (vocabulary size) and parameters $\\theta_{\\text{out}}$.\n",
        "\n",
        "\n",
        "\n",
        "In what follows we show how to extend this model with a continuous latent sentence embedding."
      ]
    },
    {
      "metadata": {
        "id": "xhO8rD2DYuar",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Deep generative language model\n",
        "\n",
        "We want to model a sentence $x$ as a draw from the marginal of deep generative model $p(z, x|\\theta) = p(z)P(x|z, \\theta)$. Note that we use $p(\\cdot)$ for pdfs and $P(\\cdot)$ for pmfs.\n",
        "\n",
        "The generative story is:\n",
        "\\begin{align}\n",
        "    Z & \\sim \\mathcal N(0, I) \\\\\n",
        "    X_i | z, x_{<i} &\\sim \\text{Cat}(f(z, x_{<i}; \\theta)) & i=1, \\ldots, n\n",
        "\\end{align}\n",
        "where $z \\in \\mathbb R^D$ and  we impose a standard Gaussian prior on latent space. Other choices of prior can induce interesting properties in latent space, however, in this notebook, we use the Gaussian for its simplicity.\n",
        "\n",
        "It is easy to design $f$ by a simple modification of the deterministic design shown before:\n",
        "\\begin{align}\n",
        "\\mathbf x_i &= \\text{emb}(x_i; \\theta_{\\text{emb}}) \\\\\n",
        "\\mathbf h_0 &= \\tanh(\\text{dense}(z; \\theta_{\\text{init}})) \\\\\n",
        "\\mathbf h_i &= \\text{rnn}(\\mathbf h_{i-1}, \\mathbf x_{i-1}; \\theta_{\\text{rnn}}) \\\\\n",
        "f(x_{<i}; \\theta) &= \\text{softmax}(\\text{dense}_v(\\mathbf h_{i};  \\theta_{\\text{out}}))\n",
        "\\end{align}\n",
        "where we just initialise the recurrent cell using $z$. Note we could also use $z$ in other places, for example, as additional input to every update of the recurrent cell $\\mathbf h_i = \\text{rnn}(\\mathbf h_{i-1}, [\\mathbf x_{i-1}, z])$. This is an architecture choice which like many others can only be judged empirically or on the basis of practical convenience.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "YwB7igyXg8uU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The marginal likelihood, necessary for parameter estimation, is now no longer tractable:\n",
        "\\begin{align}\n",
        "P(x|\\theta) &= \\int p(z)P(x|z, \\theta) \\text{d}z \\\\\n",
        "&= \\int \\mathcal N(z|0, I)\\prod_{i=1}^n \\text{Cat}(x_i|f(z,x_{<i}; \\theta) )\\text{d}z \n",
        "\\end{align}\n",
        "the intractability is clear as even if $f$ would be linear (recall it isn't) the Gaussian and the Categorical are not conjugate and the integration has no closed-form solution.\n",
        "\n",
        "We turn to variational inference and derive a lowerbound $\\mathcal E(\\theta, \\lambda|\\mathcal D)$ on the log-likelihood function\n",
        "\n",
        "\\begin{align}\n",
        "    \\mathcal E(\\theta, \\lambda|\\mathcal D) &= \\sum_{k=1}^{|\\mathcal D|} \\mathcal E_k(\\theta, \\lambda|x^{(k)}) \n",
        "\\end{align}\n",
        "\n",
        "which for a single datapoint $x$ is\n",
        "\\begin{align}\n",
        "    \\mathcal E(\\theta, \\lambda|x) &= \\mathbb{E}_{q(z|x)}\\left[P(x|z, \\theta)\\right] - \\text{KL}\\left(q(z|x, \\lambda)||p(z)\\right)\\\\\n",
        "\\end{align}\n",
        "where we have introduce an independently parameterised auxiliary distribution $q(z|x, \\lambda)$. The distribution $q$ which maximises this *evidence lowerbound* (ELBO) is also the distribution that minimises \n",
        "\\begin{align}\n",
        "\\text{KL}(q(z|x, \\lambda)||p(z|x, \\theta)) = \\mathbb E_{q(z|x, \\lambda)}\\left[ \\frac{q(z|x, \\lambda)}{p(z|x, \\theta)}\\right]\n",
        "\\end{align}\n",
        " where $p(z|x, \\theta) = \\frac{p(x, z|\\theta)}{p(x|\\theta)}$ is our intractable true posterior. For that reason, we think of $q(z|x, \\lambda)$ as an *approximate posterior*. \n",
        " \n",
        " The approximate posterior is an independent model of the latent variable given the data, for that reason we also call it an *inference model*. \n",
        " In this notebook, our inference model will be a diagonal Gaussian, to make sure that we cover the sample space of our latent variable.\n",
        " \n",
        " \\begin{align}\n",
        "    q(z|x, \\lambda) = \\mathcal N(z|\\mu(x; \\lambda), \\sigma(x; \\lambda)^2)\n",
        " \\end{align}\n",
        " \n",
        " where we employ neural network architectures to map from the sentence $x$ to a vector of locations (in $\\mathbb R^D$) and a vector of scales (in $\\mathbb R^D_{>0}$).\n",
        " \n",
        " A diagonal Gaussian models each component of the latent representation independently, and is an example of mean field inference. Because we have neural networks compute the diagonal Gaussian parameters for us, we call this *amortised* mean field inference.\n",
        " \n",
        " This makes our model an instance of what is known in the literature as a *variational auto-encoder* (VAE).\n",
        "\n",
        "Here's an example design for our inference model:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf x_i &= \\text{emb}(x_i; \\lambda_{\\text{emb}}) \\\\\n",
        "\\mathbf f_i &= \\text{rnn}(\\mathbf f_{i-1}, \\mathbf x_{i}; \\lambda_{\\text{fwd}}) \\\\\n",
        "\\mathbf b_i &= \\text{rnn}(\\mathbf b_{i+1}, \\mathbf x_{i}; \\lambda_{\\text{bwd}}) \\\\\n",
        "\\mathbf h &= \\text{dense}([\\mathbf f_{n}, \\mathbf b_1]; \\lambda_{\\text{hid}}) \\\\\n",
        "\\mu(x; \\lambda) &= \\text{dense}(\\mathbf h; \\lambda_{\\text{loc}})\\\\\n",
        "\\sigma(x; \\lambda) &= \\text{softplus}(\\text{dense}(\\mathbf h; \\lambda_{\\text{scale}}))\n",
        "\\end{align}\n",
        "\n",
        "where we use the $\\text{softplus}$ activation to make sure our scales are strictly positive. Note that $\\exp$ would also do that job, though $\\text{softplus}$ is assymptotically linear with its input, which gives us better gradient dynamics."
      ]
    },
    {
      "metadata": {
        "id": "XUqjJzdXYtMa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "izurBBZQScRx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class InferenceModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedder, hidden_size,\n",
        "                 latent_size, pad_idx, bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "        \n",
        "        # We borrow the embedder from the generative model, but we don't\n",
        "        # want tobackpropagate through it for the inference model. So we\n",
        "        # need to make sure to call detach on the embeddings later.\n",
        "        self.embedder = embedder\n",
        "        \n",
        "        # Create a (bidirectional) LSTM to encode x.\n",
        "        self.lstm = nn.LSTM(emb_size, hidden_size, batch_first=True, \n",
        "                            bidirectional=bidirectional)\n",
        "        \n",
        "        # The output of the LSTM doubles if we use a bidirectional encoder.\n",
        "        encoding_size = hidden_size * 2 if bidirectional else hidden_size\n",
        "        \n",
        "        # Create two affine layers to project the encoder final state to\n",
        "        # the mean and standard deviation of the diagonal Gaussian that\n",
        "        # we are predicting.\n",
        "        self.mu_layer = nn.Linear(encoding_size, latent_size)\n",
        "        self.sigma_layer = nn.Linear(encoding_size, latent_size)\n",
        "\n",
        "    def forward(self, x, seq_mask, seq_len):\n",
        "        \n",
        "        # Compute word embeddings and detach them so that no gradients\n",
        "        # from the infererence model flow through.\n",
        "        x_embed = self.embedder(x).detach()\n",
        "        \n",
        "        # Encode the sentence using the LSTM.\n",
        "        hidden = None        \n",
        "        packed_seq = pack_padded_sequence(x_embed, seq_len, batch_first=True)\n",
        "        _, final = self.lstm(packed_seq, hidden)        \n",
        "\n",
        "        # Take the final output h_T from the LSTM, concatenate the forward\n",
        "        # and backward directions for the bidirectional case.\n",
        "        h_T = final[0]\n",
        "        if self.bidirectional:\n",
        "            h_T_fwd = h_T[0]\n",
        "            h_T_bwd = h_T[1]\n",
        "            h_T = torch.cat([h_T_fwd, h_T_bwd], dim=-1)\n",
        "        \n",
        "        # Compute the mean and sigma of the diagonal Gaussian distribution.\n",
        "        # Use a softplus activation for the standard deviation to ensure it's\n",
        "        # positive.\n",
        "        mu = self.mu_layer(h_T)\n",
        "        sigma = F.softplus(self.sigma_layer(h_T))\n",
        "        \n",
        "        # Return the inferred Gaussian distribution q(z|x).\n",
        "        qz = Normal(mu, sigma)\n",
        "        return qz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "53Aztjb5ScR2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we define the generative model:"
      ]
    },
    {
      "metadata": {
        "id": "ObJ-Ag_5ScR4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BowmanLM(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, emb_size, hidden_size, latent_size,\n",
        "                 pad_idx, dropout=0., bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.embedder = nn.Embedding(vocab_size, emb_size,\n",
        "                                     padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb_size, hidden_size, batch_first=True)\n",
        "        self.bridge = nn.Linear(latent_size, hidden_size)\n",
        "        self.projection = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "        self.dropout_layer = nn.Dropout(p=dropout)\n",
        "        self.inference_model = InferenceModel(vocab_size, self.embedder,\n",
        "                                              hidden_size, latent_size, pad_idx, \n",
        "                                              bidirectional=bidirectional)\n",
        "    \n",
        "    \"\"\"\n",
        "        Infers the approximate posterior q(z|x) for a given x.\n",
        "    \"\"\"\n",
        "    def infer(self, x, seq_mask, seq_len):\n",
        "        qz = self.inference_model(x, seq_mask, seq_len)\n",
        "        return qz\n",
        "\n",
        "    \"\"\"\n",
        "        Returns the hidden state of the LSTM initialized with a projection of a given z.\n",
        "    \"\"\"\n",
        "    def init_hidden(self, z):\n",
        "        h = self.bridge(z).unsqueeze(0)\n",
        "        c = self.bridge(z).unsqueeze(0)\n",
        "        return (h, c)\n",
        "\n",
        "    \"\"\"\n",
        "        Performs a single LSTM step for a given previous word and hidden state.\n",
        "        Returns the unnormalized probabilities over the vocabulary for this time step. \n",
        "    \"\"\"\n",
        "    def step(self, prev_x, z, hidden):\n",
        "        x_embed = self.dropout_layer(self.embedder(prev_x))\n",
        "        output, hidden = self.lstm(x_embed, hidden)\n",
        "        scores = self.projection(self.dropout_layer(output))\n",
        "        return scores, hidden\n",
        "    \n",
        "    \"\"\"\n",
        "        Performs an entire forward pass given a sequence of words x and a z.\n",
        "    \"\"\"\n",
        "    def forward(self, x, z):\n",
        "        hidden = self.init_hidden(z)\n",
        "        outputs = []\n",
        "        for t in range(x.size(1)):\n",
        "            prev_x = x[:, t].unsqueeze(-1)\n",
        "            scores, hidden = self.step(prev_x, z, hidden)\n",
        "            outputs.append(scores)\n",
        "        return torch.cat(outputs, dim=1)\n",
        "        \n",
        "    \"\"\"\n",
        "        Computes the loss given the scores (unnormalized probabilities), targets,\n",
        "        the prior distribution p(z), and the approximate posterior distribution q(z|x).\n",
        "        If free_nats is nonzero it will clamp the KL divergence between the posterior\n",
        "        and prior to that value, preventing gradient propagation via the KL if it's\n",
        "        below that value. If evaluation is set to true, the loss will be summed instead\n",
        "        of averaged over the batch. Returns the reconstruction loss and the KL. The loss\n",
        "        can be computed from those as loss = rec_loss - KL.\n",
        "    \"\"\"\n",
        "    def loss(self, scores, targets, pz, qz, free_nats=0., evaluation=False):\n",
        "        \n",
        "        # Approximate E[log P(x|z)].\n",
        "        scores = scores.permute(0, 2, 1)\n",
        "        reconstruction_loss = F.cross_entropy(scores, targets, \n",
        "                                              ignore_index=self.pad_idx, \n",
        "                                              reduction=\"none\")\n",
        "        reconstruction_loss = reconstruction_loss.sum(dim=1)\n",
        "        \n",
        "        # Compute the KL divergence and clamp to at least the given amount of free nats.\n",
        "        KL = kl_divergence(qz, pz).sum(dim=1)\n",
        "        KL = torch.clamp(KL, min=free_nats)\n",
        "        \n",
        "        # For evaluation return the sum of individual components, for\n",
        "        # training return the mean of those components.\n",
        "        if evaluation:\n",
        "            return (reconstruction_loss.sum(), KL.sum())\n",
        "        else:\n",
        "            return (reconstruction_loss.mean(), KL.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PLfa31jEScR7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluation metrics\n",
        "\n",
        "During training we'd like to keep track of some evaluation metrics on the validation data in order to keep track of how our model is doing and to perform early stopping. One simple metric we can compute is the ELBO on all the validation or test data using a single sample from the approximate posterior $q(z|x)$:"
      ]
    },
    {
      "metadata": {
        "id": "URdX_JJ5ScR9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Computes a single sample estimate of the ELBO on a given dataset.\n",
        "\"\"\"\n",
        "def eval_elbo(model, eval_dataset, vocab, device, batch_size=128):\n",
        "    dl = DataLoader(eval_dataset, batch_size=batch_size)\n",
        "    sorted_dl = SortingTextDataLoader(dl)\n",
        "    \n",
        "    # Make sure the model is in evaluation mode (i.e. disable dropout).\n",
        "    model.eval()\n",
        "            \n",
        "    total_rec_loss = 0.\n",
        "    total_KL = 0.\n",
        "    num_sentences = 0\n",
        "        \n",
        "    # We don't need to compute gradients for this.\n",
        "    with torch.no_grad():\n",
        "        for sentences in sorted_dl:    \n",
        "            x_in, x_out, seq_mask, seq_len = create_batch(sentences, vocab, device)\n",
        "            \n",
        "            # Infer the approximate posterior and construct the prior.\n",
        "            qz = model.infer(x_in, seq_mask, seq_len)\n",
        "            pz = Normal(torch.zeros_like(qz.mean), \n",
        "                        torch.ones_like(qz.stddev))\n",
        "            \n",
        "            # Compute the unnormalized probabilities using a single sample from the\n",
        "            # approximate posterior.\n",
        "            z = qz.sample()\n",
        "            scores = model(x_in, z)\n",
        "            \n",
        "            # Compute the reconstruction loss and KL divergence.\n",
        "            reconstruction_loss, KL = model.loss(scores, x_out, pz, qz,\n",
        "                                                 free_nats=0.,\n",
        "                                                 evaluation=True)\n",
        "            total_rec_loss += reconstruction_loss\n",
        "            total_KL += KL\n",
        "            num_sentences += x_in.size(0)\n",
        "\n",
        "    # Return the average reconstruction loss and KL.\n",
        "    avg_rec_loss = total_rec_loss / num_sentences\n",
        "    avg_KL = total_KL / num_sentences\n",
        "    return avg_rec_loss, avg_KL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tGO7RBYgScR_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A common metric to evaluate language models is the perplexity per word. The perplexity per word for a dataset is defined as:\n",
        "\n",
        "\\begin{align}\n",
        "    \\text{ppl}(\\mathcal{D}) = \\exp\\left(-\\frac{1}{\\sum_{k=1}^{|\\mathcal D|} n^{(k)}} \\sum_{k=1}^{|\\mathcal{D}|} \\log P(x^{(k)})\\right) \n",
        "\\end{align}\n",
        "\n",
        "where $n^{(k)}$ is the number of tokens in a sentence and $P(x^{(k)})$ is the probability that our model assigns to the datapoint $x^{(k)}$. In order to compute $\\log P(x)$ for our model we need to solve for the integral:\n",
        "\n",
        "\\begin{align}\n",
        "    P(x) = \\int P(x|z) p(z) dz\n",
        "\\end{align}\n",
        "\n",
        "As this is an integral we cannot compute in closed-form, we have two options: we can use the earlier derived lower-bound on the log-likelihood, which will give us an upper-bound on the perplexity, or we can make an importance sampling estimate using our approximate posterior distribution. The importance sampling estimate can be done as:\n",
        "\n",
        "\\begin{align}\n",
        "    &\\frac{1}{\\sum_{k=1}^{|\\mathcal D|} n^{(k)}}  \\sum_{k=1}^{|\\mathcal D|} \\log p(x^{(k)}) \\\\\n",
        "    &\\approx \\frac{1}{\\sum_{k=1}^{|\\mathcal D|} n^{(k)}}  \\sum_{k=1}^{|\\mathcal D|} \\log \\frac{1}{S} \\sum_{s=1}^{S} \\frac{p(z^{(s)})p(x^{(k)}|z^{(s)})}{q(z^{(s)}|x^{(k)})} \\\\\n",
        "\\end{align}\n",
        "\n",
        "where $S$ is the number of samples, and $z^{(s)} \\sim q(z|x^{k})$. We define the function `eval_perplexity` below that implements this importance sampling estimate:"
      ]
    },
    {
      "metadata": {
        "id": "VnaIidkwScSA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Estimates the per-word perplexity using importance sampling with the\n",
        "    given number of samples.\n",
        "\"\"\"\n",
        "def eval_perplexity(model, eval_dataset, vocab, device, \n",
        "                    n_samples, batch_size=128):\n",
        "    \n",
        "    dl = DataLoader(eval_dataset, batch_size=batch_size)\n",
        "    sorted_dl = SortingTextDataLoader(dl)\n",
        "    \n",
        "    # Make sure the model is in evaluation mode (i.e. disable dropout).\n",
        "    model.eval()\n",
        "    \n",
        "    log_px = 0.\n",
        "    num_predictions = 0\n",
        "    num_sentences = 0\n",
        "     \n",
        "    # We don't need to compute gradients for this.\n",
        "    with torch.no_grad():\n",
        "        for sentences in sorted_dl:\n",
        "            x_in, x_out, seq_mask, seq_len = create_batch(sentences, vocab, device)\n",
        "            \n",
        "            # Infer the approximate posterior and construct the prior.\n",
        "            qz = model.infer(x_in, seq_mask, seq_len)\n",
        "            pz = Normal(torch.zeros_like(qz.mean), \n",
        "                        torch.ones_like(qz.stddev))\n",
        "\n",
        "            # Create an array to hold all samples for this batch.\n",
        "            batch_size = x_in.size(0)\n",
        "            log_px_samples = torch.zeros(n_samples, batch_size)\n",
        "            \n",
        "            # Sample log P(x) n_samples times.\n",
        "            for s in range(n_samples):\n",
        "                \n",
        "                # Sample a z^s from the posterior.\n",
        "                z = qz.sample()\n",
        "                \n",
        "                # Compute log P(x^k|z^s)\n",
        "                scores = model(x_in, z)\n",
        "                cond_log_prob = F.log_softmax(scores, dim=-1)\n",
        "                cond_log_prob = torch.gather(cond_log_prob, 2, x_out.unsqueeze(-1)).squeeze() # B x T\n",
        "                cond_log_prob = (cond_log_prob * seq_mask.type_as(cond_log_prob)).sum(dim=1) # B\n",
        "                \n",
        "                # Compute log p(z^s) and log q(z^s|x^k)\n",
        "                prior_log_prob = pz.log_prob(z).sum(dim=1) # B\n",
        "                posterior_log_prob = qz.log_prob(z).sum(dim=1) # B\n",
        "                \n",
        "                # Store the sample for log P(x^k) importance weighted with p(z^s)/q(z^s|x^k).\n",
        "                log_px_sample = cond_log_prob + prior_log_prob - posterior_log_prob\n",
        "                log_px_samples[s] = log_px_sample\n",
        "                \n",
        "            # Average over the number of samples and count the number of predictions made this batch.\n",
        "            log_px_batch = torch.logsumexp(log_px_samples, dim=0) - \\\n",
        "                    torch.log(torch.Tensor([n_samples]))\n",
        "            log_px += log_px_batch.sum()\n",
        "            num_predictions += seq_len.sum()\n",
        "            num_sentences += seq_len.size(0)\n",
        "\n",
        "    # Compute and return the perplexity per word.\n",
        "    perplexity = torch.exp(-log_px / num_predictions)\n",
        "    NLL = -log_px / num_sentences\n",
        "    return perplexity, NLL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IvoVmedYScSC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lastly, we want to occasionally qualitatively see the performance of the model during training, by letting it reconstruct a given sentence from the latent space. This gives us an idea of whether the model is using the latent space to encode some semantics about the data. For this we use a deterministic greedy decoding algorithm, that chooses the word with maximum probability at every time step, and feeds that word into the next time step."
      ]
    },
    {
      "metadata": {
        "id": "LDD5XF1GScSC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Greedily decodes a sentence from a given z, by picking the word with\n",
        "    maximum probability at each time step.\n",
        "\"\"\"\n",
        "def greedy_decode(model, z, vocab, max_len=50):\n",
        "    \n",
        "    # Disable dropout.\n",
        "    model.eval()\n",
        "    \n",
        "    # Don't compute gradients.\n",
        "    with torch.no_grad():\n",
        "        batch_size = z.size(0)\n",
        "        \n",
        "        # We feed the model the start-of-sentence symbol at the first time step.\n",
        "        prev_x = torch.ones(batch_size, 1, dtype=torch.long).fill_(vocab[SOS_TOKEN]).to(z.device)\n",
        "        \n",
        "        # Initialize the hidden state from z.\n",
        "        hidden = model.init_hidden(z)\n",
        "\n",
        "        predictions = []    \n",
        "        for t in range(max_len):\n",
        "            scores, hidden = model.step(prev_x, z, hidden)\n",
        "            \n",
        "            # Choose the argmax of the unnnormalized probabilities as the\n",
        "            # prediction for this time step.\n",
        "            prediction = torch.argmax(scores, dim=-1)\n",
        "            predictions.append(prediction)\n",
        "            \n",
        "            prev_x = prediction.view(batch_size, 1)\n",
        "            \n",
        "        return torch.cat(predictions, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GERuGgChScSE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "Now it's time to train the model. We use early stopping on the validation perplexity for model selection."
      ]
    },
    {
      "metadata": {
        "id": "XH6ocHxaScSF",
        "colab_type": "code",
        "colab": {},
        "outputId": "f9b68af0-118a-4f0a-f517-9de5e9361793"
      },
      "cell_type": "code",
      "source": [
        "# Define the model hyperparameters.\n",
        "emb_size = 256\n",
        "hidden_size = 256 \n",
        "latent_size = 16\n",
        "bidirectional_encoder = True\n",
        "free_nats = 0.\n",
        "annealing_steps = 5600\n",
        "dropout = 0.6\n",
        "word_dropout = 0.75\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "n_importance_samples = 50\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")   \n",
        "\n",
        "# Create the training data loader.\n",
        "dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "sorted_dl = SortingTextDataLoader(dl)\n",
        "\n",
        "# Create the model.\n",
        "model = BowmanLM(vocab_size=vocab.size(), \n",
        "                  emb_size=emb_size, \n",
        "                  hidden_size=hidden_size, \n",
        "                  latent_size=latent_size, \n",
        "                  pad_idx=vocab[PAD_TOKEN],\n",
        "                  dropout=dropout,\n",
        "                  bidirectional=bidirectional_encoder)\n",
        "model = model.to(device)\n",
        "\n",
        "# Create the optimizer.\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Save the best model (early stopping).\n",
        "best_model = \"./best_model.pt\"\n",
        "best_val_ppl = float(\"inf\")\n",
        "best_epoch = 0\n",
        "\n",
        "# Keep track of some statistics to plot later.\n",
        "train_ELBOs = []\n",
        "train_KLs = []\n",
        "val_ELBOs = []\n",
        "val_KLs = []\n",
        "val_perplexities = []\n",
        "val_NLLs = []\n",
        "\n",
        "step = 0\n",
        "training_ELBO = 0.\n",
        "training_KL = 0.\n",
        "num_batches = 0\n",
        "for epoch_num in range(1, num_epochs+1):    \n",
        "    for sentences in sorted_dl:\n",
        "\n",
        "        # Make sure the model is in training mode (for dropout).\n",
        "        model.train()\n",
        "\n",
        "        # Transform the sentences to input, output, seq_len, seq_mask batches.\n",
        "        x_in, x_out, seq_mask, seq_len = create_batch(sentences, vocab, device,\n",
        "                                                      word_dropout=word_dropout)\n",
        "\n",
        "        # Compute the multiplier for the KL term if we do annealing.\n",
        "        if annealing_steps > 0:\n",
        "            KL_weight = min(1., (1.0 / annealing_steps) * step)\n",
        "        else:\n",
        "            KL_weight = 1.\n",
        "        \n",
        "        # Do a forward pass through the model and compute the training loss. We use\n",
        "        # a reparameterized sample from the approximate posterior during training.\n",
        "        qz = model.infer(x_in, seq_mask, seq_len)\n",
        "        pz = Normal(torch.zeros_like(qz.mean), \n",
        "                    torch.ones_like(qz.stddev))\n",
        "        z = qz.rsample()\n",
        "        scores = model(x_in, z)\n",
        "        rec_loss, KL = model.loss(scores, x_out, pz, qz, free_nats=free_nats)\n",
        "        loss = rec_loss + KL_weight * KL\n",
        "\n",
        "        # Backpropagate and update the model weights.\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Update some statistics to track for the training loss.\n",
        "        training_ELBO += -(rec_loss - KL)\n",
        "        training_KL += KL\n",
        "        num_batches += 1\n",
        "        \n",
        "        # Every 100 steps we evaluate the model and report progress.\n",
        "        if step % 100 == 0:\n",
        "            val_rec_loss, val_KL = eval_elbo(model, val_dataset, vocab, device)\n",
        "            val_ELBO = -(val_rec_loss - val_KL)\n",
        "            print(\"(%d) step %d: training ELBO (KL) = %.2f (%.2f) --\"\n",
        "                  \" KL weight = %.2f --\"\n",
        "                  \" validation ELBO (KL) = %.2f (%.2f)\" % \n",
        "                  (epoch_num, step, training_ELBO/num_batches, \n",
        "                   training_KL/num_batches, KL_weight, val_ELBO, val_KL))\n",
        "            \n",
        "            # Update some statistics for plotting later.\n",
        "            train_ELBOs.append((step, (training_ELBO/num_batches).item()))\n",
        "            train_KLs.append((step, (training_KL/num_batches).item()))\n",
        "            val_ELBOs.append((step, val_ELBO.item()))\n",
        "            val_KLs.append((step, val_KL.item()))\n",
        "            \n",
        "            # Reset the training statistics.\n",
        "            training_ELBO = 0.\n",
        "            training_KL = 0.\n",
        "            num_batches = 0\n",
        "            \n",
        "        step += 1\n",
        "\n",
        "    # After an epoch we'll compute validation perplexity and save the model\n",
        "    # for early stopping if it's better than previous models.\n",
        "    print(\"Finished epoch %d\" % (epoch_num))\n",
        "    val_perplexity, val_NLL = eval_perplexity(model, val_dataset, vocab, device, \n",
        "                                              n_importance_samples)\n",
        "    val_rec_loss, val_KL = eval_elbo(model, val_dataset, vocab, device)\n",
        "    val_ELBO = -(val_rec_loss - val_KL)\n",
        "    \n",
        "    # Keep track of the validation perplexities / NLL.\n",
        "    val_perplexities.append((epoch_num, val_perplexity.item()))\n",
        "    val_NLLs.append((epoch_num, val_NLL.item()))\n",
        "    \n",
        "    # If validation perplexity is better, store this model for early stopping.\n",
        "    if val_perplexity < best_val_ppl:\n",
        "        best_val_ppl = val_perplexity\n",
        "        best_epoch = epoch_num\n",
        "        torch.save(model.state_dict(), best_model)\n",
        "        \n",
        "    # Print epoch statistics.\n",
        "    print(\"Evaluation epoch %d:\\n\"\n",
        "          \" - validation perplexity: %.2f\\n\"\n",
        "          \" - validation NLL: %.2f\\n\"\n",
        "          \" - validation ELBO (KL) = %.2f (%.2f)\"\n",
        "          % (epoch_num, val_perplexity, val_NLL, val_ELBO, val_KL))\n",
        "\n",
        "    # Also show some qualitative results by reconstructing a sentence from the\n",
        "    # validation data. Use the mean of the approximate posterior and greedy\n",
        "    # decoding.\n",
        "    random_sentence = val_dataset[np.random.choice(len(val_dataset))]\n",
        "    x_in, _, seq_mask, seq_len = create_batch([random_sentence], vocab, device)\n",
        "    qz = model.infer(x_in, seq_mask, seq_len)\n",
        "    z = qz.mean\n",
        "    reconstruction = greedy_decode(model, z, vocab)\n",
        "    reconstruction = batch_to_sentences(reconstruction, vocab)[0]\n",
        "    print(\"-- Original sentence: \\\"%s\\\"\" % random_sentence)\n",
        "    print(\"-- Model reconstruction: \\\"%s\\\"\" % reconstruction)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1) step 0: training ELBO (KL) = -225.00 (1.96) -- KL weight = 0.00 -- validation ELBO (KL) = -227.45 (2.16)\n",
            "(1) step 100: training ELBO (KL) = -141.31 (37.53) -- KL weight = 0.02 -- validation ELBO (KL) = -147.79 (8.30)\n",
            "(1) step 200: training ELBO (KL) = -152.53 (7.16) -- KL weight = 0.04 -- validation ELBO (KL) = -146.99 (5.53)\n",
            "(1) step 300: training ELBO (KL) = -151.14 (5.15) -- KL weight = 0.05 -- validation ELBO (KL) = -143.93 (5.68)\n",
            "(1) step 400: training ELBO (KL) = -148.81 (5.50) -- KL weight = 0.07 -- validation ELBO (KL) = -142.17 (5.06)\n",
            "(1) step 500: training ELBO (KL) = -146.67 (5.79) -- KL weight = 0.09 -- validation ELBO (KL) = -139.92 (5.43)\n",
            "(1) step 600: training ELBO (KL) = -145.52 (6.14) -- KL weight = 0.11 -- validation ELBO (KL) = -137.78 (5.89)\n",
            "Finished epoch 1\n",
            "Evaluation epoch 1:\n",
            " - validation perplexity: 944.06\n",
            " - validation NLL: 146.80\n",
            " - validation ELBO (KL) = -137.14 (6.07)\n",
            "-- Original sentence: \"James A. White and Sonja Steptoe contributed to this article\"\n",
            "-- Model reconstruction: \"An 's company 's the company 's the company 's the company 's the company 's\"\n",
            "(2) step 700: training ELBO (KL) = -142.31 (6.16) -- KL weight = 0.12 -- validation ELBO (KL) = -136.21 (6.22)\n",
            "(2) step 800: training ELBO (KL) = -141.06 (6.29) -- KL weight = 0.14 -- validation ELBO (KL) = -135.10 (6.06)\n",
            "(2) step 900: training ELBO (KL) = -139.68 (6.23) -- KL weight = 0.16 -- validation ELBO (KL) = -133.89 (6.03)\n",
            "(2) step 1000: training ELBO (KL) = -139.16 (6.24) -- KL weight = 0.18 -- validation ELBO (KL) = -132.51 (6.29)\n",
            "(2) step 1100: training ELBO (KL) = -137.52 (6.24) -- KL weight = 0.20 -- validation ELBO (KL) = -131.78 (5.99)\n",
            "(2) step 1200: training ELBO (KL) = -138.60 (6.19) -- KL weight = 0.21 -- validation ELBO (KL) = -130.44 (6.56)\n",
            "Finished epoch 2\n",
            "Evaluation epoch 2:\n",
            " - validation perplexity: 715.37\n",
            " - validation NLL: 140.86\n",
            " - validation ELBO (KL) = -130.81 (5.89)\n",
            "-- Original sentence: \"Such agency self-help borrowing is unauthorized and expensive far more expensive than direct Treasury borrowing said Rep. Fortney Stark D. Calif. the bill 's chief sponsor\"\n",
            "-- Model reconstruction: \"An U.S. and the company 's the company 's the company 's the company 's the company 's company 's a share of the company 's company 's company 's company 's a share\"\n",
            "(3) step 1300: training ELBO (KL) = -136.13 (6.13) -- KL weight = 0.23 -- validation ELBO (KL) = -129.86 (6.16)\n",
            "(3) step 1400: training ELBO (KL) = -134.69 (6.06) -- KL weight = 0.25 -- validation ELBO (KL) = -129.78 (5.78)\n",
            "(3) step 1500: training ELBO (KL) = -136.03 (6.06) -- KL weight = 0.27 -- validation ELBO (KL) = -128.95 (5.88)\n",
            "(3) step 1600: training ELBO (KL) = -133.77 (5.89) -- KL weight = 0.29 -- validation ELBO (KL) = -128.43 (6.02)\n",
            "(3) step 1700: training ELBO (KL) = -134.03 (5.85) -- KL weight = 0.30 -- validation ELBO (KL) = -128.45 (5.44)\n",
            "(3) step 1800: training ELBO (KL) = -134.17 (5.75) -- KL weight = 0.32 -- validation ELBO (KL) = -127.59 (5.84)\n",
            "Finished epoch 3\n",
            "Evaluation epoch 3:\n",
            " - validation perplexity: 602.25\n",
            " - validation NLL: 137.17\n",
            " - validation ELBO (KL) = -127.29 (5.61)\n",
            "-- Original sentence: \"Frank Tremdine\"\n",
            "-- Model reconstruction: \"Fees\"\n",
            "(4) step 1900: training ELBO (KL) = -134.20 (5.71) -- KL weight = 0.34 -- validation ELBO (KL) = -126.99 (5.76)\n",
            "(4) step 2000: training ELBO (KL) = -131.95 (5.63) -- KL weight = 0.36 -- validation ELBO (KL) = -127.19 (5.29)\n",
            "(4) step 2100: training ELBO (KL) = -131.05 (5.56) -- KL weight = 0.38 -- validation ELBO (KL) = -126.74 (5.31)\n",
            "(4) step 2200: training ELBO (KL) = -130.80 (5.45) -- KL weight = 0.39 -- validation ELBO (KL) = -125.90 (5.62)\n",
            "(4) step 2300: training ELBO (KL) = -132.88 (5.49) -- KL weight = 0.41 -- validation ELBO (KL) = -126.27 (5.09)\n",
            "(4) step 2400: training ELBO (KL) = -132.41 (5.35) -- KL weight = 0.43 -- validation ELBO (KL) = -125.85 (5.14)\n",
            "Finished epoch 4\n",
            "Evaluation epoch 4:\n",
            " - validation perplexity: 535.67\n",
            " - validation NLL: 134.66\n",
            " - validation ELBO (KL) = -125.73 (5.02)\n",
            "-- Original sentence: \"For the third year in a row consumers voted Bill Cosby first and James Garner second in persuasiveness as spokesmen in TV commercials according to Video Storyboard Tests New York\"\n",
            "-- Model reconstruction: \"For the nine months of the company 's largest company 's largest company 's largest company said it was n't disclosed to be a share in the U.S. and the company 's stock\"\n",
            "(5) step 2500: training ELBO (KL) = -131.23 (5.29) -- KL weight = 0.45 -- validation ELBO (KL) = -125.49 (5.21)\n",
            "(5) step 2600: training ELBO (KL) = -129.94 (5.27) -- KL weight = 0.46 -- validation ELBO (KL) = -125.52 (4.94)\n",
            "(5) step 2700: training ELBO (KL) = -130.10 (5.23) -- KL weight = 0.48 -- validation ELBO (KL) = -124.69 (5.39)\n",
            "(5) step 2800: training ELBO (KL) = -130.64 (5.22) -- KL weight = 0.50 -- validation ELBO (KL) = -124.88 (5.10)\n",
            "(5) step 2900: training ELBO (KL) = -127.83 (5.06) -- KL weight = 0.52 -- validation ELBO (KL) = -124.89 (4.85)\n",
            "(5) step 3000: training ELBO (KL) = -129.89 (5.01) -- KL weight = 0.54 -- validation ELBO (KL) = -124.44 (4.95)\n",
            "(5) step 3100: training ELBO (KL) = -129.83 (4.96) -- KL weight = 0.55 -- validation ELBO (KL) = -124.13 (5.06)\n",
            "Finished epoch 5\n",
            "Evaluation epoch 5:\n",
            " - validation perplexity: 496.90\n",
            " - validation NLL: 133.05\n",
            " - validation ELBO (KL) = -124.52 (4.75)\n",
            "-- Original sentence: \"Partisans of the two combatants sat side by side in the 49,000-plus seats of Oakland Coliseum and while they cheered favorites and booed the opposition hostilities advanced no further at least as far as I could see\"\n",
            "-- Model reconstruction: \"According to the company 's largest company 's largest company said it expects to be a share in the U.S. and the company 's largest stock market and the company 's largest company 's largest stock\"\n",
            "(6) step 3200: training ELBO (KL) = -127.23 (4.95) -- KL weight = 0.57 -- validation ELBO (KL) = -124.43 (4.78)\n",
            "(6) step 3300: training ELBO (KL) = -127.83 (4.87) -- KL weight = 0.59 -- validation ELBO (KL) = -124.07 (4.84)\n",
            "(6) step 3400: training ELBO (KL) = -129.32 (4.77) -- KL weight = 0.61 -- validation ELBO (KL) = -123.97 (4.73)\n",
            "(6) step 3500: training ELBO (KL) = -127.44 (4.71) -- KL weight = 0.62 -- validation ELBO (KL) = -123.98 (4.49)\n",
            "(6) step 3600: training ELBO (KL) = -129.94 (4.70) -- KL weight = 0.64 -- validation ELBO (KL) = -124.00 (4.47)\n",
            "(6) step 3700: training ELBO (KL) = -127.64 (4.54) -- KL weight = 0.66 -- validation ELBO (KL) = -123.83 (4.41)\n",
            "Finished epoch 6\n",
            "Evaluation epoch 6:\n",
            " - validation perplexity: 469.09\n",
            " - validation NLL: 131.81\n",
            " - validation ELBO (KL) = -124.16 (4.23)\n",
            "-- Original sentence: \"So what if Steinbach had struck just seven home runs in 130 regular-season games and batted in the seventh position of the A 's lineup\"\n",
            "-- Model reconstruction: \"So far the company 's largest company said it will be sold to be a share in the third quarter ended Sept. 30 % to yield million from 1.03 million or 1.02 cents a share\"\n",
            "(7) step 3800: training ELBO (KL) = -126.94 (4.49) -- KL weight = 0.68 -- validation ELBO (KL) = -124.13 (4.24)\n",
            "(7) step 3900: training ELBO (KL) = -129.05 (4.39) -- KL weight = 0.70 -- validation ELBO (KL) = -123.96 (4.19)\n",
            "(7) step 4000: training ELBO (KL) = -127.66 (4.34) -- KL weight = 0.71 -- validation ELBO (KL) = -123.74 (4.29)\n",
            "(7) step 4100: training ELBO (KL) = -125.52 (4.28) -- KL weight = 0.73 -- validation ELBO (KL) = -123.66 (4.20)\n",
            "(7) step 4200: training ELBO (KL) = -125.75 (4.19) -- KL weight = 0.75 -- validation ELBO (KL) = -123.58 (4.20)\n",
            "(7) step 4300: training ELBO (KL) = -127.92 (4.11) -- KL weight = 0.77 -- validation ELBO (KL) = -124.06 (3.85)\n",
            "Finished epoch 7\n",
            "Evaluation epoch 7:\n",
            " - validation perplexity: 448.89\n",
            " - validation NLL: 130.87\n",
            " - validation ELBO (KL) = -123.76 (4.00)\n",
            "-- Original sentence: \"The bill would allow the secretary to reject a buy-out if sufficient information has n't been provided or if the buy-out is likely to weaken the carrier financially result in a substantial reduction in size of the airline through disposal of assets or give control to a foreign interest\"\n",
            "-- Model reconstruction: \"The company said it was a new company for the first quarter of the company 's largest stock market and the company 's largest company said it was n't disclosed to be a new company for the company 's largest stock market\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "(8) step 4400: training ELBO (KL) = -126.30 (3.98) -- KL weight = 0.79 -- validation ELBO (KL) = -124.19 (3.89)\n",
            "(8) step 4500: training ELBO (KL) = -126.33 (3.92) -- KL weight = 0.80 -- validation ELBO (KL) = -123.69 (3.83)\n",
            "(8) step 4600: training ELBO (KL) = -125.48 (3.85) -- KL weight = 0.82 -- validation ELBO (KL) = -124.16 (3.67)\n",
            "(8) step 4700: training ELBO (KL) = -126.25 (3.76) -- KL weight = 0.84 -- validation ELBO (KL) = -123.68 (3.68)\n",
            "(8) step 4800: training ELBO (KL) = -126.82 (3.70) -- KL weight = 0.86 -- validation ELBO (KL) = -123.87 (3.56)\n",
            "(8) step 4900: training ELBO (KL) = -126.68 (3.58) -- KL weight = 0.88 -- validation ELBO (KL) = -123.70 (3.58)\n",
            "Finished epoch 8\n",
            "Evaluation epoch 8:\n",
            " - validation perplexity: 430.23\n",
            " - validation NLL: 129.96\n",
            " - validation ELBO (KL) = -123.55 (3.52)\n",
            "-- Original sentence: \"Sales climbed to an estimated 245 million in fiscal 1989 ended Aug. 31 from 99.9 million in fiscal 1985\"\n",
            "-- Model reconstruction: \"The company said it expects to be a share in the third quarter\"\n",
            "(9) step 5000: training ELBO (KL) = -127.50 (3.48) -- KL weight = 0.89 -- validation ELBO (KL) = -123.99 (3.37)\n",
            "(9) step 5100: training ELBO (KL) = -125.34 (3.42) -- KL weight = 0.91 -- validation ELBO (KL) = -123.79 (3.43)\n",
            "(9) step 5200: training ELBO (KL) = -125.32 (3.41) -- KL weight = 0.93 -- validation ELBO (KL) = -123.97 (3.23)\n",
            "(9) step 5300: training ELBO (KL) = -126.44 (3.23) -- KL weight = 0.95 -- validation ELBO (KL) = -123.90 (3.31)\n",
            "(9) step 5400: training ELBO (KL) = -127.45 (3.12) -- KL weight = 0.96 -- validation ELBO (KL) = -124.60 (2.88)\n",
            "(9) step 5500: training ELBO (KL) = -126.23 (3.01) -- KL weight = 0.98 -- validation ELBO (KL) = -124.40 (2.88)\n",
            "(9) step 5600: training ELBO (KL) = -125.90 (2.88) -- KL weight = 1.00 -- validation ELBO (KL) = -124.14 (2.93)\n",
            "Finished epoch 9\n",
            "Evaluation epoch 9:\n",
            " - validation perplexity: 418.01\n",
            " - validation NLL: 129.34\n",
            " - validation ELBO (KL) = -124.17 (2.85)\n",
            "-- Original sentence: \"On the screens only two forlorn blue figures remained but the index had recovered a few points and was off about 140\"\n",
            "-- Model reconstruction: \"On the nine months ended Sept. 30 % to yield billion from 1.18 million or 1.03 a share from discontinued million or 1.03 a share from discontinued million or 1.03 a share a year earlier\"\n",
            "(10) step 5700: training ELBO (KL) = -125.42 (2.81) -- KL weight = 1.00 -- validation ELBO (KL) = -124.52 (2.78)\n",
            "(10) step 5800: training ELBO (KL) = -125.72 (2.74) -- KL weight = 1.00 -- validation ELBO (KL) = -124.64 (2.64)\n",
            "(10) step 5900: training ELBO (KL) = -124.87 (2.67) -- KL weight = 1.00 -- validation ELBO (KL) = -124.97 (2.51)\n",
            "(10) step 6000: training ELBO (KL) = -126.58 (2.70) -- KL weight = 1.00 -- validation ELBO (KL) = -124.72 (2.53)\n",
            "(10) step 6100: training ELBO (KL) = -126.10 (2.59) -- KL weight = 1.00 -- validation ELBO (KL) = -124.48 (2.56)\n",
            "(10) step 6200: training ELBO (KL) = -125.40 (2.58) -- KL weight = 1.00 -- validation ELBO (KL) = -124.48 (2.58)\n",
            "Finished epoch 10\n",
            "Evaluation epoch 10:\n",
            " - validation perplexity: 409.64\n",
            " - validation NLL: 128.91\n",
            " - validation ELBO (KL) = -124.31 (2.59)\n",
            "-- Original sentence: \"However Maxus said it believes the reserves in the field are about 10 million barrels of oil\"\n",
            "-- Model reconstruction: \"Some analysts say the company 's largest stock market is n't disclosed\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h2GpYHvqScSK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Let's plot the training and validation statistics:"
      ]
    },
    {
      "metadata": {
        "id": "IYnp8E4MScSL",
        "colab_type": "code",
        "colab": {},
        "outputId": "1c71f579-d341-466e-b711-b943768b33bb"
      },
      "cell_type": "code",
      "source": [
        "steps, training_ELBO = list(zip(*train_ELBOs))\n",
        "_, training_KL = list(zip(*train_KLs))\n",
        "_, val_ELBO = list(zip(*val_ELBOs))\n",
        "_, val_KL = list(zip(*val_KLs))\n",
        "epochs, val_ppl = list(zip(*val_perplexities))\n",
        "_, val_NLL = list(zip(*val_NLLs))\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 5))\n",
        "\n",
        "# Plot training ELBO and KL\n",
        "ax1.set_title(\"Training ELBO\")\n",
        "ax1.plot(steps, training_ELBO, \"-o\")\n",
        "ax2.set_title(\"Training KL\")\n",
        "ax2.plot(steps, training_KL, \"-o\")\n",
        "plt.show()\n",
        "\n",
        "# Plot validation ELBO and KL\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 5))\n",
        "ax1.set_title(\"Validation ELBO\")\n",
        "ax1.plot(steps, val_ELBO, \"-o\", color=\"orange\")\n",
        "ax2.set_title(\"Validation KL\")\n",
        "ax2.plot(steps, val_KL, \"-o\",  color=\"orange\")\n",
        "plt.show()\n",
        "\n",
        "# Plot validation perplexities.\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 5))\n",
        "ax1.set_title(\"Validation perplexity\")\n",
        "ax1.plot(epochs, val_ppl, \"-o\", color=\"orange\")\n",
        "ax2.set_title(\"Validation NLL\")\n",
        "ax2.plot(epochs, val_NLL, \"-o\",  color=\"orange\")\n",
        "plt.show()\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AF4L6yyKScSP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's load the best model according to validation perplexity and compute its perplexity on the test data:"
      ]
    },
    {
      "metadata": {
        "id": "K25svlmSScSQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "013d604e-a3d4-41bd-d33e-b02ca9c27b5f"
      },
      "cell_type": "code",
      "source": [
        "# Load the best model from disk.\n",
        "model = BowmanLM(vocab_size=vocab.size(), \n",
        "                 emb_size=emb_size, \n",
        "                 hidden_size=hidden_size, \n",
        "                 latent_size=latent_size, \n",
        "                 pad_idx=vocab[PAD_TOKEN],\n",
        "                 dropout=dropout,\n",
        "                 bidirectional=bidirectional_encoder)\n",
        "model.load_state_dict(torch.load(best_model))\n",
        "model = model.to(device)\n",
        "\n",
        "# Compute test perplexity and ELBO.\n",
        "test_perplexity, test_NLL = eval_perplexity(model, test_dataset, vocab, \n",
        "                                            device, n_importance_samples)\n",
        "test_rec_loss, test_KL = eval_elbo(model, test_dataset, vocab, device)\n",
        "test_ELBO = test_rec_loss - test_KL\n",
        "print(\"test ELBO (KL) = %.2f (%.2f) -- test perplexity = %.2f -- test NLL = %.2f\" % \n",
        "      (test_ELBO, test_KL, test_perplexity, test_NLL))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test ELBO (KL) = 123.12 (2.63) -- test perplexity = 414.65 -- test NLL = 127.87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DlxoPIa_ScSS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Qualitative analysis\n",
        "\n",
        "Let's have a look at what how our trained model interacts with the learned latent space. First let's greedily decode some samples from the prior to assess the diversity of the model:"
      ]
    },
    {
      "metadata": {
        "id": "1x9siL9aScST",
        "colab_type": "code",
        "colab": {},
        "outputId": "1b6f68e3-b0b3-41f8-f3e7-cc34407a58e8"
      },
      "cell_type": "code",
      "source": [
        "# Generate 10 samples from the standard normal prior.\n",
        "num_prior_samples = 10\n",
        "pz = Normal(torch.zeros(num_prior_samples, latent_size), \n",
        "            torch.ones(num_prior_samples, latent_size))\n",
        "z = pz.sample()\n",
        "z = z.to(device)\n",
        "\n",
        "# Use the greedy decoding algorithm to generate sentences.\n",
        "predictions = greedy_decode(model, z, vocab)\n",
        "predictions = batch_to_sentences(predictions, vocab)\n",
        "for num, prediction in enumerate(predictions):\n",
        "    print(\"%d: %s\" % (num+1, prediction))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1: The company said it expects to be a new company in the U.S. and the company 's largest largest company\n",
            "2: The company said it expects to be a share in the third quarter\n",
            "3: It 's a lot of the U.S. and the company 's largest market and the company 's largest market\n",
            "4: The company said it will be sold by the company 's largest largest company and the company 's largest largest company said it expects to be a loss of the company 's largest stock market\n",
            "5: The company said it expects to be a share in the third quarter\n",
            "6: One of the company 's largest largest company said it expects to be a share\n",
            "7: The company said it expects to be a share\n",
            "8: The company said it will be sold by the company 's largest largest company and the company 's largest largest company said it expects to be a loss of the company 's largest stock market\n",
            "9: The company said it expects to be a share in the third quarter\n",
            "10: Some analysts say the company 's largest stock market\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O7T4hLydScSV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now have a look how good the model is at reconstructing sentences from the test dataset using the approximate posterior mean and a couple of samples:"
      ]
    },
    {
      "metadata": {
        "id": "6fi-g4ASScSW",
        "colab_type": "code",
        "colab": {},
        "outputId": "9ba5ba89-3227-44b4-8142-52f5c610509d"
      },
      "cell_type": "code",
      "source": [
        "# Pick a random test sentence.\n",
        "test_sentence = test_dataset[np.random.choice(len(test_dataset))]\n",
        "\n",
        "# Infer q(z|x).\n",
        "x_in, _, seq_mask, seq_len = create_batch([test_sentence], vocab, device)\n",
        "qz = model.infer(x_in, seq_mask, seq_len)\n",
        "\n",
        "# Decode using the mean.\n",
        "z_mean = qz.mean\n",
        "mean_reconstruction = greedy_decode(model, z_mean, vocab)\n",
        "mean_reconstruction = batch_to_sentences(mean_reconstruction, vocab)[0]\n",
        "\n",
        "print(\"Original: \\\"%s\\\"\" % test_sentence)\n",
        "print(\"Posterior mean reconstruction: \\\"%s\\\"\" % mean_reconstruction)\n",
        "\n",
        "# Decode a couple of samples from the approximate posterior.\n",
        "for s in range(3):\n",
        "    z = qz.sample()\n",
        "    sample_reconstruction = greedy_decode(model, z, vocab)\n",
        "    sample_reconstruction = batch_to_sentences(sample_reconstruction, vocab)[0]\n",
        "    print(\"Posterior sample reconstruction (%d): \\\"%s\\\"\" % (s+1, sample_reconstruction))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: \"To make point it is challenging the Italian government to explain reports that Olivetti may have supplied the Soviet Union with sophisticated computer-driven devices that could be used to build parts for combat aircraft\"\n",
            "Posterior mean reconstruction: \"A spokesman said the company 's largest largest company said it will be sold by the company 's largest largest company and the company 's largest largest company said\"\n",
            "Posterior sample reconstruction (1): \"A spokesman said the company 's largest largest company said it will be sold by the company 's largest largest company and the company 's largest largest company said\"\n",
            "Posterior sample reconstruction (2): \"The company said it will be sold by the company 's largest largest company and the company 's largest largest company said it expects to be a loss of the company 's largest stock market\"\n",
            "Posterior sample reconstruction (3): \"In addition to the U.S. and the company 's largest largest company said it will be sold by the company 's largest largest company and the company 's largest largest company said\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fi0THhQ5ScSZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can also qualitatively assess the smoothness of the learned latent space by interpolating between two sentences in the test set:"
      ]
    },
    {
      "metadata": {
        "id": "y8Ai6FxnScSZ",
        "colab_type": "code",
        "colab": {},
        "outputId": "0afefe54-14bc-45eb-ac24-a809a4cbd4b3"
      },
      "cell_type": "code",
      "source": [
        "# Pick a random test sentence.\n",
        "test_sentence_1 = test_dataset[np.random.choice(len(test_dataset))]\n",
        "\n",
        "# Infer q(z|x).\n",
        "x_in, _, seq_mask, seq_len = create_batch([test_sentence_1], vocab, device)\n",
        "qz = model.infer(x_in, seq_mask, seq_len)\n",
        "qz_1 = qz.mean\n",
        "\n",
        "# Pick a random second test sentence.\n",
        "test_sentence_2 = test_dataset[np.random.choice(len(test_dataset))]\n",
        "\n",
        "# Infer q(z|x) again.\n",
        "x_in, _, seq_mask, seq_len = create_batch([test_sentence_2], vocab, device)\n",
        "qz = model.infer(x_in, seq_mask, seq_len)\n",
        "qz_2 = qz.mean\n",
        "\n",
        "# Now interpolate between the two means and generate sentences between those.\n",
        "num_sentences = 5\n",
        "print(\"Sentence 1: \\\"%s\\\"\" % test_sentence_1)\n",
        "for alpha in np.linspace(start=0., stop=1., num=num_sentences):\n",
        "    z = (1-alpha) * qz_1 + alpha * qz_2\n",
        "    reconstruction = greedy_decode(model, z, vocab)\n",
        "    reconstruction = batch_to_sentences(reconstruction, vocab)[0]\n",
        "    print(\"(1-%.2f) * qz1.mean + %.2f qz2.mean: \\\"%s\\\"\" % (alpha, alpha, reconstruction))\n",
        "print(\"Sentence 2: \\\"%s\\\"\" % test_sentence_2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence 1: \"In recent months a string of cross-border mergers and joint ventures have reshaped the once-balkanized world of European arms manufacture\"\n",
            "(1-0.00) * qz1.mean + 0.00 qz2.mean: \"In addition to the U.S. and the company 's largest stock market is n't disclosed\"\n",
            "(1-0.25) * qz1.mean + 0.25 qz2.mean: \"In the past few years the company 's largest largest company said it was n't disclosed\"\n",
            "(1-0.50) * qz1.mean + 0.50 qz2.mean: \"In the past few years the company 's largest largest company said it was n't disclosed\"\n",
            "(1-0.75) * qz1.mean + 0.75 qz2.mean: \"In the past few years the company 's largest largest company said it was n't disclosed\"\n",
            "(1-1.00) * qz1.mean + 1.00 qz2.mean: \"But the company 's largest largest company said it was n't disclosed to be a good\"\n",
            "Sentence 2: \"If true the court wrote this contention would justify dismissal of these actions on prudential grounds\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C_dLVVbiScSd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}