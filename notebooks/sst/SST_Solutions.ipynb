{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SST-Solutions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Udt3kHMdWvYe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will need to import some helper code, so we need to run this"
      ]
    },
    {
      "metadata": {
        "id": "U8eXUCRiWvYi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "nb_dir = os.path.split(os.getcwd())[0]\n",
        "if nb_dir not in sys.path:\n",
        "    sys.path.append(nb_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rYhItDYMZi6a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Colab\n",
        "\n",
        "We will need to download some data for this notebook, so if you are using [colab](https://colab.research.google.com), set the `using_colab` flag below to `True` in order to clone our [github repo](https://github.com/probabll/dgm4nlp)."
      ]
    },
    {
      "metadata": {
        "id": "_shCMftIx1rW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "using_colab = True\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N-fFME2OW22i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if using_colab:\n",
        "  !rm -fr dgm4nlp sst\n",
        "  !git clone https://github.com/probabll/dgm4nlp.git\n",
        "  !cp -R dgm4nlp/notebooks/sst ./  \n",
        "  !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l_7NCZlZacNu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can start our lab."
      ]
    },
    {
      "metadata": {
        "id": "s9mH-rUhWvYq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "# CPU should be fine for this lab\n",
        "device = torch.device('cpu')  \n",
        "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
        "from collections import OrderedDict\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "okMoxTJ9bWjc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sentiment Classification \n",
        "\n",
        "\n",
        "We are going to augment a sentiment classifier with a layer of discrete latent variables which will help us improve the model's interpretability. But first, let's quickly review the baseline task.\n",
        "\n",
        "\n",
        "In sentiment classification, we have some text input $x = \\langle x_1, \\ldots, x_n \\rangle$, e.g. a sentence or short paragraph, which expresses a certain sentiment $y$, i.e. one of $K$ classes, towards a subject (e.g. a film or a product). \n",
        "\n",
        "\n",
        "\n",
        "We can learn a sentiment classifier by learning a categorical distribution over classes for a given input:\n",
        "\n",
        "\\begin{align}\n",
        "Y|x &\\sim \\text{Cat}(f(x; \\theta))\n",
        "\\end{align}\n",
        "\n",
        "where the Categorical pmf is $\\text{Cat}(y|\\pi) = \\pi_y$.\n",
        "\n",
        "A categorical distribution over $K$ classes is parameterised by a $K$-dimensional probability vector, here we use a neural network $f$ to map from the input to this probability vector. Technically we say *a neural network parameterise our model*, that is, it computes the parameters of our categorical observation model. The figure below is a graphical depiction of the model: circled nodes are random variables (a shaded node is an observed variable), uncircled nodes are deterministic, a plate indicates multiple draws.\n",
        "\n",
        "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/classifier.png\"  height=\"100\">\n",
        "\n",
        "The neural network (NN) $f(\\cdot; \\theta)$ has parameters of its own, i.e. the weights of the various architecture blocks used, which we denoted generically by $\\theta$.\n",
        "\n",
        "Suppose we have a dataset $\\mathcal D = \\{(x^{(1)}, y^{(1)}), \\ldots, (x^{(N)}, y^{(N)})\\}$ containing $N$ i.i.d. observations. Then we can use the log-likelihood function \n",
        "\\begin{align}\n",
        "\\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{N} \\log P(y^{(k)}|x^{(k)}, \\theta) \\\\\n",
        "&= \\sum_{k=1}^{N} \\log \\text{Cat}(y^{(k)}|f(x^{(k)}; \\theta))\n",
        "\\end{align}\n",
        " to estimate $\\theta$ by maximisation:\n",
        " \\begin{align}\n",
        " \\theta^\\star = \\arg\\max_{\\theta \\in \\Theta} \\mathcal L(\\theta|\\mathcal D) ~ .\n",
        " \\end{align}\n",
        " \n",
        "\n",
        "We can use stochastic gradient-ascent to find a local optimum of $\\mathcal L(\\theta|\\mathcal D)$, which only requires a gradient estimate:\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla_\\theta \\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{|\\mathcal D|} \\nabla_\\theta  \\log P(y^{(k)}|x^{(k)}, \\theta) \\\\ \n",
        "&= \\sum_{k=1}^{|\\mathcal D|} \\frac{1}{N} N \\nabla_\\theta  \\log P(y^{(k)}|x^{(k)}, \\theta)  \\\\\n",
        "&= \\mathbb E_{\\mathcal U(1/N)} \\left[ N \\nabla_\\theta  \\log P(y^{(K)}|x^{(K)}, \\theta) \\right]  \\\\\n",
        "&\\overset{\\text{MC}}{\\approx} \\frac{N}{M} \\sum_{m=1}^M \\nabla_\\theta  \\log P(y^{(k_m)}|x^{(k_m)}, \\theta) \\\\\n",
        "&\\text{where }K_m \\sim \\mathcal U(1/N)\n",
        "\\end{align}\n",
        "\n",
        "This is a Monte Carlo (MC) estimate of the gradient computed on $M$ data points selected uniformly at random from $\\mathcal D$.\n",
        "\n",
        "For as long as $f$ remains differentiable wrt to its inputs and parameters, we can rely on automatic differentiation to obtain gradient estimates.\n",
        "\n",
        "In what follows we show how to design $f$ and how to extend this basic model to a latent-variable model.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4LUjyO-39zan",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "We provide you some code to load the data (see `sst.sstutil.examplereader`). Play with the snippet below and inspect a few training instances:"
      ]
    },
    {
      "metadata": {
        "id": "4z8Bt5no9z6w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sst.sstutil import examplereader, Vocabulary, load_glove    \n",
        "\n",
        "\n",
        "# Let's load the data into memory.\n",
        "print(\"Loading data\")\n",
        "train_data = list(examplereader('sst/data/sst/train.txt'))\n",
        "dev_data = list(examplereader('sst/data/sst/dev.txt'))\n",
        "test_data = list(examplereader('sst/data/sst/test.txt'))\n",
        "\n",
        "print(\"train\", len(train_data))\n",
        "print(\"dev\", len(dev_data))\n",
        "print(\"test\", len(test_data))\n",
        "\n",
        "print('\\n# Examples')\n",
        "example = dev_data[0]\n",
        "print(\"First dev example:\", example)\n",
        "print(\"First dev example tokens:\", example.tokens)\n",
        "print(\"First dev example label:\", example.label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lB2lEsNuWvYx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Architecture\n",
        "\n",
        "\n",
        "The function $f$ conditions on a high-dimensional input (i.e. text), so we need to convert it to continuous real vectors. This is the job an *encoder*. \n",
        "\n",
        "**Embedding Layer**\n",
        "\n",
        "The first step is to convert the words in $x$ to vectors, which in this lab we will do with a pre-trained embedding layer (we will use GloVe).\n",
        "\n",
        "We will denote the embedding of the $i$th word of the input by:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf x_i = \\text{glove}(x_i)\n",
        "\\end{equation}\n",
        "\n",
        "**Encoder Layer**\n",
        "\n",
        "In this lab, an encoder takes a sequence of input vectors $\\mathbf x_1^n$, each $I$-dimensional, and produces a sequence of output vectors $\\mathbf t_1^n$, each $O$-dimensional and a summary vector $\\mathbf h \\in \\mathbb R^O$:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathbf t_1^n, \\mathbf h = \\text{encoder}(\\mathbf x_1^n; \\theta_{\\text{enc}})\n",
        "\\end{equation}\n",
        "\n",
        "where we use $\\theta_{\\text{enc}}$ to denote the subset of parameters in $\\theta$ that are specific to this encoder block. \n",
        "\n",
        "*Remark:* in practice for a correct batched implementation, our encoders also take a mask matrix and a vector of lengths.\n",
        "\n",
        "Examples of encoding functions can be a feed-forward NN (with an aggregator based on sum or average/max pooling) or a recurrent NN (e.g. an LSTM/GRU). Other architectures are also possible.\n",
        "\n",
        "**Output Layer**\n",
        "\n",
        "From our summary vector $\\mathbf h$, we need to parameterise a categorical distribution over $K$ classes, thus we use\n",
        "\n",
        "\\begin{align}\n",
        "f(x; \\theta) &= \\text{softmax}(\\text{dense}_K(\\mathbf h; \\theta_{\\text{output}}))\n",
        "\\end{align}\n",
        "\n",
        "where $\\text{dense}_K$ is a dense layer with $K=5$ outputs and $\\theta_{\\text{output}}$ corresponds to its parameters (weight matrix and bias vector). Note that we need to use the softmax activation function in order to guarantee that the output of $f$ is a normalised probability vector.\n"
      ]
    },
    {
      "metadata": {
        "id": "Kc15Nv2i41cq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementation\n",
        "\n",
        "To leave an indication of the shape of tensors in the code, we use the following convention\n",
        "\n",
        "```python\n",
        "[B, T, D]\n",
        "```\n",
        "\n",
        "where `B` stands for `batch_size`, `T` stands for `time` (or rather *maximum sequence length*), and `D` is the size of the representation.\n",
        "\n",
        "\n",
        "Consider the following abstract Encoder class:"
      ]
    },
    {
      "metadata": {
        "tags": [
          "encoders"
        ],
        "id": "xwEPXT2MWvYz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    An Encoder for us is a function that\n",
        "      1. transforms a sequence of I-dimensional vectors into a sequence of O-dimensional vectors\n",
        "      2. summarises a sequence of I-dimensional vectors into one O-dimensional vector\n",
        "      \n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "    def forward(self, inputs, mask, lengths):\n",
        "        \"\"\"\n",
        "        The inputs are batch-first tensors.\n",
        "        \n",
        "        :param inputs: [B, T, I]\n",
        "        :param mask: [B, T]\n",
        "        :param lengths: [B]\n",
        "        :returns: [B, T, O], [B, O]\n",
        "            where the first tensor is the transformed input\n",
        "            and the second tensor is a summary of all inputs\n",
        "        \"\"\"\n",
        "        pass\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WA5wmkcRg9Am",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's start easy, implement a *bag of words* encoder:"
      ]
    },
    {
      "metadata": {
        "id": "U-9hLQ0lF5SG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BagOfWordsEncoder(Encoder):\n",
        "    \"\"\"\n",
        "    This encoder does not transform the input sequence, \n",
        "     and its summary output is just a sum.\n",
        "    \"\"\"\n",
        "    \n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3o7s43wQF62X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# SOLUTION\n",
        "class BagOfWordsEncoder(Encoder):\n",
        "    \"\"\"\n",
        "    This encoder does not transform the input sequence, \n",
        "     and its summary output is just a sum.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(BagOfWordsEncoder, self).__init__()\n",
        "        \n",
        "    def forward(self, inputs, mask, lengths, **kwargs):\n",
        "        return inputs, (inputs * mask.unsqueeze(-1).float()).sum(dim=1) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IS7x0hLrUXfN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can also consider implementing\n",
        "\n",
        "* a feed-forward encoder with average pooling\n",
        "* and a biLSTM encoder\n",
        "\n",
        "but these are certainly optional."
      ]
    },
    {
      "metadata": {
        "id": "BpOGFpK_Uo0-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FFEncoder(Encoder):\n",
        "    \"\"\"\n",
        "    A typical feed-forward NN with tanh hidden activations.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, output_size, \n",
        "                 activation=None, \n",
        "                 hidden_sizes=[], \n",
        "                 aggregator='avg',\n",
        "                 dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param input_size: int\n",
        "        :param output_size: int\n",
        "        :param hidden_sizes: list of integers (dimensionality of hidden layers)\n",
        "        :param aggregator: 'sum' or 'avg'\n",
        "        :param dropout: dropout rate\n",
        "        \"\"\"\n",
        "        pass\n",
        "        \n",
        "    def forward(self, x, mask, lengths):\n",
        "        \"\"\"\n",
        "        :param x: sequence of word embeddings, shape [B, T, I]\n",
        "        :param mask: byte mask that is 0 for invalid positions, shape [B, T]\n",
        "        :param lengths: the lengths of each input sequence [B]\n",
        "        :return: \n",
        "            outputs [B, T, O]\n",
        "            sum/avg pooling [B, O]\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IxQ5djZ_VAvK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "class LSTMEncoder(Encoder):\n",
        "    \"\"\"\n",
        "    This module encodes a sequence using a bidirectional LSTM\n",
        "     it returns the final state\n",
        "     and the hidden states at each time step. Note: we concatenate representations\n",
        "     from the two directions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, \n",
        "                 hidden_size: int = 200,\n",
        "                 batch_first: bool = True,\n",
        "                 bidirectional: bool = True):\n",
        "        \"\"\"\n",
        "        :param in_features:\n",
        "        :param hidden_size:\n",
        "        :param batch_first:\n",
        "        :param bidirectional:\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, mask, lengths):\n",
        "        \"\"\"\n",
        "        Encode sentence x\n",
        "        :param x: sequence of word embeddings, shape [B, T, I]\n",
        "        :param mask: byte mask that is 0 for invalid positions, shape [B, T]\n",
        "        :param lengths: the lengths of each input sequence [B]\n",
        "        :return:\n",
        "            outputs [B, T, O]\n",
        "            final state [B, O]\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s_zz5zIyVkSh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is some helper code to select and return an encoder:"
      ]
    },
    {
      "metadata": {
        "id": "59ZU6JddVjMV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_encoder(layer, in_features, hidden_size, bidirectional=True):\n",
        "    \"\"\"Returns the requested layer.\"\"\"\n",
        "\n",
        "    # TODO: make pass and average layers\n",
        "    if layer == \"bow\":\n",
        "        return BagOfWordsEncoder()\n",
        "    elif layer == 'ff':\n",
        "        return FFEncoder(\n",
        "            in_features, \n",
        "            2 * hidden_size,   # for convenience\n",
        "            hidden_sizes=[hidden_size], \n",
        "            aggregator='avg')\n",
        "    elif layer == \"lstm\":\n",
        "        return LSTMEncoder(\n",
        "            in_features, \n",
        "            hidden_size,\n",
        "            bidirectional=bidirectional)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown layer\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "tags": [
          "encoders"
        ],
        "id": "ISjc0K1aWvY4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#SOLUTION\n",
        "class FFEncoder(Encoder):\n",
        "    \"\"\"\n",
        "    A typical feed-forward NN with tanh hidden activations.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, output_size, \n",
        "                 activation=None, \n",
        "                 hidden_sizes=[], \n",
        "                 aggregator='sum',\n",
        "                 dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param input_size: int\n",
        "        :param output_size: int\n",
        "        :param hidden_sizes: list of integers (dimensionality of hidden layers)\n",
        "        :param aggregator: 'sum' or 'avg'\n",
        "        :param dropout: dropout rate\n",
        "        \"\"\"\n",
        "        super(FFEncoder, self).__init__()\n",
        "        layers = []\n",
        "        if hidden_sizes:                    \n",
        "            for i, size in enumerate(hidden_sizes):\n",
        "                if dropout > 0.:\n",
        "                  layers.append(('dropout%d' % i, nn.Dropout(p=dropout)))\n",
        "                layers.append(('linear%d' % i, nn.Linear(input_size, size)))\n",
        "                layers.append(('tanh%d' % i, nn.Tanh()))\n",
        "                input_size = size\n",
        "        if dropout > 0.:\n",
        "          layers.append(('dropout', nn.Dropout(p=dropout)))\n",
        "        layers.append(('linear', nn.Linear(input_size, output_size)))       \n",
        "        self.layer = nn.Sequential(OrderedDict(layers))     \n",
        "        self.activation = activation\n",
        "        if not aggregator in ['sum', 'avg']:\n",
        "            raise ValueError(\"I can only aggregate outputs using 'sum' or 'avg'\")\n",
        "        self.aggregator = aggregator\n",
        "        \n",
        "    def forward(self, x, mask, lengths):\n",
        "        # [B, T, d]\n",
        "        y = self.layer(x)\n",
        "        if not self.activation is None:\n",
        "            y = self.activation(y)\n",
        "        # [B, d]\n",
        "        s = (y * mask.unsqueeze(-1).float()).sum(dim=1)\n",
        "        if self.aggregator == 'avg':\n",
        "            s /= lengths.unsqueeze(-1).float()\n",
        "        return y, s\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "class LSTMEncoder(Encoder):\n",
        "    \"\"\"\n",
        "    This module encodes a sequence into a single vector using an LSTM,\n",
        "     it also returns the hidden states at each time step.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, hidden_size: int = 200,\n",
        "                 batch_first: bool = True,\n",
        "                 bidirectional: bool = True):\n",
        "        \"\"\"\n",
        "        :param in_features:\n",
        "        :param hidden_size:\n",
        "        :param batch_first:\n",
        "        :param bidirectional:\n",
        "        \"\"\"\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(in_features, hidden_size, batch_first=batch_first,\n",
        "                            bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, x, mask, lengths):\n",
        "        \"\"\"\n",
        "        Encode sentence x\n",
        "        :param x: sequence of word embeddings, shape [B, T, E]\n",
        "        :param mask: byte mask that is 0 for invalid positions, shape [B, T]\n",
        "        :param lengths: the lengths of each input sequence [B]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        packed_sequence = pack_padded_sequence(x, lengths, batch_first=True)\n",
        "        outputs, (hx, cx) = self.lstm(packed_sequence)\n",
        "        outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
        "\n",
        "        # classify from concatenation of final states\n",
        "        if self.lstm.bidirectional:\n",
        "            final = torch.cat([hx[-2], hx[-1]], dim=-1)\n",
        "        else:  # classify from final state\n",
        "            final = hx[-1]\n",
        "\n",
        "        return outputs, final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kY8LZiMN5CHW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sentiment Classification with Latent Rationale\n",
        "\n",
        "A latent rationale is a compact and informative fragment of the input based on which a NN classifier makes its decisions. [Lei et al (2016)](http://aclweb.org/anthology/D16-1011) proposed to induce such rationales along with a regression model for multi-aspect sentiment analsysis, their model is trained via REINFORCE on a dataset of beer reviews.\n",
        "\n",
        "*Remark:* the model we will develop here can be seen as a probabilistic version of their model. The rest of this notebook focus on our own probabilitisc view of the model.\n",
        "\n",
        "The picture below depicts our latent-variable model for rationale extraction:\n",
        "\n",
        "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/rationale.png\"  height=\"200\">\n",
        "\n",
        "where we augment the model with a collection of latent variables $z = \\langle z_1, \\ldots, z_n\\rangle$ where $z_i$ is a binary latent variable. Each latent variable $z_i$ regulates whether or not the input $x_i$ is available to the classifier.  We use $x \\odot z$ to denote the selected words, which, in the terminology of Lei et al, is a latent rationale.\n",
        "\n",
        "Again the classifier parameterises a Categorical distribution over $K=5$ outcomes, though this time it can encode only a selection of the input:\n",
        "\n",
        "\\begin{align}\n",
        "    Z_i & \\sim \\text{Bern}(p_1) \\\\\n",
        "    Y|z,x &\\sim \\text{Cat}(f(x \\odot z; \\theta))\n",
        "\\end{align}\n",
        "\n",
        "where we have a shared and fixed Bernoulli prior (with parameter $p_1$) for all $n$ latent variables.\n",
        "\n",
        "\n",
        "Here is an example design for $f$:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf x_i &= z_i \\, \\text{glove}(x_i) \\\\\n",
        "\\mathbf t_1^n, \\mathbf h &= \\text{encoder}(\\mathbf x_1^n; \\theta_{\\text{enc}}) \\\\\n",
        "f(x \\odot z; \\theta) &= \\text{softmax}(\\text{dense}_K(\\mathbf h; \\theta_{\\text{output}}))\n",
        "\\end{align}\n",
        "\n",
        "where:\n",
        "* $z_i$ either leaves $\\mathbf x_i$ unchanged or turns it into a vector of zeros;\n",
        "* the encoder only sees features from selected inputs, i.e. $x_i$ for which $z_i = 1$;\n",
        "* $\\text{dense}_K$ is a linear layer with $K=5$ outputs.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hDHNxLHMWvY-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prior\n",
        "\n",
        "\n",
        "Our prior is a Bernoulli with fixed parameter $0 < p_1 < 1$:\n",
        "\n",
        "\\begin{align}\n",
        "Z_i & \\sim \\text{Bern}(p_1)\n",
        "\\end{align}\n",
        "\n",
        "As we will be using Bernoulli priors and posteriors, it is a good idea to implement a Bernoulli class:"
      ]
    },
    {
      "metadata": {
        "id": "iCBcHnTsOuDr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Bernoulli:\n",
        "    \"\"\"\n",
        "    This class encapsulates a collection of Bernoulli distributions. \n",
        "    Each Bernoulli is uniquely specified by p_1, where\n",
        "        Bernoulli(X=x|p_1) = pow(p_1, x) + pow(1 - p_1, 1 - x)\n",
        "    is the Bernoulli probability mass function (pmf). \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, logits=None, probs=None):\n",
        "        \"\"\"\n",
        "        We can specify a Bernoulli distribution via a logit or a probability. \n",
        "         You need to specify at least one, and if you specify both, beware that\n",
        "         in this implementation logits will be used.\n",
        "         \n",
        "        Recall that: probs = sigmoid(logits).\n",
        "         \n",
        "        :param logits: a tensor of logits (a logit is defined as log (p_1/p_0))\n",
        "            where p_0 = 1 - p_1\n",
        "        :param probs: a tensor of probabilities, each in (0, 1)\n",
        "        \n",
        "        \"\"\"        \n",
        "        pass\n",
        "    \n",
        "    def sample(self):\n",
        "        \"\"\"Returns a single sample with the same shape as the parameters\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def log_pmf(self, x):\n",
        "        \"\"\"\n",
        "        Assess the log probability of a sample. \n",
        "        :param x: either a single sample (0 or 1) or a tensor of samples with the same shape as the parameters.\n",
        "        :returns: tensor with log probabilities with the same shape as parameters\n",
        "            (if the input is a single sample we broadcast it to the shape of the parameters)\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def kl(self, other: 'Bernoulli'):\n",
        "        \"\"\"\n",
        "        Compute the KL divergence between two Bernoulli distributions (from self to other).\n",
        "        \n",
        "        :return: KL[self||other] with same shape parameters\n",
        "        \"\"\"\n",
        "        pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3uPgpyMSWvZA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# SOLUTION\n",
        "from torch.distributions import Bernoulli as PyTBernoulli\n",
        "\n",
        "class Bernoulli:\n",
        "    \"\"\"\n",
        "    This class encapsulates a collection of Bernoulli distributions. \n",
        "    Each Bernoulli is uniquely specified by p_1, where\n",
        "        Bernoulli(X=x|p_1) = pow(p_1, x) + pow(1 - p_1, 1 - x)\n",
        "    is the Bernoulli probability mass function (pmf).    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, logits=None, probs=None):\n",
        "        \"\"\"\n",
        "        We can specify a Bernoulli distribution via a logit or a probability. \n",
        "         You need to specify at least one, and if you specify both, beware that\n",
        "         in this implementation logits will be used.\n",
        "         \n",
        "        Recall that: probs = sigmoid(logits).\n",
        "         \n",
        "        :param logits: a tensor of logits (a logit is defined as log (p_1/p_0))\n",
        "            where p_0 = 1 - p_1\n",
        "        :param probs: a tensor of probabilities, each in (0, 1)\n",
        "        \n",
        "        \"\"\"        \n",
        "        if probs is None and logits is None:\n",
        "            raise ValueError('I need probabilities or logits')        \n",
        "        if logits is None:            \n",
        "            self.log_p1 = torch.log(probs)\n",
        "            self.log_p0 = torch.log(1. - probs)\n",
        "            self.probs = probs\n",
        "        else:\n",
        "            #self._bernoulli = PyTBernoulli(logits=logits)\n",
        "            #self.log_probs = torch.functional.logsigmoid(logits)\n",
        "            self.log_p1 = torch.nn.functional.logsigmoid(logits)\n",
        "            self.log_p0 = - torch.logsumexp(torch.cat(\n",
        "                (logits.unsqueeze(-1), torch.zeros_like(logits).unsqueeze(-1)), \n",
        "                dim=-1), dim=-1)\n",
        "            self.probs = torch.sigmoid(logits)    \n",
        "    \n",
        "    def sample(self):\n",
        "        \"\"\"Returns a sample with the same shape as the parameters\"\"\"\n",
        "        return torch.bernoulli(self.probs)\n",
        "    \n",
        "    def log_pmf(self, x):\n",
        "        \"\"\"\n",
        "        Assess the log probability of a sample. \n",
        "        :param x: either a single sample (0 or 1) or a tensor of samples with the same shape as the parameters.\n",
        "        :returns: tensor with log probabilities with the same shape as parameters\n",
        "            (if the input is a single sample we broadcast it to the shape of the parameters)\n",
        "        \"\"\"\n",
        "        # x * torch.log(self.probs) + (1 - x) * torch.log(1. - self.probs)\n",
        "        return torch.where(x == 1., self.log_p1, self.log_p0)\n",
        "    \n",
        "    def kl(self, other: 'Bernoulli'):\n",
        "        \"\"\"\n",
        "        Compute the KL divergence between two Bernoulli distributions (from self to other).\n",
        "        \n",
        "        :return: KL[self||other] with same shape parameters\n",
        "        \"\"\"\n",
        "        p1 = torch.exp(self.log_p1)\n",
        "        p0 = torch.exp(self.log_p0)\n",
        "        return p1 * (self.log_p1 - other.log_p1) + p0 * (self.log_p0 - other.log_p0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u0yfkCZlWvZP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classifier\n",
        "\n",
        "The classifier encodes only a selection of the input, which we denote $x \\odot z$, and parameterises a Categorical distribution over $5$ outcomes (sentiment levels).\n",
        "\n",
        "Thus let's implement a Categorical distribution (we will only need to be able to assess its lgo pmf):"
      ]
    },
    {
      "metadata": {
        "id": "F-6JLDnBQcdg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Categorical:\n",
        "    \n",
        "    def __init__(self, log_probs):\n",
        "        # [B, K]: class probs\n",
        "        self.log_probs = log_probs\n",
        "        \n",
        "    def log_pmf(self, x):\n",
        "        \"\"\"\n",
        "        :param x: [B] integers\n",
        "        :returns: [B] scalars (log probabilities)\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IWygoUSFWvZQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#SOLUTION\n",
        "class Categorical:\n",
        "    \n",
        "    def __init__(self, log_probs):\n",
        "        # [B, K]: class probs\n",
        "        self.log_probs = log_probs\n",
        "        \n",
        "    def log_pmf(self, x):\n",
        "        \"\"\"\n",
        "        :param x: [B] integers\n",
        "        \"\"\"\n",
        "        return torch.gather(self.log_probs, 1, x.unsqueeze(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CdrM_YRI8xBF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "and a classifier architecture:\n",
        "\n",
        "* implement the forward method"
      ]
    },
    {
      "metadata": {
        "id": "sz7GaKbgRCd8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    \"\"\"\n",
        "    The Encoder takes an input text (and rationale z) and computes p(y|x,z)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 embed:        nn.Embedding = None,\n",
        "                 hidden_size:  int = 200,\n",
        "                 output_size:  int = 1,\n",
        "                 dropout:      float = 0.1,\n",
        "                 layer:        str = \"pass\",\n",
        "                 ):\n",
        "\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        emb_size = embed.weight.shape[1]\n",
        "        enc_size = hidden_size * 2\n",
        "        # Here we embed the words\n",
        "        self.embed_layer = nn.Sequential(\n",
        "            embed\n",
        "            # , nn.Dropout(p=dropout)\n",
        "        )\n",
        "\n",
        "        self.enc_layer = get_encoder(layer, emb_size, hidden_size)\n",
        "\n",
        "        # and here we predict categorical parameters\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(enc_size, output_size),\n",
        "            nn.LogSoftmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        self.report_params()\n",
        "\n",
        "    def report_params(self):\n",
        "        count = 0\n",
        "        for name, p in self.named_parameters():\n",
        "            if p.requires_grad and \"embed\" not in name:\n",
        "                count += np.prod(list(p.shape))\n",
        "        print(\"{} #params: {}\".format(self.__class__.__name__, count))\n",
        "\n",
        "    def forward(self, x, mask, z) -> Categorical:\n",
        "        \"\"\"\n",
        "        :params x: [B, T, I] word representations\n",
        "        :params mask: [B, T] indicates valid positions\n",
        "        :params z: [B, T] binary selectors\n",
        "        :returns: one Categorical distribution per instance in the batch\n",
        "          each conditioning only on x_i for which z_i = 1\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RAPFOoK6WvZT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#SOLUTION\n",
        "class Classifier(nn.Module):\n",
        "    \"\"\"\n",
        "    The Encoder takes an input text (and rationale z) and computes p(y|x,z)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 embed:        nn.Embedding = None,\n",
        "                 hidden_size:  int = 200,\n",
        "                 output_size:  int = 5,\n",
        "                 dropout:      float = 0.1,\n",
        "                 layer:        str = \"pass\",\n",
        "                 ):\n",
        "\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        emb_size = embed.weight.shape[1]\n",
        "        enc_size = hidden_size * 2\n",
        "        # Here we embed the words\n",
        "        self.embed_layer = nn.Sequential(\n",
        "            embed\n",
        "            # , nn.Dropout(p=dropout)\n",
        "        )\n",
        "\n",
        "        self.enc_layer = get_encoder(layer, emb_size, hidden_size)\n",
        "\n",
        "        # and here we predict categorical parameters\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(enc_size, output_size),\n",
        "            nn.LogSoftmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        self.report_params()\n",
        "\n",
        "    def report_params(self):\n",
        "        count = 0\n",
        "        for name, p in self.named_parameters():\n",
        "            if p.requires_grad and \"embed\" not in name:\n",
        "                count += np.prod(list(p.shape))\n",
        "        print(\"{} #params: {}\".format(self.__class__.__name__, count))\n",
        "\n",
        "    def forward(self, x, mask, z) -> Categorical:\n",
        "        \"\"\"\n",
        "        :params x: [B, T, I] word representations\n",
        "        :params mask: [B, T] indicates valid positions\n",
        "        :params z: [B, T] binary selectors\n",
        "        :returns: one Categorical distribution per instance in the batch\n",
        "          each conditioning only on x_i for which z_i = 1\n",
        "        \"\"\"\n",
        "        \n",
        "        rnn_mask = mask\n",
        "        emb = self.embed_layer(x)\n",
        "\n",
        "        # [B, T]\n",
        "        rnn_mask = z > 0.\n",
        "        # [B, T, 1]\n",
        "        z_mask = z.unsqueeze(-1).float()\n",
        "        # [B, T, E]\n",
        "        emb = emb * z_mask\n",
        "\n",
        "        lengths = mask.long().sum(1)\n",
        "\n",
        "        # encode the sentence\n",
        "        _, final = self.enc_layer(emb, rnn_mask, lengths)\n",
        "\n",
        "        # predict sentiment from final state(s)\n",
        "        log_probs = self.output_layer(final)        \n",
        "        return Categorical(log_probs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p2waCCBF9MaH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "\n",
        "Computing the log-likelihood of an observation requires marginalising over assignments of $z$:\n",
        "\n",
        "\\begin{align}\n",
        "P(y|x,\\theta,p_1) &= \\sum_{z_1 = 0}^1 \\cdots \\sum_{z_n=0}^1 P(z|p_1)\\times P(y|x,z, \\theta) \\\\\n",
        "&= \\sum_{z_1 = 0}^1 \\cdots \\sum_{z_n=0}^1 \\left( \\prod_{i=1}^n \\text{Bern}(z_i|p_1)\\right) \\times \\text{Cat}(y|f(x \\odot z; \\theta)) \n",
        "\\end{align}\n",
        "\n",
        "This is clearly intractable: there are $2^n$ possible assignments to $z$ and because the classifier conditions on all latent selectors, there's no way to simplify the expression.\n",
        "\n",
        "We will avoid computing this intractable marginal by instead employing an independently parameterised inference model.\n",
        "This inference model $Q(z|x, y, \\lambda)$ is an approximation to the true postrerior $P(z|x, y, \\theta, p_1)$, and we use $\\lambda$ to denote its parameters.\n"
      ]
    },
    {
      "metadata": {
        "id": "0jcVdYTg8Wun",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We make a *mean field* assumption, whereby we model latent variables independently given the input:\n",
        "\\begin{align}\n",
        "Q(z|x, y, \\lambda) \n",
        "    &= \\prod_{i=1}^{n} Q(z_i|x; \\lambda) \\\\\n",
        "    &= \\prod_{i=1}^{n} \\text{Bern}(z_i|g_i(x; \\lambda)) \n",
        "\\end{align}\n",
        "\n",
        "where $g(x; \\lambda)$ is a NN that maps from $x = \\langle x_1, \\ldots, x_n\\rangle$ to $n$ Bernoulli parameters, each of which, is a probability value (thus $0 < g_i(x; \\lambda) < 1$).\n",
        "\n",
        "Note that though we could condition on $y$ for approximate posterior inference, we are opportunistically leaving it out. This way, $Q$ is directly available at test time for making predictions. The figure below is a graphical depiction of the inference model (we show a dashed arrow from $y$ to $z$ to remind you that in principle the label is also available).\n",
        "\n",
        "<img src=\"https://github.com/probabll/dgm4nlp/raw/master/notebooks/sst/img/inference.png\"  height=\"200\">\n",
        "\n",
        "Here is an example design for $g$:\n",
        "\\begin{align}\n",
        "\\mathbf x_i &= \\text{glove}(x_i) \\\\\n",
        "\\mathbf t_1^n, \\mathbf h &= \\text{encoder}(\\mathbf x_1^n; \\lambda_{\\text{enc}}) \\\\\n",
        "g_i(x; \\lambda) &= \\sigma(\\text{dense}_1(\\mathbf t_i; \\lambda_{\\text{output}}))\n",
        "\\end{align}\n",
        "where\n",
        "* $\\text{glove}$ is a pre-trained embedding function;\n",
        "* $\\text{dense}_1$ is a dense layer with a single output;\n",
        "* and $\\sigma(\\cdot)$ is the sigmoid function, necessary to parameterise a Bernoulli distribution.\n",
        "\n",
        "From now on we will write $Q(z|x, \\lambda)$, that is, without $y.\n",
        "\n",
        "Here we implement this product of Bernoulli distributions:\n",
        "\n",
        "* implement $g$ in the constructor \n",
        "* and the forward pass"
      ]
    },
    {
      "metadata": {
        "id": "YLxfcAbuSiFo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ProductOfBernoullis(nn.Module):\n",
        "    \"\"\"\n",
        "    This is an inference network that parameterises independent Bernoulli distributions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 embed:       nn.Embedding,\n",
        "                 hidden_size: int = 200,\n",
        "                 layer:       str = \"bow\"\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        :param embed: an embedding layer\n",
        "        :param hidden_suze: hidden size for transformed inputs\n",
        "        :param layer: 'bow' for BoW encoding\n",
        "          you may alternatively implement and 'lstm' option\n",
        "          which uses a biLSTM to transform the inputs         \n",
        "        \"\"\"\n",
        "        super(ProductOfBernoullis, self).__init__()\n",
        "        # 1. we should have an embedding layer \n",
        "        # 2. we may transform the representations\n",
        "        # 3. and we should compute parameters for Bernoulli distributions\n",
        "        pass\n",
        "\n",
        "    def report_params(self):\n",
        "        count = 0\n",
        "        for name, p in self.named_parameters():\n",
        "            if p.requires_grad and \"embed\" not in name:\n",
        "                count += np.prod(list(p.shape))\n",
        "        print(\"{} #params: {}\".format(self.__class__.__name__, count))\n",
        "\n",
        "    def forward(self, x, mask) -> Bernoulli:\n",
        "        \"\"\"\n",
        "        It takes a tensor of tokens (integers)\n",
        "         and predicts a Bernoulli distribution for each position.\n",
        "        \n",
        "        :param x: [B, T]\n",
        "        :param mask: [B, T]\n",
        "        :returns: Bernoulli\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0is57kbz8awl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#SOLUTION\n",
        "class ProductOfBernoullis(nn.Module):\n",
        "    \"\"\"\n",
        "    This is an inference network that parameterises independent Bernoulli distributions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 embed:       nn.Embedding,\n",
        "                 hidden_size: int = 200,\n",
        "                 layer:       str = \"bow\"\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        :param embed: an embedding layer\n",
        "        :param hidden_suze: hidden size for transformed inputs\n",
        "        :param layer: 'bow' for BoW encoding\n",
        "          you may alternatively implement and 'lstm' option\n",
        "          which uses a biLSTM to transform the inputs         \n",
        "        \"\"\"\n",
        "\n",
        "        super(ProductOfBernoullis, self).__init__()\n",
        "\n",
        "        emb_size = embed.weight.shape[1]\n",
        "        # I will use twice the units \n",
        "        #  just to make the output as large as that of a biLSTM\n",
        "        enc_size = hidden_size * 2  \n",
        "\n",
        "        self.embed_layer = nn.Sequential(embed)\n",
        "        self.enc_layer = get_encoder(layer, emb_size, hidden_size)\n",
        "        self.logit_layer = nn.Linear(enc_size, 1, bias=True)\n",
        "        \n",
        "        self.report_params()\n",
        "\n",
        "    def report_params(self):\n",
        "        count = 0\n",
        "        for name, p in self.named_parameters():\n",
        "            if p.requires_grad and \"embed\" not in name:\n",
        "                count += np.prod(list(p.shape))\n",
        "        print(\"{} #params: {}\".format(self.__class__.__name__, count))\n",
        "\n",
        "    def forward(self, x, mask) -> Bernoulli:\n",
        "        \"\"\"\n",
        "        It takes a tensor of tokens (integers)\n",
        "         and predicts a Bernoulli distribution for each position.\n",
        "        \n",
        "        :param x: [B, T]\n",
        "        :param mask: [B, T]\n",
        "        :returns: Bernoulli\n",
        "        \"\"\"\n",
        "\n",
        "        # encode sentence\n",
        "        # [B]\n",
        "        lengths = mask.long().sum(1)\n",
        "        # [B, T, E]\n",
        "        emb = self.embed_layer(x)  \n",
        "        # [B, T, d]\n",
        "        h, _ = self.enc_layer(emb, mask, lengths)\n",
        "\n",
        "        # compute parameters for Bernoulli p(z|x)\n",
        "        # [B, T, 1] Bernoulli distributions\n",
        "        logits = self.logit_layer(h)\n",
        "        # [B, T]\n",
        "        logits = logits.squeeze(-1)\n",
        "        return Bernoulli(logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fcCu7vkKWvZX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Parameter Estimation\n",
        "\n",
        "In variational inference, our objective is to maximise the *evidence lowerbound* (ELBO):\n",
        "\n",
        "\\begin{align}\n",
        "\\log P(y|x) &\\ge \\mathbb E_{Q(z|x, y, \\lambda)}\\left[ \\log P(y|x, z, \\theta, p_1) \\right] - \\text{KL}(Q(z|x, y, \\lambda) || P(z|p_1)) \\\\\n",
        "&\\overset{\\text{MF}}{=}\\mathbb E_{Q(z|x, y, \\lambda)}\\left[ \\log P(y|x, z, \\theta, p_1) \\right] - \\sum_{i=1}^n \\text{KL}(Q(z_i|x, \\lambda) || P(z_i|p_1)) \n",
        "\\end{align}\n",
        "\n",
        "where the *mean field* assumption we made implies that the KL term is simply a sum of KL divergences from a Bernoulli posterior to a Bernoulli prior.\n",
        "\n",
        "Note that the ELBO remains intractable, namely, solving the expectation in closed form still requires $2^n$ evaluations of the classifier network. Though unlike the true posterior $P(z|x,y, \\lambda)$, the approximation $Q(z|x,\\lambda)$ is tractable (it does not require an intractable normalisation) and can be used to obtain gradient estimates based on samples.\n",
        "\n",
        "### Gradient of the classifier network\n",
        "\n",
        "For the classifier, we encounter no problem:\n",
        "\n",
        "\\begin{align}\n",
        "&\\nabla_\\theta\\sum_{z} Q(z|x, \\lambda)\\log P(y|x,z,\\theta) \\\\\n",
        "&=\\sum_{z} Q(z|x, \\lambda)\\nabla_\\theta\\log P(y|x,z,\\theta) \\\\\n",
        "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[\\nabla_\\theta\\log P(y|x,z,\\theta) \\right] \\\\\n",
        "&\\overset{\\text{MC}}{\\approx} \\frac{1}{S} \\sum_{s=1}^S \\nabla_\\theta \\log P(y|x, z^{(s)}, \\theta) \n",
        "\\end{align}\n",
        "where $z^{(s)} \\sim Q(z|x,\\lambda)$.\n",
        "\n",
        "\n",
        "### Gradient of the inference network\n",
        "\n",
        "For the inference model, we have to use the *score function estimator* (a.k.a. REINFORCE):\n",
        "\n",
        "\\begin{align}\n",
        "&\\nabla_\\lambda\\sum_{z} Q(z|x, \\lambda)\\log P(y|x,z,\\theta)\\\\\n",
        "&=\\sum_{z} \\nabla_\\lambda Q(z|x, \\lambda)\\log P(y|x,z,\\theta) \\\\\n",
        "&=\\sum_{z}  \\underbrace{Q(z|x, \\lambda) \\nabla_\\lambda \\log Q(z|x, \\lambda)}_{\\nabla_\\lambda Q(z|x, \\lambda)} \\log P(y|x,z,\\theta) \\\\\n",
        "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[ \\log P(y|x,z,\\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda) \\right] \\\\\n",
        "&\\overset{\\text{MC}}{\\approx} \\frac{1}{S} \\sum_{s=1}^S  \\log P(y|x, z^{(s)}, \\theta) \\nabla_\\lambda \\log Q(z^{(s)}|x, \\lambda) \n",
        "\\end{align}\n",
        "\n",
        "where $z^{(s)} \\sim Q(z|x,\\lambda)$."
      ]
    },
    {
      "metadata": {
        "id": "6cdfkOYdC0LQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementation\n",
        "\n",
        "Let's implement the model and the loss (negative ELBO). We work with the notion of a *surrogate loss*, that is, a computation node whose gradients wrt to parameters are equivalent to the gradients we need.\n",
        "\n",
        "For a given sample $z \\sim Q(z|x, \\lambda)$, the following is a single-sample surrogate loss:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal S(\\theta, \\lambda|x, y) = \\log P(y|x, z, \\theta) + \\color{red}{\\text{detach}(\\log P(y|x, z, \\theta) )}\\log Q(z|x, \\lambda)\n",
        "\\end{align}\n",
        "where we introduce an auxiliary function such that\n",
        "\\begin{align}\n",
        "\\text{detach}(f(\\alpha))  &= h(\\alpha) \\\\\n",
        "\\nabla_\\beta \\text{detach}(h(\\alpha))  &= 0 \n",
        "\\end{align}\n",
        "or in words, *detach* does not alter the forward call of its argument function $h$, but it alters $h$'s backward call by setting gradients to zero.\n",
        "\n",
        "Show that it's gradients wrt $\\theta$ and $\\lambda$ are exactly what we need:\n"
      ]
    },
    {
      "metadata": {
        "id": "FednEChaX6WI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\\begin{align}\n",
        "\\nabla_\\theta \\mathcal S(\\theta, \\lambda|x, y) = \\color{red}{?}\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla_\\lambda \\mathcal S(\\theta, \\lambda|x, y) = \\color{red}{?}\n",
        "\\end{align}"
      ]
    },
    {
      "metadata": {
        "id": "MTN5McnhX72C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Solution**\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla_\\theta \\mathcal S(\\theta, \\lambda|x, y) = \\nabla_\\theta \\log P(y|x, z, \\theta) + 0\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla_\\lambda \\mathcal S(\\theta, \\lambda|x, y) &= 0 + \\underbrace{\\log Q(z|x, \\lambda)\\nabla_\\lambda \\log P(y|x, z, \\theta)  + \\log P(y|x, z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda)}_{\\text{chain rule}} \\\\ \n",
        "&= 0+ 0 + \\log P(y|x, z, \\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda)\n",
        "\\end{align}"
      ]
    },
    {
      "metadata": {
        "id": "wWBjp5L6yowL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Implement the forward pass and loss below:"
      ]
    },
    {
      "metadata": {
        "id": "Cnwwk-7tfR02",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    \n",
        "    Classifier model:\n",
        "        Z_i ~ Bern(p_1) for i in 1..n\n",
        "        Y|x,z ~ Cat(f([x_i if z_i 1 else 0 for i in 1..n ]))\n",
        "    \n",
        "    Inference model:\n",
        "        Z_i|x ~ Bern(b_i) for i in 1..n\n",
        "            where b_i = g_i(x)\n",
        "    \n",
        "    Objective:\n",
        "        Single-sample MC estimate of ELBO\n",
        "    \n",
        "    Loss: \n",
        "        Surrogate loss\n",
        "\n",
        "    Consists of:\n",
        "        - a product of Bernoulli distributions inference network\n",
        "        - a classifier network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab:       object = None,\n",
        "                 vocab_size:  int = 0,\n",
        "                 emb_size:    int = 200,\n",
        "                 hidden_size: int = 200,\n",
        "                 num_classes: int = 5,\n",
        "                 prior_p1:    float = 0.3,                 \n",
        "                 det_prior: bool = True,\n",
        "                 beta_shape:  list = [0.6, 0.6],\n",
        "                 dropout:     float = 0.1,\n",
        "                 layer_cls:   str = 'bow',\n",
        "                 layer_inf:   str = 'bow',\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        :param vocab: Vocabulary\n",
        "        :param vocab_size: necessary for embedding layer\n",
        "        :param emb_size: dimensionality of embedding layer\n",
        "        :param hidden_size: dimensionality of hidden layers\n",
        "        :param num_classes: number of classes\n",
        "        :param prior_p1: (scalar) prior Bernoulli parameter\n",
        "        :param det_prior: (boolean) whether the prior parameter is deterministic\n",
        "        :param beta_shape: (pair of positive scalars) \n",
        "            when the prior parameter is stochastic\n",
        "            it is sampled from a Beta distribution (ignore this at first)\n",
        "        :param dropout: (scalar) dropout rate\n",
        "        :param layer_cls: type of encoder for classification\n",
        "        :param layer_inf: type of encoder for inference\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.embed = embed = nn.Embedding(vocab_size, emb_size, padding_idx=1)\n",
        "\n",
        "        self.cls_net = Classifier(\n",
        "            embed=embed, \n",
        "            hidden_size=hidden_size, \n",
        "            output_size=num_classes,\n",
        "            dropout=dropout, \n",
        "            layer=layer_cls)\n",
        "        \n",
        "        self.inference_net = ProductOfBernoullis(\n",
        "            embed=embed, \n",
        "            hidden_size=hidden_size,\n",
        "            layer=layer_inf)\n",
        "        \n",
        "        self._prior_p1 = prior_p1\n",
        "        self._det_prior = det_prior\n",
        "        self._beta_shape = beta_shape\n",
        "        \n",
        "    def get_prior_p1(self, p_min=0.001, p_max=0.999):\n",
        "        \"\"\"Return the prior Bernoulli parameter\"\"\"\n",
        "        if self._det_prior:\n",
        "            return self._prior_p1\n",
        "        else:\n",
        "            a, b = self._beta_shape\n",
        "            prior_p1 = np.random.beta(a, b)\n",
        "            prior_p1 = max(prior_p1, p_min)\n",
        "            prior_p1 = min(prior_p1, p_max)\n",
        "        return prior_p1\n",
        "\n",
        "    def predict(self, py: Categorical, **kwargs):\n",
        "        \"\"\"\n",
        "        Predict deterministically using argmax.\n",
        "        :param py: B Categorical distributions (one per instance in batch)\n",
        "        :return: predictions\n",
        "            [B] sentiment levels\n",
        "        \"\"\"\n",
        "        assert not self.training, \"should be in eval mode for prediction\"\n",
        "        return py.log_probs.argmax(-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Generate a sequence z with inference model, \n",
        "         then predict with rationale xz, that is, x masked by z.\n",
        "\n",
        "        :param x: [B, T] documents\n",
        "        :return: \n",
        "            Categorical distributions P(y|x, z)\n",
        "            Bernoulli distributions Q(z|x)\n",
        "            Single sample z ~ Q(z|x) used for the conditional P(y|x, z)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def get_loss(self,                   \n",
        "                 y, \n",
        "                 py: Categorical,\n",
        "                 qz: Bernoulli, \n",
        "                 z, \n",
        "                 mask,\n",
        "                 iter_i=0, \n",
        "                 # you may ignore the rest of the arguments for the time being\n",
        "                 #  leave them as they are\n",
        "                 kl_weight=1.0,\n",
        "                 min_kl=0.0,\n",
        "                 ll_mean=0.,\n",
        "                 ll_std=1.,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        This computes the loss for the whole model.\n",
        "\n",
        "        :param y: target labels [B]\n",
        "        :param py: conditionals P(y|x, z)\n",
        "        :param qz: approximate posteriors Q(z|x)\n",
        "        :param z: sample of binary selectors [B, T]\n",
        "        :param mask: indicates valid positions [B, T]\n",
        "        :param iter_i: indicates the iteration\n",
        "        :param kl_weight: (scalar) multiplies the KL term\n",
        "        :param min_kl: (scalar) sets a minimum for the KL (aka free bits)\n",
        "        :param ll_mean: (scalar) running average of reward\n",
        "        :param ll_std: (scalar) running standard deviation of reward\n",
        "        :return: loss (torch node), terms (dict)\n",
        "        \n",
        "            terms is an OrderedDict that holds the scalar items involved in the loss\n",
        "            e.g. `terms['ll'] = ll.item()` is the log-likelihood term\n",
        "            \n",
        "            Consider tracking the following:\n",
        "            Single-sample ELBO: terms['elbo']\n",
        "            Log-Likelihood log P(y|x,z): terms['ll']\n",
        "            KL: terms['kl']\n",
        "            Score function surrogate log P(y|z, x) log Q(z|x): terms['sf']            \n",
        "            Rate of selected words: terms['selected']\n",
        "        \"\"\"\n",
        "\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MmmvgoaLaIZf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#SOLUTION\n",
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    \n",
        "    Classifier model:\n",
        "        Z_i ~ Bern(p_1) for i in 1..n\n",
        "        Y|x,z ~ Cat(f([x_i if z_i 1 else 0 for i in 1..n ]))\n",
        "    \n",
        "    Inference model:\n",
        "        Z_i|x ~ Bern(b_i) for i in 1..n\n",
        "            where b_i = g_i(x)\n",
        "    \n",
        "    Objective:\n",
        "        Single-sample MC estimate of ELBO\n",
        "    \n",
        "    Loss: \n",
        "        Surrogate loss\n",
        "\n",
        "    Consists of:\n",
        "        - a product of Bernoulli distributions inference network\n",
        "        - a classifier network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab:       object = None,\n",
        "                 vocab_size:  int = 0,\n",
        "                 emb_size:    int = 200,\n",
        "                 hidden_size: int = 200,\n",
        "                 num_classes: int = 5,\n",
        "                 prior_p1:    float = 0.3,                 \n",
        "                 det_prior: bool = True,\n",
        "                 beta_shape:  list = [0.6, 0.6],                 \n",
        "                 dropout:     float = 0.1,\n",
        "                 layer_cls:   str = 'bow',\n",
        "                 layer_inf:   str = 'bow',\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        :param vocab: Vocabulary\n",
        "        :param vocab_size: necessary for embedding layer\n",
        "        :param emb_size: dimensionality of embedding layer\n",
        "        :param hidden_size: dimensionality of hidden layers\n",
        "        :param num_classes: number of classes\n",
        "        :param prior_p1: (scalar) prior Bernoulli parameter\n",
        "        :param det_prior: (boolean) whether the prior parameter is deterministic\n",
        "        :param beta_shape: (pair of positive scalars) \n",
        "            when the prior parameter is stochastic\n",
        "            it is sampled from a Beta distribution (ignore this at first)\n",
        "        :param dropout: (scalar) dropout rate\n",
        "        :param layer_cls: type of encoder for classification\n",
        "        :param layer_inf: type of encoder for inference\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.embed = embed = nn.Embedding(vocab_size, emb_size, padding_idx=1)\n",
        "\n",
        "        self.cls_net = Classifier(\n",
        "            embed=embed, \n",
        "            hidden_size=hidden_size, \n",
        "            output_size=num_classes,\n",
        "            dropout=dropout, \n",
        "            layer=layer_cls)\n",
        "        \n",
        "        self.inference_net = ProductOfBernoullis(\n",
        "            embed=embed, \n",
        "            hidden_size=hidden_size,\n",
        "            layer=layer_inf)\n",
        "        \n",
        "        self._prior_p1 = prior_p1\n",
        "        self._det_prior = det_prior\n",
        "        self._beta_shape = beta_shape\n",
        "        \n",
        "    def get_prior_p1(self, p_min=0.001, p_max=0.999):\n",
        "        if self._det_prior:\n",
        "            return self._prior_p1\n",
        "        else:\n",
        "            a, b = self._beta_shape\n",
        "            prior_p1 = np.random.beta(a, b)\n",
        "            prior_p1 = max(prior_p1, p_min)\n",
        "            prior_p1 = min(prior_p1, p_max)\n",
        "        return prior_p1\n",
        "\n",
        "    def predict(self, py: Categorical, **kwargs):\n",
        "        \"\"\"\n",
        "        Predict deterministically using argmax.\n",
        "        :param py: B Categorical distributions (one per instance in batch)\n",
        "        :return: predictions\n",
        "            [B] sentiment levels\n",
        "        \"\"\"\n",
        "        assert not self.training, \"should be in eval mode for prediction\"\n",
        "        return py.log_probs.argmax(-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Generate a sequence z with inference model, \n",
        "         then predict with rationale xz, that is, x masked by z.\n",
        "\n",
        "        :param x: [B, T] documents\n",
        "        :return: \n",
        "            Categorical distributions P(y|x, z)\n",
        "            Bernoulli distributions Q(z|x)\n",
        "            Single sample z ~ Q(z|x) used for the conditional P(y|x, z)\n",
        "        \"\"\"\n",
        "        mask = (x != 1)  # [B,T]\n",
        "\n",
        "        qz = self.inference_net(x, mask)\n",
        "\n",
        "        if self.training:  # sample\n",
        "            # [B, T]\n",
        "            z = qz.sample()\n",
        "        else:  # deterministic\n",
        "            # [B, T]\n",
        "            # TODO: consider this\n",
        "            z = (qz.probs >= 0.5).float()\n",
        "            #z = qz.sample()\n",
        "            \n",
        "        z = torch.where(mask, z, torch.zeros_like(z))\n",
        "        \n",
        "        py = self.cls_net(x, mask, z)\n",
        "        return py, qz, z\n",
        "\n",
        "    def get_loss(self,                   \n",
        "                 y, \n",
        "                 py: Categorical,\n",
        "                 qz: Bernoulli, \n",
        "                 z, \n",
        "                 mask,\n",
        "                 iter_i=0, \n",
        "                 # you may ignore the rest of the arguments for the time being\n",
        "                 #  leave them as they are\n",
        "                 kl_weight=1.0,\n",
        "                 min_kl=0.0,\n",
        "                 ll_mean=0.,\n",
        "                 ll_std=1.,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        This computes the loss for the whole model.\n",
        "\n",
        "        :param y: target labels [B]\n",
        "        :param py: conditionals P(y|x, z)\n",
        "        :param qz: approximate posteriors Q(z|x)\n",
        "        :param z: sample of binary selectors [B, T]\n",
        "        :param mask: indicates valid positions [B, T]\n",
        "        :param iter_i: indicates the iteration\n",
        "        :param kl_weight: (scalar) multiplies the KL term\n",
        "        :param min_kl: (scalar) sets a minimum for the KL (aka free bits)\n",
        "        :param ll_mean: (scalar) running average of reward\n",
        "        :param ll_std: (scalar) running standard deviation of reward\n",
        "        :return: loss (torch node), terms (dict)\n",
        "        \n",
        "            terms is a dict that holds the scalar items involved in the loss\n",
        "            e.g. `terms['ll'] = ll.item()` is the log-likelihood term\n",
        "            \n",
        "            Consider tracking the following:\n",
        "            Single-sample ELBO: terms['elbo']\n",
        "            Log-Likelihood log P(y|x,z): terms['ll']\n",
        "            KL: terms['kl']\n",
        "            Score function surrogate log P(y|z, x) log Q(z|x): terms['sf']            \n",
        "            Rate of selected words: terms['selected']\n",
        "        \"\"\"\n",
        "\n",
        "        lengths = mask.sum(1).float()\n",
        "        batch_size = mask.size(0)\n",
        "        terms = OrderedDict()\n",
        "\n",
        "        # shape: [B]\n",
        "        # log p(y|x,z) where z ~ q\n",
        "        #one_hot_target = (targets.unsqueeze(-1) == torch.arange(5, device=device).reshape(1, 5)).float()            \n",
        "        #ll = torch.sum(py.log_probs * one_hot_target, dim=-1)\n",
        "        # [B]\n",
        "        ll = py.log_pmf(y)\n",
        "        \n",
        "        # KL(q||p)\n",
        "        # [B, T]\n",
        "        prior_p1 = self.get_prior_p1()        \n",
        "        pz = Bernoulli(probs=torch.full_like(qz.probs, prior_p1))\n",
        "        \n",
        "        kl = qz.kl(pz)\n",
        "        kl = torch.where(mask, kl, torch.zeros_like(kl))\n",
        "                \n",
        "        # Compute the log density of the sample\n",
        "        # [B, T]\n",
        "        log_q_z = qz.log_pmf(z)\n",
        "        log_q_z = torch.where(mask, log_q_z, torch.zeros_like(log_q_z))\n",
        "        # We have independent Bernoullis, thus we just sum their log probabilities\n",
        "        # [B]\n",
        "        log_q_z = log_q_z.sum(1)\n",
        "        \n",
        "        # surrogate objective for score function estimator\n",
        "        # [B]\n",
        "        reward = (ll.detach() - torch.full_like(ll, ll_mean)) / torch.full_like(ll, ll_std)\n",
        "        sf_surrogate = (reward * log_q_z)\n",
        "\n",
        "        # Make terms in the ELBO\n",
        "        # []\n",
        "        ll = ll.mean()\n",
        "        sf_surrogate = sf_surrogate.mean()\n",
        "        # KL may require annealing and free-bits\n",
        "        # [B]\n",
        "        kl = kl.sum(dim=-1)\n",
        "        kl_fb = torch.max(torch.full_like(kl, min_kl), kl)\n",
        "        # []\n",
        "        kl = kl.mean() \n",
        "        kl_fb = kl_fb.mean() \n",
        "        kl_fb = kl_fb * kl_weight\n",
        "        \n",
        "        terms['elbo'] = (ll - kl_fb).item()\n",
        "        terms['ll'] = ll.item()\n",
        "        terms['kl_fb'] = kl_fb.item()\n",
        "        terms['kl'] = kl.item()\n",
        "        terms['kl_weight'] = kl_weight\n",
        "        terms['sf'] = sf_surrogate.item()\n",
        "        terms['reward'] = reward.mean().item()\n",
        "        terms['ll_mean'] = ll_mean\n",
        "        terms['ll_std'] = ll_std\n",
        "        terms['selected'] = (z.sum(1) / lengths).mean().item()\n",
        "        terms['prior_p1'] = prior_p1\n",
        "        terms['avg_p1'] = (torch.where(mask, qz.probs, torch.zeros_like(qz.probs)).sum() / mask.sum().float()).item()\n",
        "        # TODO log min and max p1 in batch (mask properly)\n",
        "        return - ll - sf_surrogate + kl_fb, terms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wNQDXTpqWvZa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#SOLUTION\n",
        "from collections import deque\n",
        "\n",
        "class MovingStats:\n",
        "    \n",
        "    def __init__(self, memory=-1):\n",
        "        self.data = deque([])\n",
        "        self.memory = memory\n",
        "        \n",
        "    def append(self, value):\n",
        "        if self.memory != 0:\n",
        "            if self.memory > 0 and len(self.data) == self.memory:\n",
        "                self.data.popleft()\n",
        "            self.data.append(value)\n",
        "        \n",
        "    def mean(self):\n",
        "        if len(self.data):\n",
        "            return np.mean([x for x in self.data])\n",
        "        else:\n",
        "            return 0.\n",
        "    \n",
        "    def std(self):\n",
        "        return np.std(self.data) if len(self.data) > 1 else 1.\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "081YSfU9WvZc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ]
    },
    {
      "metadata": {
        "id": "9Pc80gseWvZd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# some helper code for mini batching\n",
        "#  this will take care of annoying things such as \n",
        "#  sorting training instances by length (necessary for pytorch's LSTM, for example)\n",
        "from sst.util import make_kv_string, get_minibatch, prepare_minibatch, print_parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_WVr97kilIRV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim\n",
        "# We will use Adam\n",
        "from torch.optim import Adam\n",
        "# and a couple of tricks to reduce learning rate on plateau\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "# here is some helper code to evaluate your model\n",
        "from sst.evaluate import evaluate\n",
        "\n",
        "\n",
        "cfg = dict()\n",
        "\n",
        "# Data\n",
        "cfg['training_path'] = \"sst/data/sst/train.txt\"\n",
        "cfg['dev_path'] = \"sst/data/sst/dev.txt\"\n",
        "cfg['test_path'] = \"sst/data/sst/test.txt\"\n",
        "cfg['word_vectors'] = 'sst/data/sst/glove.840B.300d.filtered.txt'\n",
        "# Model\n",
        "cfg['prior_p1'] = 0.3\n",
        "cfg['beta_a'] = 0.6\n",
        "cfg['beta_b'] = 0.6\n",
        "cfg['det_prior'] = True\n",
        "# Architecture\n",
        "cfg['num_epochs'] = 50\n",
        "cfg['print_every'] = 100\n",
        "cfg['eval_every'] = -1\n",
        "cfg['batch_size'] = 25\n",
        "cfg['eval_batch_size'] = 25\n",
        "cfg['subphrases'] = False\n",
        "cfg['min_phrase_length'] = 2\n",
        "cfg['lowercase'] = True\n",
        "cfg['fix_emb'] = True\n",
        "cfg['embed_size'] = 300\n",
        "cfg['hidden_size'] = 150\n",
        "cfg['num_layers'] = 1\n",
        "cfg['dropout'] = 0.5\n",
        "cfg['layer_inf'] = 'bow'\n",
        "cfg['layer_cls'] = 'bow'\n",
        "cfg['save_path'] = 'data/results'\n",
        "cfg['baseline_memory'] = 1000\n",
        "cfg['min_kl'] = 0.  # use more than 0 to enable free bits\n",
        "cfg['kl_weight'] = 1.  # start from zero to enable annealing\n",
        "cfg['kl_inc'] = 0.00001  \n",
        "# Optimiser (leave as is)\n",
        "cfg['lr'] = 0.0002\n",
        "cfg['weight_decay'] = 1e-5\n",
        "cfg['lr_decay'] = 0.5\n",
        "cfg['patience'] = 5\n",
        "cfg['cooldown'] = 5\n",
        "cfg['threshold'] = 1e-4\n",
        "cfg['min_lr'] = 1e-5\n",
        "cfg['max_grad_norm'] = 5.\n",
        "\n",
        "\n",
        "print('# Configuration')\n",
        "for k, v in cfg.items():\n",
        "    print(\"{:20} : {:10}\".format(k, v))\n",
        "\n",
        "\n",
        "iters_per_epoch = len(train_data) // cfg[\"batch_size\"]\n",
        "\n",
        "if cfg[\"eval_every\"] == -1:\n",
        "    eval_every = iters_per_epoch\n",
        "    print(\"Set eval_every to {}\".format(iters_per_epoch))\n",
        "\n",
        "\n",
        "# Let's load the data into memory.\n",
        "print(\"Loading data\")\n",
        "train_data = list(examplereader(\n",
        "    cfg['training_path'],\n",
        "    lower=cfg['lowercase'], \n",
        "    subphrases=cfg['subphrases'],\n",
        "    min_length=cfg['min_phrase_length']))\n",
        "dev_data = list(examplereader(cfg['dev_path'], lower=cfg['lowercase']))\n",
        "test_data = list(examplereader(cfg['test_path'], lower=cfg['lowercase']))\n",
        "\n",
        "print(\"train\", len(train_data))\n",
        "print(\"dev\", len(dev_data))\n",
        "print(\"test\", len(test_data))\n",
        "\n",
        "print('\\n# Example')\n",
        "example = dev_data[0]\n",
        "print(\"First dev example:\", example)\n",
        "print(\"First dev example tokens:\", example.tokens)\n",
        "print(\"First dev example label:\", example.label)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "6PMqtVj0WvZf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "    # Create a vocabulary object to map str <-> int\n",
        "    vocab = Vocabulary()  # populated by load_glove\n",
        "    glove_path = cfg[\"word_vectors\"]\n",
        "    vectors = load_glove(glove_path, vocab)\n",
        "\n",
        "    # You may consider using tensorboardX\n",
        "    # writer = SummaryWriter(log_dir=cfg[\"save_path\"])\n",
        "\n",
        "    # Map the sentiment labels 0-4 to a more readable form (and the opposite)\n",
        "    i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
        "    t2i = OrderedDict({p: i for p, i in zip(i2t, range(len(i2t)))})\n",
        "\n",
        "\n",
        "    print('\\n# Constructing model')\n",
        "    model = Model(\n",
        "        vocab_size=len(vocab.w2i), \n",
        "        emb_size=cfg[\"embed_size\"],\n",
        "        hidden_size=cfg[\"hidden_size\"], \n",
        "        num_classes=len(t2i),\n",
        "        prior_p1=cfg['prior_p1'],\n",
        "        det_prior=cfg['det_prior'],\n",
        "        beta_shape=[cfg['beta_a'], cfg['beta_b']],\n",
        "        vocab=vocab, \n",
        "        dropout=cfg[\"dropout\"], \n",
        "        layer_cls=cfg[\"layer_cls\"],\n",
        "        layer_inf=cfg[\"layer_inf\"])\n",
        "\n",
        "    print('\\n# Loading embeddings')\n",
        "    with torch.no_grad():\n",
        "        model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "        if cfg[\"fix_emb\"]:\n",
        "            print(\"fixed word embeddings\")\n",
        "            model.embed.weight.requires_grad = False\n",
        "        model.embed.weight[1] = 0.  # padding zero\n",
        "\n",
        "        \n",
        "    # Congigure optimiser\n",
        "    optimizer = Adam(model.parameters(), lr=cfg[\"lr\"],\n",
        "                     weight_decay=cfg[\"weight_decay\"])\n",
        "    # and learning rate scheduler\n",
        "    scheduler = ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", factor=cfg[\"lr_decay\"], patience=cfg[\"patience\"],\n",
        "        verbose=True, cooldown=cfg[\"cooldown\"], threshold=cfg[\"threshold\"],\n",
        "        min_lr=cfg[\"min_lr\"])\n",
        "\n",
        "    # Prepare a few auxiliary variables\n",
        "    iter_i = 0\n",
        "    train_loss = 0.\n",
        "    print_num = 0\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    best_eval = 1.0e9\n",
        "    best_iter = 0\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Some debugging info\n",
        "    print(model)\n",
        "    print_parameters(model)\n",
        "\n",
        "    batch_size = cfg['batch_size']\n",
        "    eval_batch_size = cfg['eval_batch_size']\n",
        "    print_every = cfg['print_every']\n",
        "\n",
        "    # Parameters of tricks to better optimise the ELBO \n",
        "    kl_inc = cfg['kl_inc']\n",
        "    kl_weight = cfg['kl_weight']\n",
        "    min_kl = cfg['min_kl']\n",
        "    # Running estimates for baselines\n",
        "    ll_moving_stats = MovingStats(cfg['baseline_memory'])\n",
        "\n",
        "    while True:  # when we run out of examples, shuffle and continue\n",
        "        for batch in get_minibatch(train_data, batch_size=batch_size, shuffle=True):\n",
        "\n",
        "            epoch = iter_i // iters_per_epoch\n",
        "            if epoch > cfg['num_epochs']:\n",
        "                break\n",
        "\n",
        "            # forward pass\n",
        "            model.train()\n",
        "            x, y, _ = prepare_minibatch(batch, model.vocab, device=device)\n",
        "\n",
        "            # with autograd.detect_anomaly():\n",
        "\n",
        "            py, qz, z = model(x)\n",
        "\n",
        "            mask = (x != 1)\n",
        "\n",
        "            # \"KL annealing\"\n",
        "            kl_weight += kl_inc\n",
        "            if kl_weight > 1.:\n",
        "                kl_weight = 1.0\n",
        "                \n",
        "            loss, terms = model.get_loss(\n",
        "                y,\n",
        "                py=py, \n",
        "                qz=qz,\n",
        "                z=z,\n",
        "                mask=mask, \n",
        "                kl_weight=kl_weight,\n",
        "                min_kl=min_kl,\n",
        "                ll_mean=ll_moving_stats.mean(),\n",
        "                ll_std=ll_moving_stats.std(),\n",
        "                iter_i=iter_i)\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            # keep an running estimate of the reward (log P(y|x,z))\n",
        "            ll_moving_stats.append(terms['ll'])\n",
        "\n",
        "            # backward pass\n",
        "            model.zero_grad()  # erase previous gradients\n",
        "\n",
        "            loss.backward()  # compute new gradients\n",
        "\n",
        "            # gradient clipping generally helps\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=cfg['max_grad_norm'])\n",
        "\n",
        "            # update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            print_num += 1\n",
        "            iter_i += 1\n",
        "\n",
        "            # print info\n",
        "            if iter_i % print_every == 0:\n",
        "\n",
        "                train_loss = train_loss / print_every\n",
        "\n",
        "                print_str = make_kv_string(terms)\n",
        "                print(\"Epoch %r Iter %r loss=%.4f %s\" %\n",
        "                      (epoch, iter_i, train_loss, print_str))\n",
        "                losses.append(train_loss)\n",
        "                print_num = 0\n",
        "                train_loss = 0.\n",
        "\n",
        "            # evaluate\n",
        "            if iter_i % eval_every == 0:\n",
        "\n",
        "                dev_eval, rationales = evaluate(\n",
        "                    model, dev_data, \n",
        "                    batch_size=eval_batch_size, \n",
        "                    device=device,\n",
        "                    cfg=cfg, iter_i=iter_i)\n",
        "                accuracies.append(dev_eval[\"acc\"])\n",
        "\n",
        "                print(\"\\n# epoch %r iter %r: dev %s\" % (\n",
        "                    epoch, iter_i, make_kv_string(dev_eval)))\n",
        "                \n",
        "                for exid in range(3):\n",
        "                    print(' dev%d [gold=%d,pred=%d]:' % (exid, dev_data[exid].label, rationales[exid][1]),  \n",
        "                          ' '.join(rationales[exid][0]))\n",
        "                print()\n",
        "\n",
        "                # adjust learning rate\n",
        "                scheduler.step(dev_eval[\"loss\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "A5uYKcw-WvZl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SVsWgmlIWvZq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2VntYV3WvZt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rTxG1AvPWvZv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}